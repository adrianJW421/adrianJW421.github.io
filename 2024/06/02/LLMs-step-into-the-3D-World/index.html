<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>When LLMs step into the 3D World A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models - NeroZac</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Zac&#039;s Blog"><meta name="msapplication-TileImage" content="/img/avatar.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Zac&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="摘要: 本文重点介绍了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和广泛的世界知识，强调它们在提升空间理解和交互方面的巨大潜力。我们的研究涵盖了各种3D数据表示，从点云到神经辐射场（NeRFs），并探讨了它们与LLMs在3D场景理解、描述、问答和对话等任务中的整合，以及基于LLM的空间推理、规划和导航代理。 本文还简要回顾了其他整合3D和语言的方法。本文的荟萃分析显示了显著的进展，但也"><meta property="og:type" content="blog"><meta property="og:title" content="Zac&#039;s Blog"><meta property="og:url" content="https://nerozac.com/2024/06/02/LLMs-step-into-the-3D-World/"><meta property="og:site_name" content="Jiawei Li (Zac)"><meta property="og:description" content="摘要: 本文重点介绍了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和广泛的世界知识，强调它们在提升空间理解和交互方面的巨大潜力。我们的研究涵盖了各种3D数据表示，从点云到神经辐射场（NeRFs），并探讨了它们与LLMs在3D场景理解、描述、问答和对话等任务中的整合，以及基于LLM的空间推理、规划和导航代理。 本文还简要回顾了其他整合3D和语言的方法。本文的荟萃分析显示了显著的进展，但也"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://nerozac.com/img/og_image.png"><meta property="article:published_time" content="2024-06-02T15:54:01.625Z"><meta property="article:modified_time" content="2024-06-03T03:49:00.170Z"><meta property="article:author" content="Jiawei Li (Zac)"><meta property="article:tag" content="论文阅读"><meta property="article:tag" content="3DLLM"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://nerozac.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://nerozac.com/2024/06/02/LLMs-step-into-the-3D-World/"},"headline":"Zac's Blog","image":["https://nerozac.com/img/og_image.png"],"datePublished":"2024-06-02T15:54:01.625Z","dateModified":"2024-06-03T03:49:00.170Z","author":{"@type":"Person","name":"Jiawei Li"},"publisher":{"@type":"Organization","name":"NeroZac","logo":{"@type":"ImageObject","url":{"text":"Zac's Blog"}}},"description":"摘要: 本文重点介绍了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和广泛的世界知识，强调它们在提升空间理解和交互方面的巨大潜力。我们的研究涵盖了各种3D数据表示，从点云到神经辐射场（NeRFs），并探讨了它们与LLMs在3D场景理解、描述、问答和对话等任务中的整合，以及基于LLM的空间推理、规划和导航代理。 本文还简要回顾了其他整合3D和语言的方法。本文的荟萃分析显示了显著的进展，但也"}</script><link rel="canonical" href="https://nerozac.com/2024/06/02/LLMs-step-into-the-3D-World/"><link rel="icon" href="/img/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Zac&#039;s Blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/adrianJW421"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-06-02T15:54:01.625Z" title="6/2/2024, 11:54:01 PM">2024-06-02</time>发表</span><span class="level-item"><time dateTime="2024-06-03T03:49:00.170Z" title="6/3/2024, 11:49:00 AM">2024-06-03</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/3DLLM/">3DLLM</a></span><span class="level-item">3 小时读完 (大约24851个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">When LLMs step into the 3D World A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models</h1><div class="content"><p>摘要: 本文重点介绍了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和广泛的世界知识，强调它们在提升空间理解和交互方面的巨大潜力。我们的研究涵盖了各种3D数据表示，从点云到神经辐射场（NeRFs），并探讨了它们与LLMs在3D场景理解、描述、问答和对话等任务中的整合，以及基于LLM的空间推理、规划和导航代理。 本文还简要回顾了其他整合3D和语言的方法。本文的荟萃分析显示了显著的进展，但也强调了需要新的方法来充分利用3D-LLMs的潜力。因此，通过本文，我们旨在为未来的研究指明方向，探索和扩展3D-LLMs在理解和互动复杂3D世界中的能力。</p>
<span id="more"></span>

<style>
    .blue-box {
        border: 2px solid blue; /* 设置边框为蓝色 */
        padding: 5px; /* 设置内边距 */
        background-color: lightblue; /* 设置背景颜色为浅蓝色 */
        color: black; /* 设置文本颜色为黑色 */
        width: 600px; /* 设置方框的宽度 */
        margin: 5px; /* 设置外边距 */
    }
</style>

<br>

<p><span id="jumpHere"> 〓 Table of Contents 〓 </span></p>
<br>

<ul>
<li><a href="#%E6%91%98%E8%A6%81">摘要</a></li>
<li><a href="#%E4%BB%8B%E7%BB%8Dintroduction">介绍Introduction</a></li>
<li><a href="#%E8%83%8C%E6%99%AF">背景</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E7%9A%843d%E8%A1%A8%E7%A4%BA">数据的3D表示</a></li>
<li><a href="#%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88llm%EF%BC%89">大型语言模型（LLM）</a><ul>
<li><a href="#%E5%85%B3%E4%BA%8Ellm%E6%9E%B6%E6%9E%84">关于LLM架构</a></li>
<li><a href="#%E5%88%86%E8%AF%8D%EF%BC%88tokenization%EF%BC%89">分词（Tokenization）</a></li>
</ul>
</li>
<li><a href="#%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B">大型语言模型的涌现能力</a></li>
<li><a href="#%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BE%AE%E8%B0%83">大型语言模型的微调</a></li>
<li><a href="#%E5%85%B3%E4%BA%8E2d%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">关于2D视觉语言模型</a></li>
<li><a href="#%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%EF%BC%88vfms%EF%BC%89">视觉基础模型（VFMs）</a></li>
</ul>
</li>
<li><a href="#%E4%BB%BB%E5%8A%A1%E5%92%8C%E6%8C%87%E6%A0%87%E4%BB%8B%E7%BB%8Dtasks-and-metrics">任务和指标介绍(TASKS AND METRICS)</a><ul>
<li><a href="#%E4%BB%BB%E5%8A%A13d%E6%A0%87%E6%B3%A8%EF%BC%883d-to%E6%96%87%E6%9C%AC%EF%BC%89">任务 3D标注（3D to 文本）</a></li>
<li><a href="#%E4%BB%BB%E5%8A%A13d%E5%AE%9A%E4%BD%8D%EF%BC%883d%E5%8A%A0%E6%96%87%E6%9C%AC-to-3d%E4%BD%8D%E7%BD%AE%EF%BC%89">任务 3D定位（3D加文本 to 3D位置）</a></li>
<li><a href="#%E4%BB%BB%E5%8A%A13d%E5%AF%B9%E8%AF%9D%EF%BC%883d%E5%8A%A0%E6%96%87%E6%9C%AC-to%E6%96%87%E6%9C%AC%EF%BC%89">任务 3D对话（3D 加 文本 to 文本）</a></li>
<li><a href="#%E4%BB%BB%E5%8A%A13d%E5%85%B7%E8%BA%AB%E4%BB%A3%E7%90%86%EF%BC%883d%E5%8A%A0%E6%96%87%E6%9C%AC-to%E8%A1%8C%E5%8A%A8%EF%BC%89">任务 3D具身代理（3D 加 文本 to 行动）</a></li>
<li><a href="#%E4%BB%BB%E5%8A%A1%E6%96%87%E6%9C%AC%E5%88%B03d%E7%94%9F%E6%88%90%EF%BC%88text-to-3d%EF%BC%89">任务 文本到3D生成（Text to 3D）</a></li>
</ul>
</li>
<li><a href="#%E5%85%B3%E4%BA%8E3d-tasks-with-llms">关于3D TASKS WITH LLMS</a><ul>
<li><a href="#%E5%85%B3%E4%BA%8Ellms%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%863d%E5%9C%BA%E6%99%AF%E4%BF%A1%E6%81%AF%EF%BC%9F">关于LLMs如何处理3D场景信息？</a></li>
<li><a href="#%E4%BD%BF%E7%94%A8llms%E6%8F%90%E5%8D%873d%E4%BB%BB%E5%8A%A1%E6%80%A7%E8%83%BD">使用LLMs提升3D任务性能</a><ul>
<li><a href="#%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95">知识增强方法</a></li>
<li><a href="#%E6%8E%A8%E7%90%86%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95">推理增强方法</a></li>
</ul>
</li>
<li><a href="#%E4%BD%BF%E7%94%A8llms%E8%BF%9B%E8%A1%8C3d%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0">使用LLMs进行3D多任务学习</a><ul>
<li><a href="#%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E6%8D%AE">多任务学习的数据</a></li>
<li><a href="#%E5%A4%9A%E4%BB%BB%E5%8A%A13d%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83">多任务3D模型的训练</a></li>
</ul>
</li>
<li><a href="#llms%E4%BD%9C%E4%B8%BA3d%E5%A4%9A%E6%A8%A1%E6%80%81%E6%8E%A5%E5%8F%A3">LLMs作为3D多模态接口</a></li>
<li><a href="#llms%E7%94%A8%E4%BA%8E%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E4%BD%93">LLMs用于具身智能体</a><ul>
<li><a href="#3d%E4%BB%BB%E5%8A%A1%E8%A7%84%E5%88%92">3D任务规划</a></li>
<li><a href="#3d%E5%AF%BC%E8%88%AA">3D导航</a></li>
<li><a href="#3d%E7%89%A9%E4%BD%93%E6%93%8D%E4%BD%9C">3D物体操作</a></li>
</ul>
</li>
<li><a href="#llms%E7%94%A8%E4%BA%8E3d%E7%94%9F%E6%88%90">LLMs用于3D生成</a><ul>
<li><a href="#%E5%AF%B9%E8%B1%A1%E7%BA%A7%E7%94%9F%E6%88%90">对象级生成</a></li>
<li><a href="#%E5%9C%BA%E6%99%AF%E7%BA%A7%E7%94%9F%E6%88%90">场景级生成</a></li>
<li><a href="#%E7%A8%8B%E5%BA%8F%E7%94%9F%E6%88%90%E4%B8%8E%E6%93%8D%E6%8E%A7">程序生成与操控</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8E%E4%B8%89%E7%BB%B4%E4%BB%BB%E5%8A%A1%E7%B3%BB%E5%88%973d-tasks-with-vlms">视觉语言模型与三维任务系列(3D TASKS WITH VLMS)</a><ul>
<li><a href="#%E5%88%A9%E7%94%A8vlms%E8%BF%9B%E8%A1%8C3d%E4%BB%BB%E5%8A%A1">利用VLMs进行3D任务</a><ul>
<li><a href="#%E5%BC%80%E6%94%BE%E8%AF%8D%E6%B1%873d%E5%9C%BA%E6%99%AF%E7%90%86%E8%A7%A3">开放词汇3D场景理解</a></li>
<li><a href="#%E6%96%87%E6%9C%AC%E9%A9%B1%E5%8A%A8%E7%9A%843d%E7%94%9F%E6%88%90">文本驱动的3D生成</a></li>
</ul>
</li>
<li><a href="#%E4%B8%89%E7%BB%B4%E8%A7%86%E8%A7%89%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%9E%B6%E6%9E%84">三维视觉与语言的端到端架构</a></li>
</ul>
</li>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86">数据集</a><ul>
<li><a href="#cap3d">Cap3D</a></li>
<li><a href="#text2shape">Text2Shape</a></li>
<li><a href="#sceneverse">SceneVerse</a></li>
<li><a href="#nu-caption">nu-Caption</a></li>
<li><a href="#nu-grounding">nu-Grounding</a></li>
<li><a href="#scanrefer">ScanRefer</a></li>
<li><a href="#referit3d">ReferIt3D</a></li>
<li><a href="#multi3drefer">Multi3DRefer</a></li>
<li><a href="#chat-3d-v2">Chat-3D v2</a></li>
<li><a href="#embodiedscan">EmbodiedScan</a></li>
<li><a href="#scanents3d">ScanEnts3D</a></li>
<li><a href="#wildrefer">WildRefer</a></li>
<li><a href="#riorefer">RIORefer</a></li>
<li><a href="#arkitscenerefer">ARKitSceneRefer</a></li>
<li><a href="#scaneru">ScanERU</a></li>
<li><a href="#densegrounding">DenseGrounding</a></li>
<li><a href="#scanqa">ScanQA</a></li>
<li><a href="#scanqa%E5%8F%A6%E4%B8%80%E4%B8%AA">ScanQA (另一个)</a></li>
<li><a href="#3dmv-vqa">3DMV-VQA</a></li>
<li><a href="#nuscenes-qa">NuScenes-QA</a></li>
<li><a href="#clevr3d">CLEVR3D</a></li>
<li><a href="#sqa-3d">SQA-3D</a></li>
<li><a href="#3d-llm">3D-LLM</a></li>
<li><a href="#scanscribe">ScanScribe</a></li>
<li><a href="#m3dbench">M3DBench</a></li>
<li><a href="#gpt4point">GPT4Point</a></li>
<li><a href="#lamm">LAMM</a></li>
</ul>
</li>
<li><a href="#%E6%9C%BA%E4%BC%9A%E4%B8%8E%E6%8C%91%E6%88%98challenges-and-opportunities">机会与挑战(CHALLENGES AND OPPORTUNITIES)</a></li>
<li><a href="#%E7%BB%93%E8%AE%BA">结论</a></li>
</ul>
<p><br><br><br></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>随着大型语言模型（Large Language Models, LLMs）的发展，它们与3D空间数据的整合（3D-LLMs）取得了快速进展，为理解和互动物理空间提供了前所未有的能力。本文综述了使LLMs能够处理、理解和生成3D数据的方法。</p>
<p>我们重点介绍了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和广泛的世界知识，强调它们在提升空间理解和交互方面的巨大潜力。我们的研究涵盖了各种3D数据表示，从点云到神经辐射场（NeRFs），并探讨了它们与LLMs在3D场景理解、描述、问答和对话等任务中的整合，以及基于LLM的空间推理、规划和导航代理。</p>
<p>本文还简要回顾了其他整合3D和语言的方法。本文的荟萃分析显示了显著的进展，但也强调了需要新的方法来充分利用3D-LLMs的潜力。因此，通过本文，我们旨在为未来的研究指明方向，探索和扩展3D-LLMs在理解和互动复杂3D世界中的能力。</p>
<p>为支持本次综述，我们建立了一个项目页面，组织和列出了与我们主题相关的论文：<a target="_blank" rel="noopener" href="https://github.com/ActiveVisionLab/Awesome-LLM-3D%E3%80%82">https://github.com/ActiveVisionLab/Awesome-LLM-3D。</a></p>
<p><br><br><br></p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>大型语言模型（Large Language Models, LLMs）的出现标志着自然语言处理领域的一个变革时代，使机器能够以前所未有的方式理解、生成和互动人类语言。然而，物理世界本质上是三维的，理解空间3D环境对于涉及感知、导航和互动的许多现实应用至关重要。随着最近的进展，LLMs的应用已经远远超出了文本的范畴。</p>
<p>将LLMs与3D数据融合，提供了一个独特的机会，可以增强计算模型对物理世界的理解和互动，从而在多个领域引领创新，包括自主系统、增强现实、机器人导航和机器人操作。近期的研究表明，整合LLMs与3D数据可以通过利用LLMs的固有优势，如零样本学习、先进推理和广泛知识，在复杂的3D环境中进行解释、推理或规划。</p>
<p>然而，LLMs与3D数据的整合并非易事。3D数据表示、模型可扩展性和计算效率等问题仍是重大障碍。此外，确保模型能够在现实世界环境中运行，需要克服与数据多样性和环境复杂性相关的挑战。解决这些问题对于充分实现LLMs在3D应用中的潜力，创造动态和情境感知的人工智能系统至关重要。</p>
<p>本文综述了LLMs与3D数据的交叉领域，提供了当前方法、应用和挑战的详尽概述。首先，介绍了常见的3D表示形式、LLMs的简要介绍以及视觉语言模型（VLMs）和视觉基础模型（VFMs）的概述。在第三部分，详细描述了当前方法旨在解决的3D视觉语言任务，并概述了当前的评估指标和协议。</p>
<p>接下来，在第四部分中，我们分析了数据格式、处理技术和在增强3D理解方面显示出前景的模型架构，展示了LLMs与3D数据成功结合的多个领域，如：利用LLMs的世界知识和推理能力增强3D任务性能，将LLMs作为多模态接口和具身代理，或使用LLMs生成复杂场景。除了LLMs，还有一些研究提出了统一3D感知与语言能力的端到端架构。广泛的研究还探讨了通过现成的2D视觉语言模型（VLMs）知识进行开放词汇3D场景理解以及文本驱动的3D生成。</p>
<p>本文在第五部分全面概述了这些方法，展示了3D+语言领域的全貌。然后，我们在第六部分列出了用于训练和评估这些方法的数据集。最后，第七部分强调了该领域的挑战和未来可能的研究方向。</p>
<p><br><br><br></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>本部分提供了关于3D表示、Large Language Models（LLMs）、2D Vision-Language Models（VLMs）和Vision Foundation Models（VFMs）的基本背景知识。</p>
<p><br><br></p>
<h3 id="数据的三维表示"><a href="#数据的三维表示" class="headerlink" title="数据的三维表示"></a>数据的三维表示</h3><p>选择一种3D表示形式来描述、建模和理解我们的世界，是一个至关重要的课题，它有助于理解3D-LLMs的当前进展。这也是计算机视觉中的一个基础研究领域。由于深度学习、计算资源和3D数据可用性的进步，该领域最近得到了显著增长。我们简要描述了目前使用的最常见的3D表示形式。</p>
<p><strong>点云</strong> 通过空间中的一组数据点表示3D形状，存储每个点在3D笛卡尔坐标系中的位置。除了存储位置外，还可以存储其他信息（例如颜色、法线）。基于点云的方法以其低存储占用而闻名，但缺乏表面拓扑信息。获取点云的典型来源包括LiDAR传感器、结构光扫描仪、飞行时间相机、立体视图和摄影测量等。</p>
<p><strong>体素网格</strong> 由3D空间中的单元立方体组成，类似于2D中的像素表示。每个体素至少编码占用信息（以二进制或概率形式），但也可以编码到表面的距离，例如签名距离函数（SDF）或截断签名距离函数（TSDF）。然而，当需要高分辨率细节时，内存占用可能会变得过高。</p>
<p><strong>多边形网格</strong> 由顶点和表面组成，以紧凑的方式描述复杂的3D形状。然而，其非结构化和不可微分的性质在将其与神经网络集成以实现端到端可微管道时提出了挑战。为解决这一问题，一些基于梯度近似的方法只能使用手工计算的梯度，其他解决方案如可微光栅化器可能导致渲染结果不精确，如内容模糊。</p>
<p><strong>神经场</strong> 在3D研究社区中逐渐受到关注，与依赖几何原语的传统表示形式不同。神经场是一种从空间坐标到场景属性（如占用、颜色、辐射等）的映射，与体素网格中从离散单元到该体素值的映射不同，神经场的映射是一个学习函数，通常是多层感知机（MLP）。这种方式使神经场能够隐式地学习紧凑、连续和可微的3D形状和场景表示。</p>
<p>一组神经场方法专注于隐式表面表示。占用网络通过神经网络编码形状，使用3D点位置和来自点云、低分辨率体素或图像的特征来估计占用概率。与此同时，Deep SDF网络使用神经网络从3D坐标和潜在向量估计SDF。最近的方法如NeuS显著提高了静态和动态物体表面重建的保真度和效率。</p>
<p>另一组方法称为Neural Radiance Fields（NeRF），在3D世界的逼真渲染能力方面表现出色。这些方法使用位置编码技术并利用MLPs预测沿摄像机光线的辐射值（颜色和不透明度）。然而，为每个空间采样点（包括空旷空间）推断颜色和占用细节需要大量计算资源，因此，减少NeRF的计算开销以实现实时应用是一个强烈的需求。</p>
<p><strong>混合表示</strong> 尝试将NeRF技术与传统的体积基础方法结合，促进高质量、实时渲染。例如，将体素网格或多分辨率哈希网格与神经网络结合，大大减少了NeRF的训练和推理时间。</p>
<p><strong>3D高斯溅射</strong> 是点云的一种变体，每个点包含代表该点周围空间区域发出的辐射的额外信息，作为各向异性3D高斯“斑点”。这些3D高斯通常从SfM点云初始化，并使用可微渲染进行优化。3D高斯溅射通过利用高效的光栅化而非光线追踪，实现了比NeRF计算量小得多的状态-of-the-art新视图合成。</p>
<p><br><br></p>
<h3 id="大语言模型"><a href="#大语言模型" class="headerlink" title="大语言模型"></a>大语言模型</h3><p>传统的自然语言处理（NLP）涵盖了广泛的任务，旨在使系统能够理解、生成和操作文本。早期的NLP方法依赖于基于规则的系统、统计模型和早期的神经架构，如循环神经网络。最近引入的大型语言模型（LLMs），采用transformer架构并在庞大的文本语料库上进行训练，达到了前所未有的性能，并在该领域引发了新的热潮。鉴于本文聚焦于3D LLMs，我们在此提供LLMs的相关背景知识。关于LLMs的深入探讨，请参阅该领域的最新综述。</p>
<p><br><br></p>
<h4 id="关于大语言模型架构"><a href="#关于大语言模型架构" class="headerlink" title="关于大语言模型架构"></a>关于大语言模型架构</h4><p>在LLMs的背景下，“编码器-解码器”和“仅解码器”架构在NLP任务中被广泛使用。</p>
<p><strong>编码器-解码器架构</strong> 由两个主要组件组成：编码器 $f_{\text {enc }}$ 和解码器 $f_{\text {dec }}$。编码器和解码器组件通常使用transformer实现，transformer利用注意力机制捕捉输入和输出序列中的长距离依赖关系。编码器接收输入序列 $X&#x3D;\left(x_1, x_2, \ldots, x_N\right)$，并将其映射为捕捉上下文信息的潜在表示序列 $H&#x3D;\left(h_1, h_2, \ldots, h_N\right)$。解码器则基于 $H$ 生成输出序列 $Y&#x3D;\left(y_1, y_2, \ldots, y_T\right)$。数学上，编码过程可以表示为 $H&#x3D;f_{\text {enc }}(X)$，整个潜在序列 $H$ 是从 $X$ 一次性生成的。而解码器则是按顺序生成输出序列 $Y$： $y_t&#x3D;f_{\text {dec }}\left(y_{&lt;t}, H\right)$，其中 $y_{&lt;t}&#x3D;\left(y_1, y_2, \ldots, y_{t-1}\right)$。</p>
<p><strong>仅解码器架构</strong> 是transformer架构的一种变体，仅使用解码器组件。它特别适合语言建模任务，其目标是根据前面的标记预测下一个标记。仅解码器架构在数学上可以表示为 $y_t&#x3D;f_{\text {dec }}\left(y_{&lt;t}\right)$。</p>
<p><br><br></p>
<h4 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h4><p>分词是一种预处理方法，将输入文本分解为一系列标记（tokens），这些标记是语言模型中的基本数据单位。标记的数量是有限的，每个标记可以对应一个单词、子词或单个字母。在推理过程中，输入文本被转换为标记序列并输入到模型中，模型预测输出标记，然后这些标记再转换回文本。分词对语言模型的性能有很大影响，因为它决定了模型如何感知文本。常用的分词技术包括词级分词、子词分词（如字节对编码、WordPiece、SentencePiece）和字符级分词。</p>
<p><br><br></p>
<h3 id="大型语言模型的涌现能力"><a href="#大型语言模型的涌现能力" class="headerlink" title="大型语言模型的涌现能力"></a>大型语言模型的涌现能力</h3><p>大型语言模型（LLM）与传统非LLM方法的一个主要区别在于大型模型中出现的涌现能力，而这些能力在小型模型中不存在。“涌现能力”指的是随着LLM规模和复杂性的增加而产生的新型复杂能力。这些能力使LLM能够高级地理解和生成自然语言，跨领域解决各种问题而无需特定训练，并通过上下文学习适应新任务。以下介绍LLM的一些常见涌现能力。</p>
<p><strong>上下文学习</strong> 是指LLM根据提示中的上下文理解和响应新任务或查询的能力，而无需显式再训练或微调。里程碑式的论文（如GPT-2&#x2F;GPT-3）展示了几次示例学习的方式，在提示中提供一些任务示例，然后要求模型处理不同的示例而无需事先明确训练。最新的LLM如GPT-4展现了卓越的上下文学习能力，能够理解复杂指令，并基于提示中的上下文执行从简单翻译到生成代码和创意写作的广泛任务。</p>
<p><strong>推理</strong> 在LLM的背景下，通常被称为“思维链提示”，涉及模型在处理复杂问题或问题时生成中间步骤或推理路径。这种方法允许LLM将任务分解为更小的可管理部分，促进更结构化和易于理解的解决过程。为了实现这一点，训练包括各种问题解决任务、逻辑谜题和模拟不确定性推理的数据集。当前最先进的LLM通常在模型参数超过60亿到1000亿时展示出高级推理能力。</p>
<p><strong>指令遵循</strong> 是指模型理解和执行用户指定命令或指令的能力。这包括解析指令，理解其意图，并生成适当的响应或动作。将这种能力适应新任务的方法可能需要从包含各种指令及其正确响应或动作的数据集中进行指令调优。监督学习、人类反馈强化学习和交互学习等技术可以进一步增强性能。</p>
<p><br><br></p>
<h3 id="大型语言模型的微调"><a href="#大型语言模型的微调" class="headerlink" title="大型语言模型的微调"></a>大型语言模型的微调</h3><p>在3D-LLMs的背景下，LLMs要么直接使用预训练状态，要么经过微调以适应新的多模态任务。然而，微调LLM的全部参数会带来巨大的计算和内存挑战，因为涉及的参数数量非常庞大。因此，参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）逐渐流行起来，通过更新模型的一小部分参数而不是重新训练整个模型来适应特定任务。以下列出了LLMs中常用的四种PEFT方法。</p>
<p><strong>低秩适配（Low-Rank Adaptation, LoRA）及其变体</strong> 通过低秩矩阵更新参数。在微调过程中，LoRA的前向传播可以表示为：$h&#x3D;W_0 x + B A x$。其中，$W_0$是LLM的冻结权重，$B A$是由新引入的矩阵$A$和$B$参数化的低秩矩阵，这些矩阵在微调阶段更新。这种方法有几个显著优势。在微调期间，仅优化$B$和$A$，大大减少了与梯度计算和参数更新相关的计算开销。一旦微调完成并且权重合并，与原始模型相比，不会有额外的推理成本，如方程所示：$h&#x3D;(W_0 + B A)x$。此外，不需要为不同任务保存多个LLM副本，可以保存多个LoRA实例，从而减少存储占用。</p>
<p><strong>层冻结</strong> 在训练过程中冻结预训练模型的选定层，同时更新其他层。这通常适用于模型输入或输出更接近的层，具体取决于任务的性质和模型架构。在3DLLM方法中，例如，除输入和输出嵌入层外的所有层可能会被冻结，以减少任务特定数据集的过拟合风险，保留预训练的通用知识并减少需要优化的参数。</p>
<p><strong>Prompt Tuning</strong>通过在提示中构建任务，引导大型语言模型（LLMs）执行特定任务，调整模型输入，而不是像传统微调那样调整模型参数。手动提示工程是最直观的方法，但即使对经验丰富的提示调优工程师来说，找到最佳提示也很困难。另一类方法是自动提示生成和优化。一种常见的方法是搜索精确的最优输入提示文本，称为硬提示。或者，可以使用优化方法来优化提示的嵌入（软提示）。</p>
<p><strong>自适应微调</strong>通过添加或移除层或模块来定制模型架构，以适应特定任务。这可能包括整合新的数据模态，如视觉信息和文本数据。自适应微调的核心思想是在预训练模型的层之间插入小型神经网络模块。在自适应微调期间，只更新这些适配器模块的参数，而原始模型的权重保持不变。</p>
<p><br><br></p>
<h3 id="关于二维视觉语言模型"><a href="#关于二维视觉语言模型" class="headerlink" title="关于二维视觉语言模型"></a>关于二维视觉语言模型</h3><p>视觉-语言模型（Vision-Language Models, VLMs）是一类旨在捕捉和利用文本与图像&#x2F;视频之间关系的模型，能够在这两种模态之间执行交互任务。大多数VLMs采用基于transformer的架构。<span style="background-color: brown;color:white">&nbsp;通过利用注意力模块，视觉和文本内容相互条件化，实现互相作用&nbsp;</span>。以下段落简要介绍了VLMs在判别任务和生成任务中的应用。</p>
<p><strong>判别任务</strong> 涉及预测数据的某些特征。VLMs，如CLIP和ALIGN，在图像分类的零样本可转移性方面表现出色。这两个模型包括两个模块：视觉编码器和文本编码器。给定一张图像及其类别，CLIP和ALIGN通过最大化图像嵌入与句子“a photo of a {image category}”的文本嵌入之间的相似性进行训练。在推理过程中，通过将“{image category}”替换为可能的候选项，并搜索与图像最佳匹配的句子，实现零样本转移性。这两项工作激发了许多后续研究，进一步提升了图像分类准确性。模型还可以将学习到的知识应用于其他任务，包括目标检测、图像分割、文档理解和视频识别。</p>
<p><strong>生成任务</strong> 利用VLMs从输入数据生成文本或图像。通过利用大规模训练数据，单个VLM通常能够执行多个图像到文本的生成任务，如图像描述和视觉问答。著名的例子包括SimVLM、BLIP和OFA等。更强大的VLMs，如BLIP-2、Flamingo和LLaVA，能够基于输入图像处理多轮对话和推理。随着扩散模型的引入，文本到图像生成也成为研究社区的焦点。通过在大量图像-文本对上训练，扩散模型能够根据文本输入生成高质量图像。这一能力还扩展到了生成视频、3D场景和动态3D物体。除了生成任务，还可以通过文本提示编辑现有图像。</p>
<p><br><br></p>
<h3 id="视觉基础模型"><a href="#视觉基础模型" class="headerlink" title="视觉基础模型"></a>视觉基础模型</h3><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>视觉基础模型（Vision Foundation Models, VFMs）是大型神经网络，旨在提取足够多样和表达能力强的图像表示，能够直接部署在各种下游任务中，类似于预训练的LLMs在下游NLP任务中所起的作用。</p>
<p>一个显著的例子是DINO，它使用自监督的教师-学生训练范式。所学习的表示在图像分类和语义图像匹配上都取得了良好效果。DINO中的注意力权重也可以作为观测场景中语义组件的分割掩码。后续的工作如iBOT和DINOv2通过引入掩码图像建模（MIM）损失进一步改进了表示。</p>
<p>SAM是一个基于transformer的图像分割模型，在包含11亿张带有语义掩码的图像数据集上训练，展示了强大的零样本迁移能力。DINO（Zhang等人）——不要与DINO（Caron等人）混淆——采用类似DETR的架构和混合查询选择进行目标检测。后续工作Grounding-DINO引入了文本监督以提高准确性。</p>
<p>Stable Diffusion，一个文本到图像生成器，通过对干净或人为噪声图像进行单次扩散步骤并提取中间特征或注意力掩码，被用作“真实”图像的特征提取器。这些特征最近被用于分割和图像匹配任务，原因在于扩散模型使用的训练集的规模和多样性，以及观察到的扩散特征的涌现属性，例如跨图像的零样本对应性。</p>
<p><br><br><br></p>
<h2 id="任务和指标介绍"><a href="#任务和指标介绍" class="headerlink" title="任务和指标介绍"></a>任务和指标介绍</h2><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<h3 id="任务-3D标注（3D-to-文本）"><a href="#任务-3D标注（3D-to-文本）" class="headerlink" title="任务 3D标注（3D to 文本）"></a>任务 3D标注（3D to 文本）</h3><p>在给定场景或物体的3D数据后，3D标注任务是生成相应的简短自然语言描述。根据标注的数据类型和生成的标注类型，我们将该任务分解为几种常见的变体。</p>
<p><strong>物体级标注</strong> 要求模型生成单个3D物体的简短自然语言描述。此描述应关注物体的关键特征，包括其形状和语义特征。</p>
<p><strong>场景级标注</strong> 是为整个3D场景生成简短自然语言描述。此类描述通常关注全局场景信息（如房间类型和风格）、场景中的关键物体及其关系。我们将“基础标注”视为场景标注的一种变体，其中模型输出场景中物体之间关系的描述，并可能附带这些物体的位置信息。</p>
<p><strong>3D密集标注</strong> 是在3D场景中定位物体实例并使用自然语言描述它们的联合任务。在这种情况下，输出还可能包含被标注物体的位置信息。通常，3D定位数据集中的参考描述用于生成3D密集标注所需的标注和位置信息。例如，Scan2Cap中的标注是使用ScanRefer中的参考表达生成的。</p>
<p>3D标注的评价指标需要将生成的标注与测试样本的真实标注进行比较。</p>
<p><strong>精确匹配（Exact Match, EM）</strong> 要求生成的标注与真实标注完全匹配。精确匹配有不同的准确性阈值，表示为EM@K，意思是正确答案在模型生成的前K个答案之内。常用的阈值是EM@1和EM@10。然而，具有相同语义的自然语言标注可以有多种表达方式，因此标注的主要指标是自动文本生成指标，这些指标旨在测量匹配的n元语法或语义相似性，而不是完整句子的匹配。</p>
<p><strong>BLEU</strong> 匹配预测标注和真实标注之间的n元语法，”BLEU@x”指的是匹配长度为”x”的n元语法（典型值在1-4范围内）。这仍然需要匹配确切的词，但对措辞的重排更为鲁棒。</p>
<p><strong>ROUGE</strong> 同样旨在匹配n元语法，常用的ROUGE-L关注句子的结构相似性。</p>
<p><strong>METEOR</strong> 基于单词匹配的精确度和召回率，“匹配”也存在于同义词和形态变体的单词之间。</p>
<p><strong>CIDEr</strong> 根据n元语法的频率加权，频率越高的n元语法权重越低。</p>
<p>由于上述指标依赖于n元语法匹配，它们无法处理语义相似但不同的词。因此，引入了各种衡量语义内容重叠的指标，如SentenceSim和BERT Score。</p>
<p>对于3D密集标注，其中标注定位到场景的部分，需调整基准。通常仍使用BLEU、ROUGE、METEOR和CIDEr分数，但如果预测边界框与物体的交并比（IoU）低于阈值”k”，则分数设为零。常用的”k”值为0.25和0.5。然而，这些指标关注标注的召回率而忽略了误报。这一点在最近的工作中得到解决，后者还测量生成标注相对于BLEU、ROUGE、METEOR和CIDEr指标的精确度和F1分数。</p>
<p><br><br></p>
<h3 id="任务-3D定位（3D加文本-to-3D位置）"><a href="#任务-3D定位（3D加文本-to-3D位置）" class="headerlink" title="任务 3D定位（3D加文本 to 3D位置）"></a>任务 3D定位（3D加文本 to 3D位置）</h3><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>在给定一个3D场景和一个描述场景中物体相对其他物体的“参考表达”后，3D定位涉及生成目标物体的位置、边界框或分割掩码。</p>
<p><strong>单物体定位</strong> 涉及在场景中定位单个查询物体，使用参考信息，如语言描述或附加手势。</p>
<p><strong>多物体定位</strong> 涉及使用参考表达定位多个物体。这种定位有两种主要变体。第一种涉及单句描述，这可能是模糊的，可能指代3D场景中同一类别的零个、一个或多个目标物体。第二种变体使用段落长度的参考表达，描述多个可能属于不同类别的物体及其之间的空间关系。</p>
<p>3D定位的评价指标需要将预测位置（通常以边界框的形式）与测试样本中的物体的真实位置进行比较。Acc@K IoU是3D视觉定位中广泛使用的指标，测量正预测的百分比，其交并比（IoU）与真实值大于阈值K，通常设为0.25或0.5。值得注意的是，一些数据集在不同场景中评估性能。例如，ScanRefer将数据集分为唯一&#x2F;多个&#x2F;总体部分。一些方法测量平均IoU，而其他方法测量边界框中心之间的平均距离。对于多物体定位，使用F1分数作为指标。首先，根据IoUs在预测和真实边界框之间进行一对一匹配，然后将IoUs高于阈值的对视为真正例。</p>
<p><br><br></p>
<h3 id="任务-3D对话（3D-加-文本-to-文本）"><a href="#任务-3D对话（3D-加-文本-to-文本）" class="headerlink" title="任务 3D对话（3D 加 文本 to 文本）"></a>任务 3D对话（3D 加 文本 to 文本）</h3><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>自然地，我们可以考虑在3D场景中进行提问的任务，无论是在单轮对话设置中还是在更自然的多轮对话设置中。</p>
<p><strong>3D问答（3D-QA）</strong> 是一个任务，要求模型在给定3D场景的情况下，生成用户提出问题的答案。问题的主题范围广泛，模型必须理解3D场景和问题才能生成正确的回答。问题包括简单任务，如确定物体的存在，以及更复杂的任务，如空间推理。由于有几个成熟的基准测试，而且基准中的大多数问题都是具有唯一答案的事实性问题，3D-QA是评估多任务模型能力的热门任务。</p>
<p><strong>3D情境问答（3D-SQA）</strong> 是3D-QA的一个特例。关键区别在于3D-QA要求模型从旁观者的角度回答问题，旁观者可以获取所有关于场景的信息，而3D-SQA则需要从预定义情境中玩家的角度回答问题。例如，3D-SQA可能会在“站在餐桌后面并面对餐桌”的情境下问“我面前有多少把椅子？”。</p>
<p><strong>3D对话</strong> 要求模型与用户进行连贯且自然的多轮对话，而不是单轮问答。例如，用户可能想了解一个房间，因此他们会不断地询问关于房间各部分的问题，而模型需要正确且连贯地回应。</p>
<p>评价指标涉及将模型的响应与测试样本的真实响应进行比较。对于3D-QA和3D-SQA，主要指标是精确匹配（Exact Match, EM），这意味着模型生成的答案必须与正确答案完全匹配。这是因为现有的3D-QA基准中的大多数问题都是事实性问题，只有一个明确的正确答案。</p>
<p>对于3D对话和任务规划，其答案不是唯一的，使用诸如BLEU、ROUGE、METEOR、CIDEr和SPICE等语义指标来评估生成响应与基准提供的参考答案之间的相似性。这些指标也用于3D-QA，特别是ScanQA基准，以在准确性之外测量语义相似性。</p>
<p><br><br></p>
<h3 id="任务-3D具身代理（3D-加-文本-to-行动）"><a href="#任务-3D具身代理（3D-加-文本-to-行动）" class="headerlink" title="任务 3D具身代理（3D 加 文本 to 行动）"></a>任务 3D具身代理（3D 加 文本 to 行动）</h3><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>考虑涉及与3D场景交互的任务，这些任务基于描述所需动作或目标的特定文本提示进行。</p>
<p><strong>3D任务规划</strong> 是指用户提供一个高层次目标，模型需要列出实现该目标的低层次步骤。例如，给定一个房间的3D场景，用户可能会询问如何清洁房间，模型需要提供详细的清洁步骤。</p>
<p><strong>3D导航</strong> 是指使3D代理（如机器人或虚拟角色）能够在3D空间中移动和定向。此任务涉及理解和解释3D环境，识别障碍物，并规划安全、高效的路径以到达指定目标。</p>
<p><strong>3D操作</strong> 是指3D代理与其环境中的物体进行物理交互的能力。这可以包括拾取和移动物体，甚至是更复杂的动作序列，如组装部件或开门。</p>
<p><strong>3D任务规划</strong> 的评价指标依赖于将模型的文本&#x2F;标记输出与测试样本的真实动作进行匹配。使用BLEU、ROUGE、METEOR、CIDEr和SPICE来评估生成响应与真实答案之间的相似性。</p>
<p><strong>3D导航</strong> 有两个主要指标来评估性能：</p>
<ol>
<li><strong>成功率（Success Rate, SR）</strong>：衡量3D代理是否在预定的距离阈值内到达目标位置。</li>
<li><strong>按路径长度加权的成功率（SPL）</strong>：计算为SR乘以真实路径长度与实际路径长度的比率，旨在反映模型实现目标的效率。其他指标包括Oracle成功率（OSR）、轨迹长度（TL）和目标进度（GP）。</li>
</ol>
<p>除了上述指标外，还需要考虑代理路径与语言指定路径的匹配程度（当使用语言指定详细路径时）。一个这样的指标是按标准化动态时间规整加权的成功率（SDTW），它结合了SR与代理路径和真实路径之间的差异。</p>
<p><strong>3D操作</strong> 的关键指标是成功率，定义为成功操作的次数除以任务样本总数。不同的数据集对于如何使用文本表示其动作有不同的约定，例如使用结构化输出、使用标准化数值评分或引入新标记。</p>
<p>以上讨论专注于3D-LLMs方法中使用的指标。建议读者参考Gu等人的工作以了解导航指标的总结。</p>
<p><br><br></p>
<h3 id="任务-文本到3D生成（Text-to-3D）"><a href="#任务-文本到3D生成（Text-to-3D）" class="headerlink" title="任务 文本到3D生成（Text to 3D）"></a>任务 文本到3D生成（Text to 3D）</h3><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>除了使用文本描述和与现有3D场景进行交互，还可以通过语言描述生成3D物体和场景。这里我们简要总结这一领域，详细综述参见Lee等人的工作。</p>
<p><strong>3D物体生成</strong> 涉及根据文本描述生成单个物体的3D模型。文本输入可以提供关于物体类别、属性、部件结构和其他应反映在生成3D形状中的特性。</p>
<p><strong>3D场景生成</strong> 是根据文本场景描述创建完整3D环境的任务，如房间或户外空间。这涉及生成文本中指定物体的3D模型，并根据文本中指定的约束（如物体类别、数量、空间关系和场景属性）智能地排列和组合多个3D物体模型。</p>
<p><strong>3D编辑</strong> 指的是根据文本指令修改现有3D资产，如形状或场景。这可能涉及添加、移除或变换物体，改变材料或颜色，或根据给定文本改变高层次的场景属性。</p>
<p>3D生成任务的评价指标评估生成形状&#x2F;场景的质量及其与输入文本的匹配程度。常用的几何生成质量衡量指标包括Chamfer Distance（CD）和Mesh-Volume&#x2F;Surface Distance（MVD）。CD通过对与真实3D数据的点对点距离平方求和计算，而MVD则通过计算两个网格之间的体积&#x2F;表面距离来衡量几何误差。</p>
<p>为了评估整体质量，分类准确性检查语义属性是否被保留，而Frechet Inception Distance（FID）则捕捉真实感和多样性。为了检查生成形状是否与输入文本匹配，通常测量文本与3D形状对齐嵌入（如ULIP）或渲染图像（如CLIP）的相似性。人类研究也常用于评估。然而，最近的研究表明，使用类似GPT-4的LVLMs可以作为人类评判的替代方案。</p>
<p>对于基于文本的3D编辑，CD和IoU评估指令编辑在输入几何体上应用的效果，确保没有过度变形。</p>
<p><br><br><br></p>
<h2 id="关于3D-TASKS-WITH-LLMS"><a href="#关于3D-TASKS-WITH-LLMS" class="headerlink" title="关于3D TASKS WITH LLMS"></a>关于3D TASKS WITH LLMS</h2><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>3D场景理解任务已被广泛研究。其核心在于识别和分类指定3D环境内的所有物体，即语义理解或实例级理解。这一阶段至关重要，因为它为更细致的解读奠定了基础。随后，场景理解的更高层次集中于空间理解，即构建空间场景图和对象关系的语义。此外，还可以预测潜在的交互，例如可供性、场景变化以及理解场景的更广泛背景，如功能和美学风格。3D数据还带来一些2D数据所没有的独特挑战，如获取和标注3D数据的相对高成本、稀疏的3D数据结构以及需要协调同一物体的多个（可能被遮挡的）视角。为此，研究人员利用语言的力量，将3D世界的语义和关系嵌入其中。</p>
<p>最近将大型语言模型（LLMs）与3D数据集成的努力显示出在实现多层次理解和交互方面的潜力，利用LLMs固有的优势，如零样本学习、上下文学习、逐步推理和广泛的世界知识。</p>
<p>在第4.1节以及图2中，我们简要描述LLMs如何处理3D场景信息，重点介绍如何将3D特征与语言对齐，以便通过LLMs解释和推理这些信息，这是后续部分的基础。本节其余部分按照图3中呈现的分类结构展开，描述LLMs在解决3D任务中所扮演的角色。我们首先在第4.2节展示LLMs的世界知识（有时称为“常识知识”）和推理能力如何增强3D任务的表现。第4.3节详细介绍如何将多个3D任务整合到一个LLM中以实现多任务学习。第4.4节探讨如何使用LLMs作为统一界面结合其他模态。随后在第4.5节中描述LLMs如何作为具身代理与3D世界进行交互。最后，第4.6节展示LLMs如何作为助手生成语义多样的3D物体和场景。此外，我们提供表1，以对比3D-LLMs方法在三个轴上的差异：3D组件、LLMs组件以及3D视觉与语言的对齐，旨在提供对这一不断发展的领域的高层次见解。</p>
<p><br><br></p>
<h3 id="关于LLMs如何处理3D场景信息？"><a href="#关于LLMs如何处理3D场景信息？" class="headerlink" title="关于LLMs如何处理3D场景信息？"></a>关于LLMs如何处理3D场景信息？</h3><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>传统LLMs仅限于文本作为输入和输出，使得能够接收3D信息成为所有3D-LLM方法的首要关注点。总体思路是将3D物体或场景信息映射到语言空间，使LLMs能够理解和处理这些3D输入。具体而言，通常包括两个步骤：（i）使用预训练的3D编码器处理相应的3D表示，生成原始3D特征；（ii）采用对齐模块将这些3D特征转换为LLMs可以处理的3D标记，类似于第2.2.1节中提到的标记化过程。预训练的LLMs随后可以在生成输出时使用这些对齐的3D标记。</p>
<p>鉴于3D表示的多样性，如第2.1节所述，有多种方法可获取3D特征。如表1的3D几何列所示，点云因其简单性和与各种预训练3D编码器的兼容性最为常见，这使其成为多任务和多模态学习方法的热门选择。多视图图像也经常使用，因为2D特征提取的研究已很成熟，这意味着3D特征提取仅需要一个额外的2D到3D提升方案。通过深度摄像头获取的RGB-D数据通常用于3D具身代理系统，以提取导航和理解所需的视点相关信息。3D场景图是一种更抽象的3D表示，在建模物体的存在及其关系以及捕捉场景的高层次信息方面表现优越，常用于3D场景分类和规划任务。NeRFs在3D-LLM方法中使用较少，我们认为这是由于其隐式性质，使其更难以标记和与前馈神经网络集成。</p>
<p>当前方法使用不同的架构和模块来将3D特征与LLM输入空间对齐。如表1的3D+LLM列所示。对于仅接受3D输入的模型（图2a），使用线性层或MLP作为对齐模块将3D特征转换为LLM输入空间。接受3D和文本输入的模型通常使用两个独立的分支来对齐3D特征和文本（图2b）。一些工作采用单层vanilla transformer，使3D物体特征在对齐过程中相互关注。其他工作创建基于transformer的对齐模块，调整标准transformer架构以更好地适应不同类型的3D数据，如密集点云和稀疏LiDAR扫描。文本则使用预先存在的LLM文本嵌入表进行编码。</p>
<p>其他工作遵循Q-Former风格的方法，引入固定长度的查询标记作为额外输入，并采用基于BERT的结构，以促进对齐过程中3D和文本特征之间的交互。大多数上述三种类型的架构通过利用3D标注数据集实现对齐，其中使用标注损失，即LLMs生成的标题与场景简要描述的交叉熵损失，微调对齐模块，同时冻结预训练的3D特征提取器和LLM。</p>
<p>最后，一些模型使用闭源模型如ChatGPT，根本不训练对齐模块。相反，将3D数据直接生成文本描述，如描述3D边界框、位置和关系，或使用现有标题。这些文本描述输入到ChatGPT。这些工作中未提出额外的对齐模块，因此不需要训练。</p>
<p><br><br></p>
<h3 id="使用LLMs提升3D任务性能"><a href="#使用LLMs提升3D任务性能" class="headerlink" title="使用LLMs提升3D任务性能"></a>使用LLMs提升3D任务性能</h3><p>经过大量数据训练的LLMs已被证明能够获得关于世界的常识知识。LLMs的世界知识和推理能力在提升3D场景理解和重构若干3D任务流程方面展现了潜力。本节重点介绍利用LLMs提升现有3D视觉语言任务性能的方法。</p>
<p>在应用LLMs于3D任务时，可以将其用法分为两大类：知识增强和推理增强。知识增强方法利用LLMs中嵌入的广泛世界知识来提升3D任务性能。这可能提供上下文见解，填补知识空白，或增强3D环境的语义理解。推理增强方法则依赖LLMs逐步推理的能力，从而更好地推广解决更复杂的3D挑战。以下两个小节分别描述这些方法。</p>
<p><br><br></p>
<h4 id="知识增强方法"><a href="#知识增强方法" class="headerlink" title="知识增强方法"></a>知识增强方法</h4><p>有几种方法利用LLMs的世界知识。Chen等人使用LLMs从RGB-D图像进行3D房间分类。这里，LLMs中嵌入的知识用于根据房间内的物体类别信息确定房间类别。首先，该方法从Matterport3D数据创建场景图，节点代表区域和物体，物体节点链接到房间节点。然后，选择关键物体形成每种房间类型的查询。LLMs对选定物体提取的描述进行评分，得分最高的预测房间标签。还可以提供大小或位置等空间信息。</p>
<p>ViewRefer使用LLMs扩展带有视图相关描述的定位文本。例如，给定原始文本“面向沙发前面，右边的桌子”，LLMs生成另一个视角的相似句子，如“背对沙发前面，选择左边的桌子”。通过多次改写输入文本及其相对视角的同义词，模型改进了跨视角定位。还采用了带有视角间注意力的融合transformer，并包括可学习的多视角原型，以捕捉视角间知识，进一步提升3D定位性能。</p>
<p>Abdelreheem等人解决3D形状中的语义对应问题。他们通过将渲染视图输入BLIP2模型生成类别建议列表来分类3D形状。ChatGPT将这些统一为每个形状的单一类别，还生成语义部分名称和成对映射（例如，手臂→翼）。然后，一个3D分割器基于语义区域分割形状，利用部分映射生成稀疏对应图。</p>
<p>上述知识增强策略在零样本场景中表现出色，尤其是当没有针对特定物体或场景类型的标注3D数据时。这允许关于物体部分、关系和语义的开放性推理，如Chen等人生成空间和语义物体描述，ViewRefer描述多视角物体关系，Abdelreheem等人跨形状生成和匹配物体部分语义。</p>
<p><br><br></p>
<h4 id="推理增强方法"><a href="#推理增强方法" class="headerlink" title="推理增强方法"></a>推理增强方法</h4><p>除了世界知识，LLMs的推理能力也有助于解决其他3D任务，特别是在具有详细几何和多个物体的复杂3D场景中的视觉定位。在这种情况下，物体的文本描述应包括它们的外观和与周围物体的空间关系。普通的定位方法在这种设置中往往表现不佳，因为它们无法理解详细的文本描述。</p>
<p>LLM-Grounder、Transcribe3D和零样本3DVG通过利用LLMs的推理能力分析文本描述，并生成定位物体的指令序列来解决这个问题。具体而言，LLM首先从文本描述中识别锚点和目标物体。然后，它基于定位工具返回的坐标分析多个候选物体之间的空间关系（或描述的属性），选择与文本描述最匹配的候选物体。</p>
<p>此外，Transcribe3D和LLM-Grounder采用多轮互动问答过程帮助用户澄清意图，促使他们提供更多信息以获得更准确的结果。LLM-Grounder包含多种定位工具选择，如OpenScene或LERF，以适应不同的3D表示，如点云或NeRF。这些方法的一个常见缺点是LLM的“盲目性”，因为它只提供3D场景的抽象文本描述，而不是场景的原始点云。这可能导致重要场景细节的丢失。因此，当3D场景包含多个同类物体时，缺乏必要的场景细节意味着无法解决基于文本的引用中的歧义，从而限制整体性能。</p>
<p>除了视觉定位，LLMs的推理能力也促进了其他任务。3DAP利用GPT-4V从2D图像推断物体的3D信息，使用视觉提示技术为输入图像注释3D轴，以增强LLM的3D尺度感知。ConceptFusion使用GPT-3生成指令，利用预定义的基本空间比较模块，使其提议的3D特征图能够进行更复杂的空间推理。</p>
<p><br><br></p>
<h3 id="使用LLMs进行3D多任务学习"><a href="#使用LLMs进行3D多任务学习" class="headerlink" title="使用LLMs进行3D多任务学习"></a>使用LLMs进行3D多任务学习</h3><p>许多研究集中在利用LLMs的指令跟随和上下文学习能力，将多个3D任务统一到单一的语言空间中。通过使用不同的文本提示来表示不同的任务，这些研究旨在让LLMs作为统一的对话界面。实现多任务学习通常包括几个关键步骤，首先是构建3D-文本数据对。这些数据对需要设计任务指令的文本形式，并定义每个不同任务的输出。</p>
<p>接下来，将3D数据（通常是点云）输入3D编码器以提取3D特征。对齐模块随后用于：(i) 在多个层次上（对象层次、关系层次和场景层次）将3D特征与LLMs的文本嵌入对齐；(ii) 将3D特征转化为LLMs可解释的tokens。最后，需要选择合适的训练策略，如单阶段或多阶段的3D-语言对齐训练和多任务指令微调。</p>
<p>在本节的其余部分，我们详细探讨这些方面。此外，我们在表2中总结了本节回顾的每种方法的范围和能力。</p>
<p><br><br></p>
<h4 id="多任务学习的数据"><a href="#多任务学习的数据" class="headerlink" title="多任务学习的数据"></a>多任务学习的数据</h4><p>如表2所示，我们将任务分为四类：描述、定位、问答（QA）和具身代理任务（如规划、导航和操作）。相应地，每个任务的文本输出遵循预定义格式。对于描述和问答任务，输出是纯文本，不受特定格式约束。定位任务的输出是一个3D边界框，通常是所指物体中心的坐标及其3D尺寸。通常，点和大小的值被标准化到0-255范围内，以限制LLMs需要预测的token范围。</p>
<p>对于规划，模型输出一个以文本形式执行任务的步骤序列；而对于导航，输出是一系列空间坐标；对于操作，输出是以文本形式的动作序列。现有方法遵循这些指南构建多任务指令微调数据集。</p>
<p>一旦确定了文本格式，不同的方法使用不同的策略获取数据集的文本注释。几种方法利用人工标注生成每个样本的“真实”注释，然而这可能是一个昂贵且耗时的过程。另一种方法是使用ChatGPT为每个样本生成文本注释，这是3DMIT、LiDAR-LLM、Chat-3D和Chat-3D v2使用的策略。在这里，3D场景数据被转换为文本（通常通过描述物体边界框和空间关系），并创建任务描述以描述所需输出。</p>
<p>为了引导ChatGPT生成预期的任务输出格式，提供了示例，这允许ChatGPT进行上下文学习，为其他3D场景生成合理的文本注释。或者，其他多任务数据集仅通过合并现有的3D视觉语言（VL）数据集来构建。一些多任务数据集结合了这三种方法，旨在结合人工注释的准确性和使用LLM生成注释的可扩展性。</p>
<p><br><br></p>
<h4 id="多任务3D模型的训练"><a href="#多任务3D模型的训练" class="headerlink" title="多任务3D模型的训练"></a>多任务3D模型的训练</h4><p>训练LLMs以处理多个3D任务的第一步是获取有意义的3D特征，提取方法因3D场景类型而异。对于单个物体点云，Point-LLM、Chat-3D和GPT4Point使用Point-BERT提取3D物体特征。对于室内场景，LEO使用PointNet++进行特征提取，而Chat-3D v2和3DMIT则分割场景，并使用Uni-3D提取每个分割部分的特征。同时，MultiPLY将提取的物体特征整合到场景图中，以表示整个场景。</p>
<p>3D-LLM和Scene-LLM从2D多视图图像中提取特征转换为3D表示。3D-LLM从Mask2Former或SAM中提取2D语义特征。Scene-LLM遵循ConceptFusion的方法，融合全局信息和局部细节，将逐像素的CLIP特征映射到逐点的3D特征。对于户外3D场景，LiDAR-LLM使用VoxelNet提取3D体素特征。</p>
<p>对于对齐模块，如第4.1节所述，使用了各种网络架构。值得注意的是，MultiPLY [24] 使用不同的线性层来对齐每种模态的特征。Chat-3D [171] 和 Chat-3D v2 [172] 采用单层vanilla transformer，以允许3D对象特征在对齐过程中彼此关注。LEO [270] 和 LiDAR-LLM [271] 修改了transformers作为对齐模块，以更好地适应不同类型的3D数据（密集点云与稀疏LiDAR）。LEO [270] 修改了自注意力机制，明确编码点云中对象对之间的空间关系。相反，LiDAR-LLM [271] 使用自注意力和交叉注意力机制来对齐鸟瞰视图（BEV）特征与文本特征。3D-LLM [153] 和 GPT4Point [268] 采用了Q-Former，而LL3DA [19] 在Q-Former基础上增加了一个分支，允许查询令牌与用户提供的视觉提示交互。</p>
<p>LLMs 可以通过不同策略进行微调以结合多种3D任务，这在第2.2.3节中有讨论。LEO [270] 和 3DMIT [269] 使用低秩适应（LoRA）进行微调。因此，包括对齐模块和3D编码器在内的可训练参数总量不到原始LLMs参数的10%，显著提高了训练效率。Chat-3D [171]，LL3DA [19]，Chat-3D v2 [172]，LiDAR-LLM [271] 和 MultiPLY [24] 采用自适应微调。具体来说，这些模型包括模块以对齐3D场景中的空间信息与语言，例如transformer层，以捕捉对象关系。这些模块连同预训练的3D编码器和LLMs一起进行微调以实现对齐。3D-LLM [153]，Scene-LLM [266]，Point-LLM [267] 和 GPT4Point [268] 采用层冻结策略。通过冻结大多数LLM层并微调某些层如嵌入层，这种策略在保留语言能力的同时提高了3D理解能力。最后，Agent3D-Zero [17] 使用提示微调，这是一种不需要训练的方法，用于指导LLMs理解3D任务。该方法使用定制提示，在3D场景的BEV图像上添加网格线和刻度线，帮助2D VLMs理解3D几何。</p>
<p>训练这些模型以进行3D多任务学习还涉及3D-语言特征对齐的微调。Point-LLM [267]，3D-LLM [153]，Scene-LLM [266]，LEO [270] 和 GPT4Point [268] 都采用单阶段对齐方法。具体来说，Point-LLM [267] 仅使用字幕数据训练一个MLP，并另外更新输入嵌入层以适应新添加的标记点云令牌的起始和结束标记（⟨p start⟩，⟨p end⟩）。3D-LLM [153] 使用自定义数据集训练对齐模块，并更新输入和输出嵌入层以适应新添加的位置令牌。Scene-LLM [266] 仅训练一个线性层，使LLMs能够通过相机和世界坐标系中的3D框架语言对字幕任务理解自我中心和场景中心视角。它还更新输入嵌入层以适应新添加的3D令牌的起始和结束标记（⟨3D⟩，⟨&#x2F;3D⟩）。LEO [270] 也使用字幕任务训练对齐模块，但独特地收集了三种类型的字幕数据：对象级[293]，场景中的对象[228, 294] 和场景级[295]，并用这三种数据集训练其对齐模块。GPT4Point [268] 遵循BLIP2 [195] 的结构和训练策略，通过三个任务实现对齐：点文本对比（PTC），点文本匹配（PTM），和点字幕生成（PTG）。</p>
<p>与这些单阶段对齐方法相反，LiDAR-LLM [271]，Chat-3D [171] 和 Chat-3D v2 [172] 都采用两阶段3D-语言对齐过程。LiDAR-LLM [271] 通过3D字幕任务分两个阶段增强局部和全局场景感知：首先集中于单视图字幕，然后扩展到全景场景描述。他们通过结合字幕和对齐任务开发实例级感知能力。Chat-3D [171] 首先使用3D对象分类数据集[296, 293, 297] 对齐3D对象与文本，旨在通过仅更新对齐模块来最大化映射3D对象特征与对象类别词嵌入之间的余弦相似度。在场景级对齐的第二阶段，它使用ScanRefer [218] 以实现字幕能力，特别更新一个transformer层来建模对象的空间关系。同样地，Chat-3D v2 [172] 综合对象级和场景级对齐，在第二阶段另外训练一个位置嵌入层。为了提高训练效率，LL3DA [19] 和 3DMIT [269] 跳过对齐阶段，专注于以下所述的指令微调阶段。</p>
<p>几乎所有的多任务学习方法最终都需要根据指令完成各种3D任务。因此，作为训练的最后阶段，每种方法通常使用它们自己构建的多任务指令跟随数据集进行指令微调。由于所有任务输出都统一为文本形式，训练损失使用的是LLMs的标准自回归损失。这一阶段通常涉及对齐模块和LLM的联合训练。一个例外是Agent3D-Zero [17]，它通过向GPT4V提供来自不同视点的2D图像完成各种3D任务，因此不需要任何训练。</p>
<p><br><br></p>
<h3 id="LLMs作为3D多模态接口"><a href="#LLMs作为3D多模态接口" class="headerlink" title="LLMs作为3D多模态接口"></a>LLMs作为3D多模态接口</h3><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>除了探索3D多任务学习者，一些最新研究还整合了不同模态的信息，以进一步提高模型的能力并实现新颖的交互。除了文本和3D场景，3D多模态LLMs还可能包括2D图像、音频或触觉信息作为输入。大多数工作旨在构建一个跨不同模态的共同表示空间。由于一些现有工作已经提供了将文本、图像或音频映射到共同空间的预训练编码器，一些工作选择学习一个3D编码器，将3D嵌入对齐到其他模态预训练编码器的嵌入空间。JM3D-LLM [279] 学习一个3D点云编码器，将点云的嵌入空间对齐到SLIP [301] 的文本-图像嵌入空间。它在训练期间渲染一系列点云图像，并构建一个层次化的文本树，以实现详细对齐。Point-Bind [272] 也学习了一个类似的3D编码器，并将其对齐到ImageBind [302]，以统一图像、文本、音频和点云的嵌入空间。这使得可以使用不同的任务头处理不同模态之间的任务，如检索、分类和生成。然而，这种方法的一个显著限制是，由于计算成本高，3D编码器只能处理小规模对象级场景，而不能处理包含数百万点的大场景。此外，大多数预训练多模态编码器如CLIP设计用于单对象场景，不适用于包含多个对象和局部细节的大场景。</p>
<p>大场景需要更细致的设计以整合多种模态。ConceptFusion [18] 构建了一个增强的特征图，融合每个组成图像的全局信息和局部细节。通过使用已对齐不同模态（包括文本和音频）的预训练特征提取器实现这一目标。然后使用传统SLAM方法将特征图映射到场景的点云。MultiPLY [24] 采用类似于ConceptGraph [290] 的表示方法。它识别场景中的所有显著对象，获取每个对象的全局嵌入，最终构建一个场景图。结果表示是与Llama [140] 嵌入空间对齐的场景嵌入。包括音频、温度和触觉在内的其他模态嵌入也可以通过线性投影映射到同一空间。所有嵌入被标记并一次性发送到LLM。与对象级场景方法相比，可以处理大场景的方法通过依赖预训练编码器弥合模态差距，而不是从头学习新的编码器，从而降低成本。</p>
<p><br><br></p>
<h3 id="LLMs用于具身智能体"><a href="#LLMs用于具身智能体" class="headerlink" title="LLMs用于具身智能体"></a>LLMs用于具身智能体</h3><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>LLMs的规划、工具使用和决策能力可以用于创建3D具身智能体。这些能力使LLMs能够在3D环境中进行导航，与物体交互，并选择适当的工具来执行特定任务。本节描述了3D具身智能体在规划、导航和操作任务中的表现。</p>
<p><br><br></p>
<h4 id="3D任务规划"><a href="#3D任务规划" class="headerlink" title="3D任务规划"></a>3D任务规划</h4><p>对于具身智能体，‘任务规划’是指在给定任务描述和3D环境的情况下生成执行特定任务的步骤。任务规划通常是导航和操作任务的先决条件，因为规划的准确性直接影响后续任务的性能。LEO [270] 和 LLM-Planner [12] 利用LLMs生成逐步计划，并根据环境感知动态调整。LEO [270] 强调基于当前场景配置的场景感知规划，而LLM-Planner [12] 采用GPT-3将规划分为高层次的子目标和低层次的动作，并在任务执行过程中遇到障碍时重新规划。3D-VLA [276] 通过生成世界模型整合3D感知、推理和行动。它通过使用其生成模型预测未来状态表示（例如目标图像和点云）来增强规划能力。Agent3D-Zero [17] 引入了Set-of-Line Prompting（SoLP），通过生成多样化的观察视点增强VLM对场景几何方面的理解。具体来说，SoLP在BEV图像上叠加网格线和刻度线，并提示VLM提供更准确的相机位置和方向，从而使VLM能够理解3D空间概念。UniHSI [277] 解决了人与场景交互（HSI）任务，涉及基于输入语言命令在3D环境中生成人与物体之间的交互。它使用LLM作为规划器，将语言命令翻译为任务计划，表示为接触链（CoC），即表示人体关节点与物体位置之间时间关系的序列。</p>
<p>虽然上述方法主要关注单个场景内的规划，SayPlan [275] 可以处理多个房间和楼层，通过使用3D场景图进行语义搜索和将经典路径规划与迭代重规划管道结合以细化计划。</p>
<p><br><br></p>
<h4 id="3D导航"><a href="#3D导航" class="headerlink" title="3D导航"></a>3D导航</h4><p>3D导航是指具身智能体在3D环境中移动和定向的能力，通常基于视觉输入和语言指令。每种方法——LEO [270]，Agent3D-Zero [17]，LLM-Planner [12] 和 NaviLLM [11]——都以不同方式实现3D导航。LEO [270] 处理自我中心的2D图像和以对象为中心的3D点云以及文本指令。它生成对应于可执行导航命令（如‘向前移动’或‘向右转’）的动作令牌序列。LEO采用‘最短路径导航试验’，提供比人类演示更少噪音和更直接的学习环境。Agent3D-Zero [17] 通过持续选择基于环境评估的新视点进行导航。它结合历史视点数据，以优化导航路径达到特定目标，如在办公室环境中找到打印机。LLM-Planner [12] 采用分层方法，首先生成高层次计划作为子目标序列，然后由低层次规划器翻译为一系列原始动作。这使整个过程能够适应当前环境。NaviLLM [11] 使用基于方案的指令将各种具身导航任务转化为生成问题。这些指令包括4个要素：由词序列定义的任务、所有可达视点的观察、过去视觉观察的历史记录和指导动作生成的输出提示（例如选择方向或对象）。</p>
<p><br><br></p>
<h4 id="3D物体操作"><a href="#3D物体操作" class="headerlink" title="3D物体操作"></a>3D物体操作</h4><p>在3D具身智能体的背景下，操作指的是它们与物体物理交互的能力，从移动物体到复杂的操作序列，如组装零件或开门。使LLMs能够执行操作任务的核心思想在于将动作序列标记化。为了让LLMs输出特定动作，首先需要定义动作令牌，使LLMs能够根据任务和3D场景上下文生成这些动作。随后，CLIPort [242] 或机器人手臂中的运动规划模块等平台将这些标记化动作翻译为智能体执行的物理动作。</p>
<p>LEO [270]，MultiPLY [24] 和 3D-VLA [276] 各自使用不同的动作令牌，将口头或书面指令转换为机器人在3D空间中的动作。LEO [270] 使用超过500个特定令牌使机器人动作更加精确。具体来说，对于CLIPort [242] 任务，动作姿态使用516个令牌进行编码：320个令牌用于x轴姿态区间，160个令牌用于y轴，36个令牌用于z轴旋转区间。MultiPLY [24] 引入了如⟨SELECT⟩用于物体交互，⟨NAVIGATE⟩用于移动，⟨OBSERVE⟩用于观察，⟨TOUCH⟩用于触觉反馈，⟨HIT⟩用于听觉反馈，⟨PICK-UP⟩和⟨PUT-DOWN⟩用于操作，⟨LOOK-AROUND⟩用于感知的令牌。这种方法还集成了感官反馈（触觉、温度和听觉），增强了机器人与周围环境的交互。3D-VLA [276] 包含(i) 对象令牌（⟨obj⟩⟨&#x2F;obj⟩）用于识别操作对象，(ii) 位置令牌（⟨loc0-255⟩）用于空间定位，以及(iii) 专门的机器人动作令牌，如手臂位置&#x2F;旋转&#x2F;夹持器状态。令牌结构通过⟨ACT SEP⟩分隔，便于理解和执行复杂3D操作。</p>
<p>虽然这些系统通过将指令映射到动作来使机器人执行复杂任务，但它们忽略了可操作物体的语义理解，通常无法区分适合与不适合操作的部分。为解决这一问题，VoxPoser [13]，LAN-grasp [14] 和 ManipLLM [15] 关注“可操作性”，创建可操作性地图以表示可用于执行特定任务的物体和特征，如可抓取的把手、可按压的按钮或可移动的物体。具体来说，VoxPoser [13] 使用LLM分解自由形式的语言指令，推断可操作性和约束条件，并通过使用代码接口与VLM交互生成3D体素地图。这些地图可以生成对动态变化具有鲁棒性的闭环机器人轨迹，能够在接触丰富的环境中从在线经验中学习。LAN-grasp [14] 通过结合多个模型识别可抓取部分，利用基础模型加深机器人对物体的语义理解，无需重新训练。ManipLLM [15] 通过从文本提示、RGB图像和深度图中识别接触点和夹持器方向的3D坐标来预测操作结果。</p>
<p><br><br></p>
<h3 id="LLMs用于3D生成"><a href="#LLMs用于3D生成" class="headerlink" title="LLMs用于3D生成"></a>LLMs用于3D生成</h3><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>传统上，3D建模是一个复杂且耗时的过程，需要详细关注几何形状、纹理和光照以实现逼真的效果。本节探讨了LLMs与3D生成技术的整合，展示了语言如何提供生成情境化对象的方式，并为3D内容创建和操控提供创新解决方案。</p>
<p><br><br></p>
<h4 id="对象级生成"><a href="#对象级生成" class="headerlink" title="对象级生成"></a>对象级生成</h4><p>Shape-GPT使用特定形状的3D VQ-VAE将3D形状量化为离散的“形状词”令牌。这使得形状数据可以与文本和图像一起整合到T5语言模型的多模态输入中。这种多模态表示使T5能够学习跨模态交互，例如文本到形状的生成和形状编辑&#x2F;补全。GPT4Point采用双流方法，通过Point-QFormer将点云几何与文本对齐，然后将其输入耦合的LLM和扩散路径，以理解文本并生成高保真度的符合文本输入的3D对象。</p>
<p>相比之下，MeshGPT和PolyGen不依赖文本生成，但它们仍采用类似于LLM中序列建模的自回归方法。MeshGPT使用图卷积将网格几何&#x2F;拓扑编码为丰富的嵌入，通过残差向量量化压缩，然后输入GPT样式的变压器，以自回归方式预测生成具有期望属性的网格的令牌&#x2F;嵌入。PolyGen是一种基于自回归变压器的3D网格模型，利用指针网络。它包括一个无条件建模网格顶点的顶点模型和一个基于输入顶点条件建模网格面的面模型，使用自回归网络输出面索引和顶点坐标，以生成多样化的高质量网格。</p>
<p><br><br></p>
<h4 id="场景级生成"><a href="#场景级生成" class="headerlink" title="场景级生成"></a>场景级生成</h4><p>Holodeck和GALA-3D采用多阶段管道，从文本逐步细化初始粗略的3D场景布局为详细的逼真3D环境。Holodeck利用专门的模块创建基本布局，选择材料，并根据GPT-4的空间推理和布局&#x2F;风格建议融入门窗等元素。然后根据GPT-4的文本描述用Objaverse资产填充布局。一个优化器安排这些对象，遵循从GPT-4获得的空间关系约束，确保真实的对象布局和交互。</p>
<p>GALA-3D首先使用LLM从文本生成粗略布局，然后将其转化为3D高斯表示。此表示作为创建详细3D内容的基础，使用实例级文本到图像扩散先验。它采用组合优化来微调布局引导的高斯参数，确保最终场景在对象放置、规模和交互方面与文本对齐。</p>
<p>两者都利用LLMs的互补优势提取高层语义布局，并使用生成模型&#x2F;优化将这些布局转化为几何和物理上合理的3D场景。</p>
<p><br><br></p>
<h4 id="程序生成与操控"><a href="#程序生成与操控" class="headerlink" title="程序生成与操控"></a>程序生成与操控</h4><p>LLMR、3D-GPT和SceneCraft采用模块化架构，具有专门的组件&#x2F;代理用于交互式3D世界创建和从自然语言生成代码。LLMR包括用于生成Unity场景代码的不同组件，理解现有场景对象和属性以实现修改，识别执行指令所需的功能，并评估最终代码质量。同样，3D-GPT包含解释指令和确定所需生成功能的组件，丰富描述以包含详细的建模属性，并将丰富的描述翻译为Blender API的Python代码。总体而言，这些方法展示了LLM组件的任务分解和专业化，以处理指令解释、功能映射和强大的代码生成。</p>
<p><br><br><br></p>
<h2 id="视觉语言模型与三维任务系列-3D-TASKS-WITH-VLMS"><a href="#视觉语言模型与三维任务系列-3D-TASKS-WITH-VLMS" class="headerlink" title="视觉语言模型与三维任务系列(3D TASKS WITH VLMS)"></a>视觉语言模型与三维任务系列(3D TASKS WITH VLMS)</h2><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<h3 id="利用VLMs进行3D任务"><a href="#利用VLMs进行3D任务" class="headerlink" title="利用VLMs进行3D任务"></a>利用VLMs进行3D任务</h3><p>虽然第4节讨论了将LLMs整合到3D任务中的方法，但大量研究已经通过2D视觉-语言模型（VLMs）的视角探索了3D理解的各个方面。VLMs包含更丰富的视觉信息，可以直接与3D关联。本节回顾了最近一系列论文的贡献，涵盖了语言驱动的开放世界理解、实例级理解、统一的端到端架构、空间推理、生成等内容。</p>
<h4 id="开放词汇3D场景理解"><a href="#开放词汇3D场景理解" class="headerlink" title="开放词汇3D场景理解"></a>开放词汇3D场景理解</h4><p>开放词汇3D场景理解旨在使用自然语言描述而非预定义的类别标签来识别和描述场景元素。OpenScene采用零样本方法，通过预测与CLIP的文本和图像像素嵌入在共享特征空间中共嵌入的3D场景点的密集特征，实现任务无关的训练和开放词汇查询，以识别对象、材料、可操作性、活动和房间类型。CLIP-FO3D采用类似方法，修改CLIP以从3D场景中提取密集像素特征，这些特征被投影到点云，然后通过知识蒸馏训练一个3D模型以传递CLIP的知识。</p>
<p>Semantic Abstraction从CLIP中提取相关性图作为抽象的对象表示，以推广到新的语义、词汇和领域。Open-Fusion结合SEEM视觉-语言模型和TSDF 3D映射，用于实时开放词汇场景创建和查询，利用基于区域的嵌入和置信图。</p>
<p>PLA和RegionPLC等方法利用对比学习将字幕与2D和3D数据模式结合起来，以关联视觉和语义信息。PLA使用3D-字幕对和对比学习，将多视图图像与字幕关联，以学习视觉-语义表示，而RegionPLC通过结合从2D模型映射到3D点的区域级字幕提出了区域感知对比学习。OVIR-3D将2D区域提案和来自现成2D检测器的文本对齐特征融合到3D实例中，以实现高效的开放词汇检索。CoDA在其3D新对象发现（3D-NOD）策略中使用带注释的基本类别的3D几何先验和CLIP的2D语义先验。其Discovery-driven Cross-Modal Alignment（DCMA）对齐3D和图像&#x2F;文本特征，以实现新对象的定位和分类。</p>
<p>实例级场景理解工作如Open-Mask3D和Open3DIS利用预测的类别无关3D实例掩码和2D分段级CLIP嵌入，以实现开放词汇3D实例分割。OpenIns3D在没有对齐图像的情况下通过“Mask-Snap-Lookup”流水线实现开放词汇理解，该流水线预测3D掩码提案，生成合成场景图像，并通过语言模块为掩码分配类别。Rozenberszki等人建议利用CLIP特征为3D语义和实例分割提供支持。</p>
<p>利用NeRFs进行语言落地在开放词汇场景理解中表现出良好结果。DFF、LERF、VL-Fields和3D-OVS等方法通过最小化体渲染特征相对于2D特征的误差，将DINO或CLIP等2D特征提取器的知识蒸馏到3D特征场中，从而实现基于查询的本地编辑和将语言嵌入到神经隐式表示中。LERF通过体渲染CLIP嵌入优化密集的、尺度条件的3D语言场。LangSplat和N2F2通过分层监督和多尺度特征场展示了在3D高斯喷溅表示中进行高效的开放词汇查询和交互。</p>
<h4 id="文本驱动的3D生成"><a href="#文本驱动的3D生成" class="headerlink" title="文本驱动的3D生成"></a>文本驱动的3D生成</h4><p>第4.6节讨论了使用LLMs进行3D生成的方法。这里，我们调查利用2D VLMs指导和文本到图像扩散模型进行文本到3D生成的方法。早期的工作如DreamFields、CLIP-Mesh、CLIP-Forge和Text2Mesh探索了由CLIP指导的零样本3D生成。DreamFusion引入了得分蒸馏采样（SDS），通过使其从任意视角渲染的图像看起来非常真实来优化3D表示的参数，该方法由预训练的2D扩散模型评估。它使用文本到图像的Imagen模型通过SDS优化NeRF表示。</p>
<p>Magic3D提出了一个两阶段框架：首先使用低分辨率扩散先验和稀疏3D哈希网格生成粗略模型，然后使用高效的可微渲染器和高分辨率潜在扩散模型优化带纹理的3D网格模型。Fantasia3D将几何和外观解耦，使用混合DMTet表示和空间变化的BRDFs。ProlificDreamer引入了变分得分蒸馏（VSD），这是一个基于粒子的框架，将3D参数视为随机变量，以提高保真度和多样性。Dream3D利用显式3D形状先验和文本到图像扩散模型来增强文本引导的3D合成。MVDream采用多视图一致的扩散模型，可以在少量数据上进行个性化训练，以实现个性化生成。Text2NeRF结合NeRF表示和预训练的文本到图像扩散模型，从语言生成多样化的室内&#x2F;室外3D场景。</p>
<p>除了同时生成几何和外观外，一些研究还探索了基于给定几何形状合成纹理的可能性。</p>
<p>对于人类头像，AvatarCraft使用扩散模型从文本提示指导神经隐式场几何&#x2F;纹理学习。此外，它通过一个明确的变形场将目标人类网格映射到模板人类网格，实现了这些人类头像的动画化。AvatarCLIP提出了一个零样本的CLIP监督框架，用于从文本生成3D头像、几何雕刻、纹理映射和动作合成。CG-HOI使用扩散模型从文本描述动态的人类-对象交互。GenZI通过预训练的视觉-语言模型蒸馏关于人类交互的信息，从文本提示中生成零样本的3D人类-场景交互合成。</p>
<p>在探索组合生成方面，CG3D通过组合单个对象而不使用边界框，生成可扩展的3D场景，使用显式3D高斯辐射场。Po等人引入了局部条件扩散，通过文本提示和边界框实现细粒度场景控制。GraphDreamer通过将场景图分解为全局-局部描述来优化对象SDFs，从而生成组合场景。总体而言，这些方法结合了扩散模型、视觉-语言模型、神经表示和3D先验，实现了对象、头像和场景的文本到3D生成。</p>
<h3 id="三维视觉与语言的端到端架构"><a href="#三维视觉与语言的端到端架构" class="headerlink" title="三维视觉与语言的端到端架构"></a>三维视觉与语言的端到端架构</h3><p>预训练于大型3D-文本数据集上的Transformer模型可以学习到强大的联合表示，能够跨越视觉和语言模态。3D-VisTA 是一种Transformer模型，使用自注意力机制共同建模3D视觉和文本数据，有效地在遮蔽语言&#x2F;对象建模和场景-文本匹配等目标上进行预训练。UniT3D 采用统一的Transformer方法，结合PointGroup 3D检测骨干网、BERT文本编码器和多模态融合模块，在合成生成的3D-语言数据上进行联合预训练。SpatialVLM 采用另一种方法，在大型合成3D空间推理数据集上共同训练VLMs，提升3D空间视觉问答任务的性能，并实现链式推理等应用。Multi-CLIP 预训练了一个3D场景编码器，以将场景特征与CLIP的文本和图像嵌入对齐，旨在通过转移CLIP的知识来改善3D理解。</p>
<p>除了预训练方法，研究人员还探索了将3D感知与语言能力统一在端到端框架中的架构。D3Net 将密集标注和视觉定位与3D对象检测器、从检测中生成标注的发言者以及使用标注区分对象的听者结合起来。Uni3DL 作用于点云，包含文本编码、点编码、语义&#x2F;掩码预测和多种任务输出（如分割、检测、定位和标注）的模块。InstanceRefer 使用全景分割和语言提示基于语言描述过滤实例候选对象，以进行3D点云中的视觉定位任务，而LanguageRefer 将语言嵌入与3D边界框的空间嵌入结合起来。3DVG-Transformer 也解决了点云中的3D定位问题，具有坐标引导的上下文聚合模块和多重注意力机制以实现有效的特征融合。</p>
<p><br><br><br></p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>我们现在提供用于训练和评估3D视觉-语言模型的数据集的高层概述。在表3中，我们列出了数据集及其用于的任务，以及关于3D扫描和注释的信息。在图4中，我们在时间线上展示了这些数据集，显示每个数据集的3D信息来源。目前的3D视觉-语言数据集几乎完全通过对现有的3D视觉数据集进行人工、模型或模板注释生成。正如在表3中所见，大多数现有数据集专注于真实的室内场景，这可以部分解释为大多数现有数据集使用ScanNet 和3RScan 的3D扫描。许多这里展示的数据集共享相同的3D数据，而主要在其选择的注释策略和设计用于的3D视觉-语言任务上有所不同。</p>
<p>用于语言导航和操作的3D数据集通常围绕特定需求设计，并且与现有的研究主体有很大的重叠。我们建议读者参考现有的综述论文以获得这些数据集的概述。同样，对于文本到3D生成数据集，我们建议读者参考Lee等人 最近的综述。由于之前的广泛覆盖，我们在这里省略了进一步的讨论，因为许多方法使用的是2D视觉-语言数据而不是特定的3D数据集。</p>
<h3 id="Cap3D"><a href="#Cap3D" class="headerlink" title="Cap3D"></a>Cap3D</h3><p>Cap3D 是一个3D对象标注数据集，基于Objaverse 数据集的66万个对象开发。它通过从3D对象的多个视图生成2D图像标注，并使用图像-文本对齐和LLMs进行整合来构建。</p>
<h3 id="Text2Shape"><a href="#Text2Shape" class="headerlink" title="Text2Shape"></a>Text2Shape</h3><p>Text2Shape 是ShapeNet 中的8,447个桌子和6,591个椅子的人工标注形式，结合了用模板生成标签的原始形状的程序生成数据集。它最初用于生成性文本到3D形状任务。</p>
<h3 id="SceneVerse"><a href="#SceneVerse" class="headerlink" title="SceneVerse"></a>SceneVerse</h3><p>SceneVerse 是一个大型多用途的注释场景数据集，通过汇编现有3D数据集的68k场景制作。SceneVerse包含总计250万个视觉-语言对，用于对象标注、场景标注和生成相对描述，主要通过使用3D场景图和LLMs生成。</p>
<h3 id="nu-Caption"><a href="#nu-Caption" class="headerlink" title="nu-Caption"></a>nu-Caption</h3><p>nu-Caption 是一个包含来自nuScenes 数据集的42万个LiDAR扫描的标注版本，使用GPT-4和2D多模态语言模型（MLLMs）进行注释。标注包括一般场景描述、详细的对象及其关系描述以及道路潜在风险的识别。</p>
<h3 id="nu-Grounding"><a href="#nu-Grounding" class="headerlink" title="nu-Grounding"></a>nu-Grounding</h3><p>nu-Grounding 基于nu-Caption，专注于定位任务，使用nuScenes的注释创建28万对问题和答案，用于视觉定位和定位标注。</p>
<h3 id="ScanRefer"><a href="#ScanRefer" class="headerlink" title="ScanRefer"></a>ScanRefer</h3><p>ScanRefer 引入了使用自然语言表达进行3D RGB-D定位的任务，通过在ScanNet 数据集的800个场景中创建51,583个人工注释的“引用表达”，这些表达描述了11,046个对象。输入包括一个扫描的3D场景的点云和一个指定目标对象的自由形式描述，输出是相应对象的边界框。ScanRefer提供了一个评估服务器和在线基准，以便于不同方法之间的比较。</p>
<h3 id="ReferIt3D"><a href="#ReferIt3D" class="headerlink" title="ReferIt3D"></a>ReferIt3D</h3><p>ReferIt3D 引入了一些数据集（Nr3D，Sr3D和Sr3D+），这些数据集包含707个ScanNet场景中的对象。类似于ScanRefer，这些对象用引用表达进行注释，重点是场景中包含多个目标类别实例的查询，引用表达需要在它们之间进行区分。Nr3D包含41,503个人工注释的自由形式表达，以引用3D场景中的对象；Sr3D包含83,572个基于模板的表达；SR3D+是Sr3D的增强表达版本。ReferIt3D还提供了一个评估服务器和在线基准。</p>
<h3 id="Multi3DRefer"><a href="#Multi3DRefer" class="headerlink" title="Multi3DRefer"></a>Multi3DRefer</h3><p>Multi3DRefer 是ScanRefer数据集的修改版本。与始终引用场景中一个对象的引用表达不同，Multi3DRefer包含零目标的6,688个描述，单目标的42,060个描述和多目标的13,178个描述，这些描述针对800个ScanNet场景中的11,609个对象。ChatGPT 用于重新措辞引用表达。</p>
<h3 id="Chat-3D-v2"><a href="#Chat-3D-v2" class="headerlink" title="Chat-3D v2"></a>Chat-3D v2</h3><p>Chat-3D v2 是ScanRefer的另一个修改版本，使用ScanNet中的705个场景的引用表达构建描述场景中对象关系的场景标注。这些场景标注通过向GPT-4提供关于对象的真实信息生成。生成的标注包含对直接表示场景中每个对象的“对象标识符”的引用。</p>
<h3 id="EmbodiedScan"><a href="#EmbodiedScan" class="headerlink" title="EmbodiedScan"></a>EmbodiedScan</h3><p>EmbodiedScan 是Matterport3D、3RScan 和ScanNet 的注释组合，设计为用于3D场景理解的多模态、自我中心的数据集。使用Segment Anything 和其他注释工具提供3D边界框、语义占用和970k基于模板的语言描述，涵盖总计5185个场景。</p>
<h3 id="ScanEnts3D"><a href="#ScanEnts3D" class="headerlink" title="ScanEnts3D"></a>ScanEnts3D</h3><p>ScanEnts3D 扩展了ScanRefer 和ReferIt3D，通过专业注释员将引用句子中提到的每个对象链接到其在3D场景中的实例。在原始论文中，这个数据集仅用于训练目的，发现可以提高模型在其他视觉定位和标注数据集上的性能。</p>
<h3 id="WildRefer"><a href="#WildRefer" class="headerlink" title="WildRefer"></a>WildRefer</h3><p>WildRefer 提出了STRefer和LifeRefer数据集，强调了野外环境中以人为中心的设置，提供了全面的3D和语言人类注释，用于3D定位。STRefer包含来自STCrowd数据集的662个场景的5,458个引用表达，而LifeRefer包含来自新的3D扫描集的3,172个场景的25,380个引用表达。</p>
<h3 id="RIORefer"><a href="#RIORefer" class="headerlink" title="RIORefer"></a>RIORefer</h3><p>RIORefer 是3RScan数据集的人工注释版本，用于3D定位。它包含1,380个场景的63k对象描述。这个数据集作为测试模型跨数据集泛化能力的一种方式引入，例如提出的“ScanRefer到RIORefer泛化”和“RIORefer到ScanRefer泛化”任务。</p>
<h3 id="ARKitSceneRefer"><a href="#ARKitSceneRefer" class="headerlink" title="ARKitSceneRefer"></a>ARKitSceneRefer</h3><p>ARKitSceneRefer 是ARKitScenes数据集的注释版本，强调了现实世界室内环境中小型日常物品的3D定位。它包含1,605个场景中的15k对象描述。</p>
<h3 id="ScanERU"><a href="#ScanERU" class="headerlink" title="ScanERU"></a>ScanERU</h3><p>ScanERU 是ScanNet的修改和人工注释版本，结合了ScanRefer的46k引用表达和包含一个3D人体模型指向引用对象的706个ScanNet场景，位置由人工注释员指定。</p>
<h3 id="DenseGrounding"><a href="#DenseGrounding" class="headerlink" title="DenseGrounding"></a>DenseGrounding</h3><p>DenseGrounding 类似于Multi3DRefer，旨在扩展3D定位任务以包含多个对象，但每个输入是一个结合多个引用查询的段落，每个查询对应一个对象。这些段落通过从ScanRefer和ReferIt3D的随机对象的最近邻居构建，并结合其引用表达形成段落。</p>
<h3 id="ScanQA"><a href="#ScanQA" class="headerlink" title="ScanQA"></a>ScanQA</h3><p>ScanQA（Azuma等） 是ScanNet的注释版本，生成了800个场景中的41k问题-答案对。问题通过使用ScanRefer中的引用表达自动生成，并由人工注释员精炼，而答案完全由人工注释员提供。这个数据集通常被称为“ScanQA”。</p>
<h3 id="ScanQA-另一个"><a href="#ScanQA-另一个" class="headerlink" title="ScanQA (另一个)"></a>ScanQA (另一个)</h3><p>ScanQA（Ye等） 与ScanQA（Azuma等）同时发布，也是ScanNet的人工注释版本，用作3D问答数据集。Ye等包含806个ScanNet场景的10k问题-答案对。Azuma等最初使用ScanRefer的引用表达生成问题，而Ye等完全依赖人工注释员进行问题创建。</p>
<h3 id="3DMV-VQA"><a href="#3DMV-VQA" class="headerlink" title="3DMV-VQA"></a>3DMV-VQA</h3><p>3DMV-VQA 是Habitat-Matterport 3D数据集（HM3D）中的5k场景的注释版本，使用HM3DSem中的语义信息生成50k问题，分为“概念”、“计数”、“关系”和“比较”四种类型。这些问题以模板生成，然后转化为自然语言问题。</p>
<h3 id="NuScenes-QA"><a href="#NuScenes-QA" class="headerlink" title="NuScenes-QA"></a>NuScenes-QA</h3><p>NuScenes-QA 由nuScenes数据集的34k场景组成，标注了460k模板样式的问题-答案对，通过构建的场景图生成。问题分为5种类型：“存在”、“计数”、“查询对象”、“查询状态”和“比较”，并可能包含空间推理。</p>
<h3 id="CLEVR3D"><a href="#CLEVR3D" class="headerlink" title="CLEVR3D"></a>CLEVR3D</h3><p>CLEVR3D 是3RScan数据集的注释版本，设计用于室内3D问答任务。使用3DSSG中的场景图注释生成基于模板的问题和答案。最初为1,333个场景生成了44k问题，但他们使用“组合场景操纵”技术随机替换场景图中的对象，人工生成171k问题，涵盖8,771个场景。</p>
<h3 id="SQA-3D"><a href="#SQA-3D" class="headerlink" title="SQA-3D"></a>SQA-3D</h3><p>SQA-3D使用相同的虚拟场景，生成53k个不同类型的问答对，用于更复杂的推理任务。</p>
<h3 id="3D-LLM"><a href="#3D-LLM" class="headerlink" title="3D-LLM"></a>3D-LLM</h3><p>3D-LLM (Wallace et al.)是一个3D问答数据集，生成于一个虚拟环境中，包含生成的3D问答对。</p>
<h3 id="ScanScribe"><a href="#ScanScribe" class="headerlink" title="ScanScribe"></a>ScanScribe</h3><p>ScanScribe是ScanNet的一个注释版本，包含41k个场景描述，标注了800个场景中的场景信息，用于描述任务。</p>
<h3 id="M3DBench"><a href="#M3DBench" class="headerlink" title="M3DBench"></a>M3DBench</h3><p>M3DBench是一个多模态多任务基准数据集，汇编了多个现有3D视觉语言数据集，用于评估统一的3D视觉语言模型。</p>
<h3 id="GPT4Point"><a href="#GPT4Point" class="headerlink" title="GPT4Point"></a>GPT4Point</h3><p>GPT4Point包含25k对3D视觉问答对，基于ScanNet数据集生成，使用大语言模型生成问题和答案。</p>
<h3 id="LAMM"><a href="#LAMM" class="headerlink" title="LAMM"></a>LAMM</h3><p>LAMM包含25k个3D问答对，基于ScanNet和ReferIt3D数据集生成，使用GPT-4生成问题和答案。</p>
<p><br><br><br></p>
<h2 id="机会与挑战-CHALLENGES-AND-OPPORTUNITIES"><a href="#机会与挑战-CHALLENGES-AND-OPPORTUNITIES" class="headerlink" title="机会与挑战(CHALLENGES AND OPPORTUNITIES)"></a>机会与挑战(CHALLENGES AND OPPORTUNITIES)</h2><p><a href="#jumpHere">〓 ReTURN 〓</a> </p>
<p>尽管在将大规模语言模型（LLMs）与3D数据集成方面取得了一定进展，但在数据表示、计算效率和基准测试方面仍然存在挑战，亟需创新解决方案。</p>
<p>表示选择对3D视觉语言模型的性能有很大影响。目前，点云主要用于表示室内（如网格顶点）和室外（如LiDAR点云）环境，因其简洁性和与神经网络的兼容性。然而，点云难以捕捉准确、丰富的空间模型所需的细节。开发新的3D场景表示方法，更有效地弥合空间信息与语言之间的差距，可能会解锁新的理解和交互水平。通过找到创新的方法，将语言和语义信息编码到3D表示中，如使用精简的语言和语义嵌入，可以帮助弥合这两种模态之间的差距。</p>
<p>3D数据处理和LLMs的计算需求构成了重大挑战。随着3D环境的复杂性和语言模型规模的增加，可扩展性仍然是一个问题。专为适应性和计算效率设计的LLM架构的进步，可能显著扩大其应用范围。</p>
<p>改进基准测试对于全面评估和提升多模态LLMs在3D任务中的能力至关重要。目前的基准测试范围有限，尤其在3D推理方面，阻碍了对空间推理能力和3D决策&#x2F;交互系统开发的评估。此外，现有的度量标准未能全面捕捉LLMs在3D环境中的能力。制定更精准测量不同3D任务性能的特定任务度量标准是必要的。最后，当前场景理解基准的粒度过于简单，限制了对复杂3D环境理解的洞察。需要更为多样的任务集合。</p>
<p>在使用LLMs进行3D理解时，安全性和伦理问题也必须考虑。LLMs可能产生虚构和不准确的信息，导致在关键3D应用中出现错误决策。此外，LLMs常以不可预测且难以解释的方式失败，可能会继承训练数据中的社会偏见，在现实3D场景预测中不成比例地影响某些群体。因此，在3D情境中使用LLMs时，必须谨慎，采用策略创建更包容的数据集，建立用于偏见检测和校正的稳健评估框架，并制定机制以减少虚构，确保负责任和公平的结果。</p>
<p><br><br><br></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>这篇综述论文全面探讨了大型语言模型（LLMs）与3D数据的整合。系统性地回顾了LLMs在处理、理解和生成3D数据的方法、应用和新兴能力，强调了LLMs在各种3D任务中具有变革潜力。从增强在3D环境中的空间理解和交互，到推动具身人工智能系统（embodied AI systems）能力的进步，LLMs在推进这一领域中显得尤为关键。</p>
<p>主要发现包括识别出LLMs的独特优势，如零样本学习（zero-shot learning）、高级推理和广泛的世界知识，这些对弥合文本信息与空间解释之间的差距至关重要。论文展示了LLMs与3D数据成功整合的广泛任务种类。对其他3D视觉-语言方法的探讨揭示了一个丰富的研究领域，旨在加深我们对3D世界的理解。</p>
<p>此外，综述还强调了数据表示、模型可扩展性和计算效率等重大挑战，提出克服这些障碍对于充分实现LLMs在3D应用中的潜力至关重要。综上所述，这篇综述不仅提供了LLMs在3D任务中应用的当前状态的全面概述，还为未来的研究方向奠定了基础。它呼吁通过合作努力，探索和扩展LLMs在理解和与复杂3D世界互动方面的能力，为空间智能领域的进一步进展铺平道路。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>When LLMs step into the 3D World A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models</p><p><a href="https://nerozac.com/2024/06/02/LLMs-step-into-the-3D-World/">https://nerozac.com/2024/06/02/LLMs-step-into-the-3D-World/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Jiawei Li</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2024-06-02</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2024-06-03</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><a class="link-muted mr-2" rel="tag" href="/tags/3DLLM/">3DLLM</a></div><div class="sharethis-inline-share-buttons"></div><script src="nerozac.com" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2024/06/03/FID%E5%9B%BE%E5%83%8F%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">FID图像相似距离</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2024/06/02/%E9%99%90%E5%88%B6docker%E5%AE%B9%E5%99%A8%E6%97%A5%E5%BF%97%E7%A9%BA%E9%97%B4%E5%8D%A0%E7%94%A8/"><span class="level-item">限制gitlab的docker容器日志空间占用</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/jlijz2.jpg" alt="Jiawei Li"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jiawei Li</p><p class="is-size-6 is-block">CSE PhD student at HKUST. Focusing on Large Language Model Agents and 3D databases.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Clear Water Bay, Kowloon, Hong Kong</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">22</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">19</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">26</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/adrianJW421" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/adrianJW421"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="/null"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="/null"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="/null"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#摘要"><span class="level-left"><span class="level-item">1</span><span class="level-item">摘要</span></span></a></li><li><a class="level is-mobile" href="#介绍"><span class="level-left"><span class="level-item">2</span><span class="level-item">介绍</span></span></a></li><li><a class="level is-mobile" href="#背景"><span class="level-left"><span class="level-item">3</span><span class="level-item">背景</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#数据的三维表示"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">数据的三维表示</span></span></a></li><li><a class="level is-mobile" href="#大语言模型"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">大语言模型</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#关于大语言模型架构"><span class="level-left"><span class="level-item">3.2.1</span><span class="level-item">关于大语言模型架构</span></span></a></li><li><a class="level is-mobile" href="#分词"><span class="level-left"><span class="level-item">3.2.2</span><span class="level-item">分词</span></span></a></li></ul></li><li><a class="level is-mobile" href="#大型语言模型的涌现能力"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">大型语言模型的涌现能力</span></span></a></li><li><a class="level is-mobile" href="#大型语言模型的微调"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">大型语言模型的微调</span></span></a></li><li><a class="level is-mobile" href="#关于二维视觉语言模型"><span class="level-left"><span class="level-item">3.5</span><span class="level-item">关于二维视觉语言模型</span></span></a></li><li><a class="level is-mobile" href="#视觉基础模型"><span class="level-left"><span class="level-item">3.6</span><span class="level-item">视觉基础模型</span></span></a></li></ul></li><li><a class="level is-mobile" href="#任务和指标介绍"><span class="level-left"><span class="level-item">4</span><span class="level-item">任务和指标介绍</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#任务-3D标注（3D-to-文本）"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">任务 3D标注（3D to 文本）</span></span></a></li><li><a class="level is-mobile" href="#任务-3D定位（3D加文本-to-3D位置）"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">任务 3D定位（3D加文本 to 3D位置）</span></span></a></li><li><a class="level is-mobile" href="#任务-3D对话（3D-加-文本-to-文本）"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">任务 3D对话（3D 加 文本 to 文本）</span></span></a></li><li><a class="level is-mobile" href="#任务-3D具身代理（3D-加-文本-to-行动）"><span class="level-left"><span class="level-item">4.4</span><span class="level-item">任务 3D具身代理（3D 加 文本 to 行动）</span></span></a></li><li><a class="level is-mobile" href="#任务-文本到3D生成（Text-to-3D）"><span class="level-left"><span class="level-item">4.5</span><span class="level-item">任务 文本到3D生成（Text to 3D）</span></span></a></li></ul></li><li><a class="level is-mobile" href="#关于3D-TASKS-WITH-LLMS"><span class="level-left"><span class="level-item">5</span><span class="level-item">关于3D TASKS WITH LLMS</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#关于LLMs如何处理3D场景信息？"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">关于LLMs如何处理3D场景信息？</span></span></a></li><li><a class="level is-mobile" href="#使用LLMs提升3D任务性能"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">使用LLMs提升3D任务性能</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#知识增强方法"><span class="level-left"><span class="level-item">5.2.1</span><span class="level-item">知识增强方法</span></span></a></li><li><a class="level is-mobile" href="#推理增强方法"><span class="level-left"><span class="level-item">5.2.2</span><span class="level-item">推理增强方法</span></span></a></li></ul></li><li><a class="level is-mobile" href="#使用LLMs进行3D多任务学习"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">使用LLMs进行3D多任务学习</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#多任务学习的数据"><span class="level-left"><span class="level-item">5.3.1</span><span class="level-item">多任务学习的数据</span></span></a></li><li><a class="level is-mobile" href="#多任务3D模型的训练"><span class="level-left"><span class="level-item">5.3.2</span><span class="level-item">多任务3D模型的训练</span></span></a></li></ul></li><li><a class="level is-mobile" href="#LLMs作为3D多模态接口"><span class="level-left"><span class="level-item">5.4</span><span class="level-item">LLMs作为3D多模态接口</span></span></a></li><li><a class="level is-mobile" href="#LLMs用于具身智能体"><span class="level-left"><span class="level-item">5.5</span><span class="level-item">LLMs用于具身智能体</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#3D任务规划"><span class="level-left"><span class="level-item">5.5.1</span><span class="level-item">3D任务规划</span></span></a></li><li><a class="level is-mobile" href="#3D导航"><span class="level-left"><span class="level-item">5.5.2</span><span class="level-item">3D导航</span></span></a></li><li><a class="level is-mobile" href="#3D物体操作"><span class="level-left"><span class="level-item">5.5.3</span><span class="level-item">3D物体操作</span></span></a></li></ul></li><li><a class="level is-mobile" href="#LLMs用于3D生成"><span class="level-left"><span class="level-item">5.6</span><span class="level-item">LLMs用于3D生成</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#对象级生成"><span class="level-left"><span class="level-item">5.6.1</span><span class="level-item">对象级生成</span></span></a></li><li><a class="level is-mobile" href="#场景级生成"><span class="level-left"><span class="level-item">5.6.2</span><span class="level-item">场景级生成</span></span></a></li><li><a class="level is-mobile" href="#程序生成与操控"><span class="level-left"><span class="level-item">5.6.3</span><span class="level-item">程序生成与操控</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#视觉语言模型与三维任务系列-3D-TASKS-WITH-VLMS"><span class="level-left"><span class="level-item">6</span><span class="level-item">视觉语言模型与三维任务系列(3D TASKS WITH VLMS)</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#利用VLMs进行3D任务"><span class="level-left"><span class="level-item">6.1</span><span class="level-item">利用VLMs进行3D任务</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#开放词汇3D场景理解"><span class="level-left"><span class="level-item">6.1.1</span><span class="level-item">开放词汇3D场景理解</span></span></a></li><li><a class="level is-mobile" href="#文本驱动的3D生成"><span class="level-left"><span class="level-item">6.1.2</span><span class="level-item">文本驱动的3D生成</span></span></a></li></ul></li><li><a class="level is-mobile" href="#三维视觉与语言的端到端架构"><span class="level-left"><span class="level-item">6.2</span><span class="level-item">三维视觉与语言的端到端架构</span></span></a></li></ul></li><li><a class="level is-mobile" href="#数据集"><span class="level-left"><span class="level-item">7</span><span class="level-item">数据集</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Cap3D"><span class="level-left"><span class="level-item">7.1</span><span class="level-item">Cap3D</span></span></a></li><li><a class="level is-mobile" href="#Text2Shape"><span class="level-left"><span class="level-item">7.2</span><span class="level-item">Text2Shape</span></span></a></li><li><a class="level is-mobile" href="#SceneVerse"><span class="level-left"><span class="level-item">7.3</span><span class="level-item">SceneVerse</span></span></a></li><li><a class="level is-mobile" href="#nu-Caption"><span class="level-left"><span class="level-item">7.4</span><span class="level-item">nu-Caption</span></span></a></li><li><a class="level is-mobile" href="#nu-Grounding"><span class="level-left"><span class="level-item">7.5</span><span class="level-item">nu-Grounding</span></span></a></li><li><a class="level is-mobile" href="#ScanRefer"><span class="level-left"><span class="level-item">7.6</span><span class="level-item">ScanRefer</span></span></a></li><li><a class="level is-mobile" href="#ReferIt3D"><span class="level-left"><span class="level-item">7.7</span><span class="level-item">ReferIt3D</span></span></a></li><li><a class="level is-mobile" href="#Multi3DRefer"><span class="level-left"><span class="level-item">7.8</span><span class="level-item">Multi3DRefer</span></span></a></li><li><a class="level is-mobile" href="#Chat-3D-v2"><span class="level-left"><span class="level-item">7.9</span><span class="level-item">Chat-3D v2</span></span></a></li><li><a class="level is-mobile" href="#EmbodiedScan"><span class="level-left"><span class="level-item">7.10</span><span class="level-item">EmbodiedScan</span></span></a></li><li><a class="level is-mobile" href="#ScanEnts3D"><span class="level-left"><span class="level-item">7.11</span><span class="level-item">ScanEnts3D</span></span></a></li><li><a class="level is-mobile" href="#WildRefer"><span class="level-left"><span class="level-item">7.12</span><span class="level-item">WildRefer</span></span></a></li><li><a class="level is-mobile" href="#RIORefer"><span class="level-left"><span class="level-item">7.13</span><span class="level-item">RIORefer</span></span></a></li><li><a class="level is-mobile" href="#ARKitSceneRefer"><span class="level-left"><span class="level-item">7.14</span><span class="level-item">ARKitSceneRefer</span></span></a></li><li><a class="level is-mobile" href="#ScanERU"><span class="level-left"><span class="level-item">7.15</span><span class="level-item">ScanERU</span></span></a></li><li><a class="level is-mobile" href="#DenseGrounding"><span class="level-left"><span class="level-item">7.16</span><span class="level-item">DenseGrounding</span></span></a></li><li><a class="level is-mobile" href="#ScanQA"><span class="level-left"><span class="level-item">7.17</span><span class="level-item">ScanQA</span></span></a></li><li><a class="level is-mobile" href="#ScanQA-另一个"><span class="level-left"><span class="level-item">7.18</span><span class="level-item">ScanQA (另一个)</span></span></a></li><li><a class="level is-mobile" href="#3DMV-VQA"><span class="level-left"><span class="level-item">7.19</span><span class="level-item">3DMV-VQA</span></span></a></li><li><a class="level is-mobile" href="#NuScenes-QA"><span class="level-left"><span class="level-item">7.20</span><span class="level-item">NuScenes-QA</span></span></a></li><li><a class="level is-mobile" href="#CLEVR3D"><span class="level-left"><span class="level-item">7.21</span><span class="level-item">CLEVR3D</span></span></a></li><li><a class="level is-mobile" href="#SQA-3D"><span class="level-left"><span class="level-item">7.22</span><span class="level-item">SQA-3D</span></span></a></li><li><a class="level is-mobile" href="#3D-LLM"><span class="level-left"><span class="level-item">7.23</span><span class="level-item">3D-LLM</span></span></a></li><li><a class="level is-mobile" href="#ScanScribe"><span class="level-left"><span class="level-item">7.24</span><span class="level-item">ScanScribe</span></span></a></li><li><a class="level is-mobile" href="#M3DBench"><span class="level-left"><span class="level-item">7.25</span><span class="level-item">M3DBench</span></span></a></li><li><a class="level is-mobile" href="#GPT4Point"><span class="level-left"><span class="level-item">7.26</span><span class="level-item">GPT4Point</span></span></a></li><li><a class="level is-mobile" href="#LAMM"><span class="level-left"><span class="level-item">7.27</span><span class="level-item">LAMM</span></span></a></li></ul></li><li><a class="level is-mobile" href="#机会与挑战-CHALLENGES-AND-OPPORTUNITIES"><span class="level-left"><span class="level-item">8</span><span class="level-item">机会与挑战(CHALLENGES AND OPPORTUNITIES)</span></span></a></li><li><a class="level is-mobile" href="#结论"><span class="level-left"><span class="level-item">9</span><span class="level-item">结论</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://nerozac.com/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">link1</span></span><span class="level-right"><span class="level-item tag">nerozac.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/DSF/"><span class="level-start"><span class="level-item">DSF</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/DSF/%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"><span class="level-start"><span class="level-item">服务器环境配置</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"><span class="level-start"><span class="level-item">好文转载</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/%E5%A4%A7%E7%A5%9E%E8%AF%AD%E5%BD%95/"><span class="level-start"><span class="level-item">大神语录</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/"><span class="level-start"><span class="level-item">技术百科</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/MacOS/"><span class="level-start"><span class="level-item">MacOS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/computer-vision/"><span class="level-start"><span class="level-item">computer_vision</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/gitlab/"><span class="level-start"><span class="level-item">gitlab</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"><span class="level-start"><span class="level-item">三维重建</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/%E7%B3%BB%E7%BB%9F%E7%8E%AF%E5%A2%83%E5%8F%8A%E7%BC%96%E8%AF%91/"><span class="level-start"><span class="level-item">系统环境及编译</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/%E8%A7%86%E9%A2%91%E7%BC%96%E8%BE%91/"><span class="level-start"><span class="level-item">视频编辑</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><span class="level-start"><span class="level-item">论文阅读</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/3DLLM/"><span class="level-start"><span class="level-item">3DLLM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLMAgent/"><span class="level-start"><span class="level-item">LLMAgent</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/PCDM/"><span class="level-start"><span class="level-item">PCDM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/awesome%E5%90%88%E9%9B%86/"><span class="level-start"><span class="level-item">awesome合集</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"><span class="level-start"><span class="level-item">三维重建</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-04T06:16:39.096Z">2024-09-04</time></p><p class="title"><a href="/2024/09/04/FreeReg/">FREEREG 利用预训练扩散模型和单目深度估计器的图像到点云配准</a></p><p class="categories"><a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a> / <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/PCDM/">PCDM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-07-20T03:41:06.137Z">2024-07-20</time></p><p class="title"><a href="/2024/07/20/awesome-X/">awesome_X</a></p><p class="categories"><a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a> / <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/awesome%E5%90%88%E9%9B%86/">awesome合集</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-07-03T03:58:15.422Z">2024-07-03</time></p><p class="title"><a href="/2024/07/03/LLMAgent-current-and-future/">智源大会2024 LLM Agent - Current and Future</a></p><p class="categories"><a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a> / <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLMAgent/">LLMAgent</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-15T11:24:24.008Z">2024-06-15</time></p><p class="title"><a href="/2024/06/15/%E4%BD%BF%E7%94%A8Blender%E8%BF%9B%E8%A1%8Cply%E8%BD%ACobj/">使用Blender进行ply转obj</a></p><p class="categories"><a href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/">技术百科</a> / <a href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/">三维重建</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-11T08:05:29.112Z">2024-06-11</time></p><p class="title"><a href="/2024/06/11/Buffer-of-Thoughts/">Buffer of Thoughts - Thought-Augmented Reasoning with Large Language Models</a></p><p class="categories"><a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a> / <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLMAgent/">LLMAgent</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">九月 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/07/"><span class="level-start"><span class="level-item">七月 2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/06/"><span class="level-start"><span class="level-item">六月 2024</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/05/"><span class="level-start"><span class="level-item">五月 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/3DLLM/"><span class="tag">3DLLM</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CUDA%E9%A9%B1%E5%8A%A8/"><span class="tag">CUDA驱动</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DSF/"><span class="tag">DSF</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLMAgent/"><span class="tag">LLMAgent</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MacOS/"><span class="tag">MacOS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PCDM/"><span class="tag">PCDM</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transformer/"><span class="tag">Transformer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/awesome%E5%90%88%E9%9B%86/"><span class="tag">awesome合集</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/computer-vision/"><span class="tag">computer_vision</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/conda-forge/"><span class="tag">conda-forge</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/gcc/"><span class="tag">gcc</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/gitlab/"><span class="tag">gitlab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"><span class="tag">三维重建</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8D%9A%E5%A3%AB%E7%94%9F%E6%B6%AF/"><span class="tag">博士生涯</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%96%E6%8E%A5%E8%AE%BE%E5%A4%87/"><span class="tag">外接设备</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%A7%E7%A5%9E%E8%AF%AD%E5%BD%95/"><span class="tag">大神语录</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"><span class="tag">好文转载</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/"><span class="tag">技术百科</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"><span class="tag">服务器</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%82%B9%E4%BA%91/"><span class="tag">点云</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%B3%BB%E7%BB%9F%E7%8E%AF%E5%A2%83%E5%8F%8A%E7%BC%96%E8%AF%91/"><span class="tag">系统环境及编译</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A7%86%E9%A2%91%E7%BC%96%E8%BE%91/"><span class="tag">视频编辑</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><span class="tag">论文阅读</span><span class="tag">9</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Zac&#039;s Blog</a><p class="is-size-7"><span>&copy; 2024 Jiawei Li</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/adrianJW421"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>