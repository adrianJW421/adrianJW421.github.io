<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>转载： Attention为什么要除以$\sqrt{d}$ - NeroZac</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Zac&#039;s Blog"><meta name="msapplication-TileImage" content="https://github.com/adrianJW421/MyPicBed/raw/main/theme/icarus/avatar.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Zac&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="摘要:  Attention 的计算公式中 Attention $(Q, K, V)&amp;#x3D;\operatorname{softmax}\left(\frac{Q K^{\top}}{\sqrt{d}}\right)$ 为什么要除以 $\sqrt{d}$ ?  原文链接： https:&amp;#x2F;&amp;#x2F;mp.weixin.qq.com&amp;#x2F;s&amp;#x2F;3o0NgpFPKS1RNICNuMuygg"><meta property="og:type" content="blog"><meta property="og:title" content="Zac&#039;s Blog"><meta property="og:url" content="https://nerozac.com/2024/06/02/Attention%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E9%99%A4%E4%BB%A5%E6%A0%B9%E5%8F%B7d/"><meta property="og:site_name" content="Jiawei Li (Zac)"><meta property="og:description" content="摘要:  Attention 的计算公式中 Attention $(Q, K, V)&amp;#x3D;\operatorname{softmax}\left(\frac{Q K^{\top}}{\sqrt{d}}\right)$ 为什么要除以 $\sqrt{d}$ ?  原文链接： https:&amp;#x2F;&amp;#x2F;mp.weixin.qq.com&amp;#x2F;s&amp;#x2F;3o0NgpFPKS1RNICNuMuygg"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://nerozac.com/img/og_image.png"><meta property="article:published_time" content="2024-06-02T15:08:37.655Z"><meta property="article:modified_time" content="2024-06-02T15:08:37.655Z"><meta property="article:author" content="Jiawei Li (Zac)"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="好文转载"><meta property="article:tag" content="NLP"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://nerozac.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://nerozac.com/2024/06/02/Attention%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E9%99%A4%E4%BB%A5%E6%A0%B9%E5%8F%B7d/"},"headline":"Zac's Blog","image":["https://nerozac.com/img/og_image.png"],"datePublished":"2024-06-02T15:08:37.655Z","dateModified":"2024-06-02T15:08:37.655Z","author":{"@type":"Person","name":"Jiawei Li"},"publisher":{"@type":"Organization","name":"NeroZac","logo":{"@type":"ImageObject","url":{"text":"Zac's Blog"}}},"description":"摘要:  Attention 的计算公式中 Attention $(Q, K, V)&#x3D;\\operatorname{softmax}\\left(\\frac{Q K^{\\top}}{\\sqrt{d}}\\right)$ 为什么要除以 $\\sqrt{d}$ ?  原文链接： https:&#x2F;&#x2F;mp.weixin.qq.com&#x2F;s&#x2F;3o0NgpFPKS1RNICNuMuygg"}</script><link rel="canonical" href="https://nerozac.com/2024/06/02/Attention%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E9%99%A4%E4%BB%A5%E6%A0%B9%E5%8F%B7d/"><link rel="icon" href="https://github.com/adrianJW421/MyPicBed/raw/main/theme/icarus/avatar.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Zac&#039;s Blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/adrianJW421"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-06-02T15:08:37.655Z" title="6/2/2024, 11:08:37 PM">2024-06-02</time>发表</span><span class="level-item"><time dateTime="2024-06-02T15:08:37.655Z" title="6/2/2024, 11:08:37 PM">2024-06-02</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/">好文转载</a><span> / </span><a class="link-muted" href="/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/NLP/">NLP</a></span><span class="level-item">18 分钟读完 (大约2748个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">转载： Attention为什么要除以$\sqrt{d}$</h1><div class="content"><p>摘要:  Attention 的计算公式中 Attention $(Q, K, V)&#x3D;\operatorname{softmax}\left(\frac{Q K^{\top}}{\sqrt{d}}\right)$ 为什么要除以 $\sqrt{d}$ ? </p>
<p>原文链接： <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/3o0NgpFPKS1RNICNuMuygg">https://mp.weixin.qq.com/s/3o0NgpFPKS1RNICNuMuygg</a></p>
<span id="more"></span>

<style>
    .blue-box {
        border: 2px solid blue; /* 设置边框为蓝色 */
        padding: 5px; /* 设置内边距 */
        background-color: lightblue; /* 设置背景颜色为浅蓝色 */
        color: black; /* 设置文本颜色为黑色 */
        width: 800px; /* 设置方框的宽度 */
        margin: 5px; /* 设置外边距 */
    }
</style>

<br>

<p><span id="jumpHere"> 〓 Table of Contents 〓 </span></p>
<br>

<ul>
<li><a href="#%E6%9D%A5%E8%87%AA%E5%8E%9F%E8%AE%BA%E6%96%87%E7%9A%84%E5%88%86%E6%9E%90">来自原论文的分析</a></li>
<li><a href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%9A%84%E5%8E%9F%E5%9B%A0">梯度消失的原因</a><ul>
<li><a href="#d-k%E5%8F%98%E5%A4%A7-q-cdot-k-top%E6%96%B9%E5%B7%AE%E4%BC%9A%E5%8F%98%E5%A4%A7%E3%80%82">$d_k$ 变大, $q \cdot k^{\top}$ 方差会变大。</a></li>
<li><a href="#%E6%96%B9%E5%B7%AE%E5%8F%98%E5%A4%A7%E4%BC%9A%E5%AF%BC%E8%87%B4%E5%90%91%E9%87%8F%E4%B9%8B%E9%97%B4%E5%85%83%E7%B4%A0%E7%9A%84%E5%B7%AE%E5%80%BC%E5%8F%98%E5%A4%A7%E3%80%82">方差变大会导致向量之间元素的差值变大。</a></li>
<li><a href="#softmax%E9%80%80%E5%8C%96%E4%B8%BA-argmax">softmax 退化为 argmax</a></li>
<li><a href="#softmax%E4%BB%80%E4%B9%88%E6%83%85%E5%86%B5%E4%B8%8B%E4%BC%9A%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1">softmax 什么情况下会梯度消失</a></li>
</ul>
</li>
<li><a href="#scale%E7%9A%84%E5%80%BC%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF-sqrt-d-k%E6%9C%89%E6%9B%B4%E5%A5%BD%E7%9A%84%E5%80%BC%E4%B9%88">scale 的值为什么是 $\sqrt{d_k}$, 有更好的值么?</a></li>
</ul>
<p><br><br></p>
<h2 id="来自原论文的分析"><a href="#来自原论文的分析" class="headerlink" title="来自原论文的分析"></a>来自原论文的分析</h2><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<p>《Attention is All Your Need》的原论文给出了一个粗略的答案。 </p>
<blockquote>
<p>While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ [3]. We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by $\frac{1}{\sqrt{d_k}}$.</p>
</blockquote>
<p>当 $d_k$ 的值变大的时候，softmax 函数会导致梯度消失问题，因此设置了一个 softmax 的 temperature 来缓解这个问题。这里 temperature 被设置为了 $\sqrt{d_k}$ ，也就是乘上 $\frac{1}{\sqrt{d_k}}$.<br>延伸讨论：</p>
<ol>
<li>为什么会导致梯度消失?</li>
<li>为什么是 $\sqrt{d_k}$, 有更好的值么?</li>
</ol>
<p><br><br></p>
<h2 id="梯度消失的原因"><a href="#梯度消失的原因" class="headerlink" title="梯度消失的原因"></a>梯度消失的原因</h2><p><a href="#jumpHere">〓 ReTURN 〓</a></p>
<ol>
<li>如果 $d_k$ 变大, $q \cdot k^{\top}$ 方差会变大。</li>
<li>方差变大会导致向量之间元素的差值变大。</li>
<li>元素的差值变大会导致 softmax 退化为 argmax, 也就是最大值 softmax 后的值为 1 , 其他值则为 0 。</li>
<li>softmax 只有一个值为 1 的元素, 其他都为 0 的话, 反向传播的梯度会变为 0 , 也就是所谓的梯度消失。</li>
</ol>
<h3 id="d-k-变大-q-cdot-k-top-方差会变大。"><a href="#d-k-变大-q-cdot-k-top-方差会变大。" class="headerlink" title="$d_k$ 变大, $q \cdot k^{\top}$ 方差会变大。"></a>$d_k$ 变大, $q \cdot k^{\top}$ 方差会变大。</h3><p>假设 $q$ 和 $k$ 的向量长度为 $d_k$ ，均值为 0 , 方差为 1 。则 $q$ 和 $k$ 的点积的方差为:<br>$$<br>\begin{aligned}<br>\operatorname{var}\left[q \cdot k^{\top}\right] &amp; &#x3D;\operatorname{var}\left[\sum_{i&#x3D;1}^{d_k} q_i \times k_i\right] \<br>&amp; &#x3D;\sum_{i&#x3D;1}^{d_k} \operatorname{var}\left[q_i \times k_i\right] \<br>&amp; &#x3D;\sum_{i&#x3D;1}^{d_k} \operatorname{var}\left[q_i\right] \times \operatorname{var}\left[k_i\right] \<br>&amp; &#x3D;\sum_{i&#x3D;1}^{d_k} 1 \<br>&amp; &#x3D;d_k<br>\end{aligned}<br>$$</p>
<p>当 $d_k$ 变大时, 方差变大。</p>
<h3 id="方差变大会导致向量之间元素的差值变大。"><a href="#方差变大会导致向量之间元素的差值变大。" class="headerlink" title="方差变大会导致向量之间元素的差值变大。"></a>方差变大会导致向量之间元素的差值变大。</h3><p>方差变大就是代表了数据之间的差异性变大。<br>看上去显然， 如果非要给出证明， 可以将这个问题换一个问题来侧面回答这个问题。<br>新的问题<mark>假设向量是通过独立同分布的数据采样出来的 $d_k$ 个数据, 那么这 $d_k$ 个数的最大值的期望是多少?</mark></p>
<p>因为分布很多, 这里只给出最常用的正态分布的证明, 详细证明见:<br><a target="_blank" rel="noopener" href="http://www.gautamkamath.com/writings/gaussian_max.pdf">http://www.gautamkamath.com/writings/gaussian_max.pdf</a></p>
<p>这里只给出结论如下:<br>Theorem 1. Let $Y&#x3D;\max _{1 \leq i \leq n} X_i$, where $X_i \sim \mathcal{N}\left(0, \sigma^2\right)$ are i.i.d. random variables. Then<br>$$<br>\frac{1}{\sqrt{\pi \log 2}} \sigma \sqrt{\log n} \leq \mathbf{E}[Y] \leq \sqrt{2} \sigma \sqrt{\log n} .<br>$$</p>
<p>从期望的下界, 可以看出, 方差越大, 最大值的期望越大。同时还有个结论就是 $d_k$ 越大, 最大值的期望也越大。由于正太分布是对称的, 最小值就是最大值取负号。</p>
<p>所以方差变大, 数据分布的最大最小值的差值变大了, 也就从侧面证明了向量元素之间的差值变大了。</p>
<h3 id="softmax-退化为-argmax"><a href="#softmax-退化为-argmax" class="headerlink" title="softmax 退化为 argmax"></a>softmax 退化为 argmax</h3><p>对于 softmax 函数中的每个分量 $\operatorname{softmax}\left(x_i\right)$, 我们可以写成:<br>$$<br>\operatorname{softmax}\left(x_i\right)&#x3D;\frac{e^{x_i}}{\sum_{j&#x3D;1}^n e^{x_j}}<br>$$</p>
<p>当 $x_k$ 是最大的元素时, $e^{x_k}$ 会显著大于其他 $e^{x_i}$ (其中 $i \neq k$ ), 尤其是当这些 $x_i$ 和 $x_k$ 之间的差距变得非常大时。为了更清楚地看出这一点, 我们将 $x_i$ 的每个元素表示成最大元素 $x_k$ 减去一个差值 $\Delta_i$, 即 $x_i&#x3D;x_k-\Delta_i$, 其中 $\Delta_k&#x3D;0$ 。</p>
<p>因此, softmax 函数可以重写为:<br>$$<br>\operatorname{softmax}\left(x_i\right)&#x3D;\frac{e^{x_i}}{\sum_{j&#x3D;1}^n e^{x_j}}&#x3D;\frac{e^{x_k-\Delta_i}}{\sum_{j&#x3D;1}^n e^{x_k-\Delta_j}}<br>$$</p>
<p>由于 $e^{x_k}$ 是公因子, 根据 $\exp$ 的性质, 可以提出来并且约掉:<br>$$<br>\operatorname{softmax}\left(x_i\right)&#x3D;\frac{e^{x_k} e^{-\Delta_i}}{e^{x_k} \sum_{j&#x3D;1}^n e^{-\Delta_j}}&#x3D;\frac{e^{-\Delta_i}}{\sum_{j&#x3D;1}^n e^{-\Delta_j}}<br>$$</p>
<p>当 $\Delta_i$ 非常大（即 $x_i$ 远小于 $x_k$ ）时, $e^{-\Delta_i}$ 会接近于 0。因此, 除了 $\Delta_k&#x3D;0$ 以外的所有项,其他项 $e^{-\Delta_j}$ 都会非常小, 可以忽略不计(其实都不用非常小, $e^{-5}&#x3D;0.0067379 \ldots, e^{-10}&#x3D;4.534-05$, 只要差值大于 10 , 就可以忽略不计了)。于是, 对于 $i&#x3D;k$ :<br>$$<br>\operatorname{softmax}\left(x_k\right) \approx \frac{1}{1}&#x3D;1<br>$$</p>
<p>而对于 $i \neq k$ :<br>$$<br>\operatorname{softmax}\left(x_i\right) \approx 0<br>$$</p>
<p>所以说当输入向量 $\mathbf{x}$ 的方差变得非常大时, softmax 函数将会趋近于将最大的元素赋值为 1 ,</p>
<p>而其他元素赋值为 0 , 也就是是 argmax 函数。用公式表示的话:<br>$$<br>\lim _{\operatorname{var}(\mathbf{x}) \rightarrow \infty} \operatorname{softmax}(\mathbf{x})&#x3D;\operatorname{argmax}(\mathbf{x})<br>$$</p>
<p>所以方差变大时， softmax 函数会退化为 argmax 函数。<br>这里我们可以做个实验看一下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">n = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">x1 = np.random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=n)</span><br><span class="line">x2 = np.random.normal(loc=<span class="number">0</span>, scale=np.sqrt(<span class="number">512</span>), size=n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;x1最大值和最小值的差值:&#x27;</span>, <span class="built_in">max</span>(x1) - <span class="built_in">min</span>(x1))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;x1最大值和最小值的差值:&#x27;</span>, <span class="built_in">max</span>(x2) - <span class="built_in">min</span>(x2))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) / np.<span class="built_in">sum</span>(np.exp(x), keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax_grad</span>(<span class="params">y</span>):</span><br><span class="line">    <span class="keyword">return</span> np.diag(y) - np.outer(y, y)</span><br><span class="line"></span><br><span class="line">ex1 = softmax(x1)</span><br><span class="line">ex2 = softmax(x2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;softmax(x1) =&#x27;</span>, ex1)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;softmax(x2) =&#x27;</span>, ex2)</span><br></pre></td></tr></table></figure>

<p>其结果为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x1最大值和最小值的差值: 1.8973472870218264</span><br><span class="line">x1最大值和最小值的差值: 66.62254341144866</span><br><span class="line">softmax(x1) = [0.16704083 0.21684976 0.0579299  0.05408421 0.16109133 0.14433417</span><br><span class="line"> 0.03252007 0.05499126 0.04213939 0.06901908]</span><br><span class="line">softmax(x2) = [4.51671361e-19 2.88815837e-21 9.99999972e-01 3.02351231e-17</span><br><span class="line"> 3.73439970e-25 8.18066523e-13 2.78385563e-08 1.16465424e-29</span><br><span class="line"> 7.25661271e-20 3.21813750e-21]</span><br></pre></td></tr></table></figure>

<p>可以看出, 在方差为 $\sqrt{512}$ 的时候, softmax 只有第三个元素接近 1 , 其他都几乎为 0 .</p>
<h3 id="softmax-什么情况下会梯度消失"><a href="#softmax-什么情况下会梯度消失" class="headerlink" title="softmax 什么情况下会梯度消失"></a>softmax 什么情况下会梯度消失</h3><p>我们来对 softmax 函数进行求导。定义 softmax 为<br>$$<br>\operatorname{softmax}\left(z_i\right)&#x3D;\frac{\exp \left(z_i\right)}{\sum_j \exp \left(z_j\right)}<br>$$</p>
<p>假设我们有一个向量 $\mathbf{z}&#x3D;\left[z_1, z_2, \ldots, z_n\right]$, softmax 函数的输出是一个向量 $\mathbf{y}&#x3D;\left[y_1, y_2, \ldots, y_n\right]$ , 其中:<br>$$<br>y_i&#x3D;\operatorname{softmax}\left(z_i\right)&#x3D;\frac{\exp \left(z_i\right)}{\sum_{j&#x3D;1}^n \exp \left(z_j\right)}<br>$$</p>
<p>我们需要计算 softmax 函数的导数, 即 $\frac{\partial y_i}{\partial z_k}$, 分为两种情况:</p>
<ol>
<li>当 $i&#x3D;k$</li>
<li>当 $i \neq k$</li>
</ol>
<p>首先, 计算 $y_i$ 对 $z_k$ 的导数：</p>
<ol>
<li>当 $i&#x3D;k$ 时<br>$$<br>\frac{\partial y_i}{\partial z_i}&#x3D;\frac{\partial}{\partial z_i}\left(\frac{\exp \left(z_i\right)}{\sum_j \exp \left(z_j\right)}\right)<br>$$</li>
</ol>
<p>使用商的导数法则, 我们得到:<br>$$<br>\frac{\partial y_i}{\partial z_i}&#x3D;\frac{\exp \left(z_i\right) \sum_j \exp \left(z_j\right)-\exp \left(z_i\right) \exp \left(z_i\right)}{\left(\sum_j \exp \left(z_j\right)\right)^2}<br>$$</p>
<p>化简得到:<br>$$<br>\frac{\partial y_i}{\partial z_i}&#x3D;\frac{\exp \left(z_i\right)\left(\sum_j \exp \left(z_j\right)-\exp \left(z_i\right)\right)}{\left(\sum_j \exp \left(z_j\right)\right)^2}&#x3D;\frac{\exp \left(z_i\right)}{\sum_j \exp \left(z_j\right)}\left(1-\frac{\exp \left(z_i\right)}{\sum_j \exp \left(z_j\right)}\right)<br>$$</p>
<p>即:<br>$$<br>\frac{\partial y_i}{\partial z_i}&#x3D;y_i\left(1-y_i\right)<br>$$<br>2. 当 $i \neq k$ 时<br>$$<br>\frac{\partial y_i}{\partial z_k}&#x3D;\frac{\partial}{\partial z_k}\left(\frac{\exp \left(z_i\right)}{\sum_j \exp \left(z_j\right)}\right)<br>$$</p>
<p>同样使用商的导数法则, 我们得到:<br>$$<br>\frac{\partial y_i}{\partial z_k}&#x3D;\frac{0 \cdot \sum_j \exp \left(z_j\right)-\exp \left(z_i\right) \exp \left(z_k\right)}{\left(\sum_j \exp \left(z_j\right)\right)^2}&#x3D;-\frac{\exp \left(z_i\right) \exp \left(z_k\right)}{\left(\sum_j \exp \left(z_j\right)\right)^2}<br>$$</p>
<p>即:<br>$$<br>\frac{\partial y_i}{\partial z_k}&#x3D;-y_i y_k<br>$$</p>
<p>两种情况合并一下</p>
<p>将两种情况合并, softmax 的导数可以表示为：<br>$$<br>\frac{\partial y_i}{\partial z_k}&#x3D;y_i\left(\delta_{i k}-y_k\right)<br>$$</p>
<p>其中, $\delta_{i k}$ 是 Kronecker delta 函数, 定义为:<br>$$<br>\delta_{i k}&#x3D; \begin{cases}1, &amp; \text { if } i&#x3D;k \ 0, &amp; \text { if } i \neq k\end{cases}<br>$$</p>
<p>最终可以用 Jacobian 矩阵表示, Jacobians 矩阵的第 $i$ 行和第 $k$ 列元素是 $\frac{\partial y_i}{\partial z_k}$ :<br>$$<br>\mathbf{J}&#x3D;\left[\begin{array}{cccc}<br>y_1\left(1-y_1\right) &amp; -y_1 y_2 &amp; \cdots &amp; -y_1 y_n \<br>-y_2 y_1 &amp; y_2\left(1-y_2\right) &amp; \cdots &amp; -y_2 y_n \<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>-y_n y_1 &amp; -y_n y_2 &amp; \cdots &amp; y_n\left(1-y_n\right)<br>\end{array}\right]<br>$$</p>
<p>然后与第三步的世界线交汇，好玩的来了</p>
<p>在第三步中我们证明了当方差变大的时候, softmax 退化成了 argmax, 也就是变成一个只有一个 1 其他全为 0 的向量。</p>
<p>这个向量带入到上面的雅可比矩阵会发生什么? 我们发现对于任意的 $y_k&#x3D;1, y_{j \neq k}&#x3D;0$ 的向量来说, 雅可比矩阵变成了一个全 0 矩阵。</p>
<p>也就是说梯度全为 0 了。到这里才算是证明了为什么 $q \cdot k^{\top}$ 的方差不能太大, 太大了就梯度消失。</p>
<p>梯度实验</p>
<p>我们同样做个实验，看看梯度到底为多少。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">n = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">x1 = np.random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=n)</span><br><span class="line">x2 = np.random.normal(loc=<span class="number">0</span>, scale=np.sqrt(<span class="number">512</span>), size=n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;x1最大值和最小值的差值:&#x27;</span>, <span class="built_in">max</span>(x1) - <span class="built_in">min</span>(x1))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;x1最大值和最小值的差值:&#x27;</span>, <span class="built_in">max</span>(x2) - <span class="built_in">min</span>(x2))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) / np.<span class="built_in">sum</span>(np.exp(x), keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax_grad</span>(<span class="params">y</span>):</span><br><span class="line">    <span class="keyword">return</span> np.diag(y) - np.outer(y, y)</span><br><span class="line"></span><br><span class="line">ex1 = softmax(x1)</span><br><span class="line">ex2 = softmax(x2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;softmax(x1) =&#x27;</span>, ex1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;max of gradiant of softmax(x1) =&#x27;</span>, np.<span class="built_in">max</span>(softmax_grad(ex1)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;softmax(x2) =&#x27;</span>, ex2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;max gradiant of softmax(x2) =&#x27;</span>, np.<span class="built_in">max</span>(softmax_grad(ex2)))</span><br></pre></td></tr></table></figure>
<p>其结果为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x1最大值和最小值的差值: 1.8973472870218264</span><br><span class="line">x1最大值和最小值的差值: 66.62254341144866</span><br><span class="line">softmax(x1) = [0.16704083 0.21684976 0.0579299  0.05408421 0.16109133 0.14433417</span><br><span class="line"> 0.03252007 0.05499126 0.04213939 0.06901908]</span><br><span class="line">max of gradiant of softmax(x1) = 0.1698259433168865</span><br><span class="line">softmax(x2) = [4.51671361e-19 2.88815837e-21 9.99999972e-01 3.02351231e-17</span><br><span class="line"> 3.73439970e-25 8.18066523e-13 2.78385563e-08 1.16465424e-29</span><br><span class="line"> 7.25661271e-20 3.21813750e-21]</span><br><span class="line">max gradiant of softmax(x2) = 2.7839373695215386e-08</span><br></pre></td></tr></table></figure>

<p>可以看出，在方差为  的时候，长度仅仅为10的向量x2，其梯度就已经快没有了，最大值为2.78e-8。 而如果将方差控制在1，则最大的梯度为0.1698</p>
<h2 id="scale-的值为什么是-sqrt-d-k-有更好的值么"><a href="#scale-的值为什么是-sqrt-d-k-有更好的值么" class="headerlink" title="scale 的值为什么是 $\sqrt{d_k}$, 有更好的值么?"></a>scale 的值为什么是 $\sqrt{d_k}$, 有更好的值么?</h2><p>从上一节的第一步的证明, 可以发现, scale 的值为 $\sqrt{d_k}$ 其实是把 $q \cdot k^{\top}$ 归一化成了一个均值为 0 , 方差为 1 的向量。</p>
<p>至于是不是最好呢? 不好说, 因为参数的分布我们不太清楚。苏神曾经试图求解了一些常用分布的最佳 scale 值, 感兴趣的可以看下：<a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/9812">https://spaces.ac.cn/archives/9812</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>转载： Attention为什么要除以$\sqrt{d}$</p><p><a href="https://nerozac.com/2024/06/02/Attention为什么要除以根号d/">https://nerozac.com/2024/06/02/Attention为什么要除以根号d/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Jiawei Li</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2024-06-02</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2024-06-02</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Transformer/">Transformer</a><a class="link-muted mr-2" rel="tag" href="/tags/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/">好文转载</a><a class="link-muted mr-2" rel="tag" href="/tags/NLP/">NLP</a></div><div class="sharethis-inline-share-buttons"></div><script src="nerozac.com" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2024/06/02/LLMs-step-into-the-3D-World/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">When LLMs step into the 3D World A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2024/06/02/DSF%E6%9C%8D%E5%8A%A1%E5%99%A8cuda%E9%A9%B1%E5%8A%A8/"><span class="level-item">DSF服务器cuda驱动</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://github.com/adrianJW421/MyPicBed/raw/main/theme/icarus/jlijz2.jpg" alt="Jiawei Li"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jiawei Li</p><p class="is-size-6 is-block">CSE PhD student at HKUST. Focusing on Large Language Model Agents and 3D databases.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Clear Water Bay, Kowloon, Hong Kong</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">25</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">22</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">28</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/adrianJW421" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/adrianJW421"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="/null"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="/null"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="/null"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://nerozac.com/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">link1</span></span><span class="level-right"><span class="level-item tag">nerozac.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/DSF/"><span class="level-start"><span class="level-item">DSF</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/DSF/%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"><span class="level-start"><span class="level-item">服务器环境配置</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/HKUST/"><span class="level-start"><span class="level-item">HKUST</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/HKUST/%E5%AE%98%E6%96%B9%E6%96%87%E4%BB%B6/"><span class="level-start"><span class="level-item">官方文件</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"><span class="level-start"><span class="level-item">好文转载</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/%E5%A4%A7%E7%A5%9E%E8%AF%AD%E5%BD%95/"><span class="level-start"><span class="level-item">大神语录</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/"><span class="level-start"><span class="level-item">技术百科</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/MacOS/"><span class="level-start"><span class="level-item">MacOS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/computer-vision/"><span class="level-start"><span class="level-item">computer_vision</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/gitlab/"><span class="level-start"><span class="level-item">gitlab</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"><span class="level-start"><span class="level-item">三维重建</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/%E7%B3%BB%E7%BB%9F%E7%8E%AF%E5%A2%83%E5%8F%8A%E7%BC%96%E8%AF%91/"><span class="level-start"><span class="level-item">系统环境及编译</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/%E8%A7%86%E9%A2%91%E7%BC%96%E8%BE%91/"><span class="level-start"><span class="level-item">视频编辑</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><span class="level-start"><span class="level-item">论文阅读</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/3DLLM/"><span class="level-start"><span class="level-item">3DLLM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLMAgent/"><span class="level-start"><span class="level-item">LLMAgent</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/PCDM/"><span class="level-start"><span class="level-item">PCDM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/awesome%E5%90%88%E9%9B%86/"><span class="level-start"><span class="level-item">awesome合集</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"><span class="level-start"><span class="level-item">三维重建</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/"><span class="level-start"><span class="level-item">技术百科</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-10-11T11:43:52.000Z">2025-10-11</time></p><p class="title"><a href="/2025/10/11/resilio_sync%E5%8A%A0mweb/">告别 rsync 和 SSH：我在 iPadOS 上折腾 Markdown 同步的最终解决方案</a></p><p class="categories"><a href="/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/">技术百科</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-09-29T14:33:52.231Z">2025-09-29</time></p><p class="title"><a href="/2025/09/29/%E7%BB%93%E5%90%88Zotero%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%9A%84%E5%A5%BD%E6%96%B9%E6%B3%95/">用Zotero管理论文, 并养成良好总结的习惯</a></p><p class="categories"><a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a> / <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/">技术百科</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-09T03:02:14.422Z">2024-09-09</time></p><p class="title"><a href="/2024/09/09/school-calendar/">HKUST 校历</a></p><p class="categories"><a href="/categories/HKUST/">HKUST</a> / <a href="/categories/HKUST/%E5%AE%98%E6%96%B9%E6%96%87%E4%BB%B6/">官方文件</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-04T06:16:39.096Z">2024-09-04</time></p><p class="title"><a href="/2024/09/04/FreeReg/">FREEREG ``利用预训练扩散模型和单目深度估计器的图像到点云配准</a></p><p class="categories"><a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a> / <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/PCDM/">PCDM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-08-13T08:32:20.181Z">2024-08-13</time></p><p class="title"><a href="/2024/08/13/LLMAgent-current-and-future/">智源大会2024 LLM Agent - Current and Future</a></p><p class="categories"><a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a> / <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLMAgent/">LLMAgent</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/10/"><span class="level-start"><span class="level-item">十月 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/09/"><span class="level-start"><span class="level-item">九月 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">九月 2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">八月 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/07/"><span class="level-start"><span class="level-item">七月 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/06/"><span class="level-start"><span class="level-item">六月 2024</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/05/"><span class="level-start"><span class="level-item">五月 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/3DLLM/"><span class="tag">3DLLM</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CUDA%E9%A9%B1%E5%8A%A8/"><span class="tag">CUDA驱动</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DSF/"><span class="tag">DSF</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/HKUST/"><span class="tag">HKUST</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLMAgent/"><span class="tag">LLMAgent</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MacOS/"><span class="tag">MacOS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PCDM/"><span class="tag">PCDM</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transformer/"><span class="tag">Transformer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/awesome%E5%90%88%E9%9B%86/"><span class="tag">awesome合集</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/computer-vision/"><span class="tag">computer_vision</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/conda-forge/"><span class="tag">conda-forge</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/gcc/"><span class="tag">gcc</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/gitlab/"><span class="tag">gitlab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"><span class="tag">三维重建</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8D%9A%E5%A3%AB%E7%94%9F%E6%B6%AF/"><span class="tag">博士生涯</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%96%E6%8E%A5%E8%AE%BE%E5%A4%87/"><span class="tag">外接设备</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%A7%E7%A5%9E%E8%AF%AD%E5%BD%95/"><span class="tag">大神语录</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"><span class="tag">好文转载</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AE%98%E6%96%B9%E6%96%87%E4%BB%B6/"><span class="tag">官方文件</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/"><span class="tag">技术百科</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"><span class="tag">服务器</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%82%B9%E4%BA%91/"><span class="tag">点云</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%B3%BB%E7%BB%9F%E7%8E%AF%E5%A2%83%E5%8F%8A%E7%BC%96%E8%AF%91/"><span class="tag">系统环境及编译</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A7%86%E9%A2%91%E7%BC%96%E8%BE%91/"><span class="tag">视频编辑</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><span class="tag">论文阅读</span><span class="tag">10</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Zac&#039;s Blog</a><p class="is-size-7"><span>&copy; 2025 Jiawei Li</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/adrianJW421"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>