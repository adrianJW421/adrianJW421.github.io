{"posts":[{"title":"3D point network distillation from an open-vocabulary 2D image model via 3D fusion","text":"摘要: “a 3D point network distilled from an open-vocabulary 2D image model through 3D fusion”指的是通过三维融合技术，从一个开放词汇的二维图像模型中提炼出一个三维点网络。在这种情况下，”distilled”表达的是从二维模型中提取和转化信息到三维模型的过程。”3D fusion”则是指结合多个来源或多种类型的数据来增强三维点网络的构建或功能。总的来说，这一技术通过结合二维视觉数据和三维处理技术，实现对三维场景的更精确理解和表示。 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 [toc] 从二维图像模型中提取并创建三维点网络〓 ReTURN 〓 从二维图像模型中提取并创建三维点网络的过程，可以大致分为以下几个步骤： 二维图像模型的训练和应用：首先，需要有一个预训练的开放词汇二维图像模型。这个模型能够识别和处理图像中的各种对象，不受限于预定义的类别。这是基础，确保模型具有广泛的视觉理解能力。 三维融合技术的应用：接下来，使用三维融合技术将二维图像模型中的知识和信息转化为三维数据的表示。三维融合可能涉及多个数据源，比如将从二维图像中提取的特征与从其他传感器（如深度传感器）获取的三维信息结合起来。 三维点网络的构建：在融合的基础上，构建一个三维点网络。这个网络专门处理三维空间中的点云数据，能够理解和解释这些数据所代表的场景结构和内容。点网络通常需要调整和优化，以适应从二维转化来的数据特性和三维空间的特点。 模型的细化和应用：最后，通过进一步训练和调整，这个三维点网络可以用于具体的应用，如三维场景理解、对象识别和位置估计等。这个过程中，网络通过处理实际的三维数据来完善其性能和准确度。 从一个开放词汇的二绑图像模型中提取和转化知识到三维点网络，最终得到一个能够进行高效且精确的三维场景分析和理解的工具。这种技术尤其适用于那些需要高级三维视觉能力的应用，比如自动驾驶、机器人导航等。","link":"/2024/06/06/3d-point-network-distillation-via-3d-fusion/"},{"title":"转载： Attention为什么要除以$\\sqrt{d}$","text":"摘要: Attention 的计算公式中 Attention $(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{\\top}}{\\sqrt{d}}\\right)$ 为什么要除以 $\\sqrt{d}$ ? 原文链接： https://mp.weixin.qq.com/s/3o0NgpFPKS1RNICNuMuygg .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 800px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 来自原论文的分析 梯度消失的原因 $d_k$ 变大, $q \\cdot k^{\\top}$ 方差会变大。 方差变大会导致向量之间元素的差值变大。 softmax 退化为 argmax softmax 什么情况下会梯度消失 scale 的值为什么是 $\\sqrt{d_k}$, 有更好的值么? 来自原论文的分析〓 ReTURN 〓 《Attention is All Your Need》的原论文给出了一个粗略的答案。 While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ [3]. We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by $\\frac{1}{\\sqrt{d_k}}$. 当 $d_k$ 的值变大的时候，softmax 函数会导致梯度消失问题，因此设置了一个 softmax 的 temperature 来缓解这个问题。这里 temperature 被设置为了 $\\sqrt{d_k}$ ，也就是乘上 $\\frac{1}{\\sqrt{d_k}}$.延伸讨论： 为什么会导致梯度消失? 为什么是 $\\sqrt{d_k}$, 有更好的值么? 梯度消失的原因〓 ReTURN 〓 如果 $d_k$ 变大, $q \\cdot k^{\\top}$ 方差会变大。 方差变大会导致向量之间元素的差值变大。 元素的差值变大会导致 softmax 退化为 argmax, 也就是最大值 softmax 后的值为 1 , 其他值则为 0 。 softmax 只有一个值为 1 的元素, 其他都为 0 的话, 反向传播的梯度会变为 0 , 也就是所谓的梯度消失。 $d_k$ 变大, $q \\cdot k^{\\top}$ 方差会变大。假设 $q$ 和 $k$ 的向量长度为 $d_k$ ，均值为 0 , 方差为 1 。则 $q$ 和 $k$ 的点积的方差为:$$\\begin{aligned}\\operatorname{var}\\left[q \\cdot k^{\\top}\\right] &amp; =\\operatorname{var}\\left[\\sum_{i=1}^{d_k} q_i \\times k_i\\right] \\&amp; =\\sum_{i=1}^{d_k} \\operatorname{var}\\left[q_i \\times k_i\\right] \\&amp; =\\sum_{i=1}^{d_k} \\operatorname{var}\\left[q_i\\right] \\times \\operatorname{var}\\left[k_i\\right] \\&amp; =\\sum_{i=1}^{d_k} 1 \\&amp; =d_k\\end{aligned}$$ 当 $d_k$ 变大时, 方差变大。 方差变大会导致向量之间元素的差值变大。方差变大就是代表了数据之间的差异性变大。看上去显然， 如果非要给出证明， 可以将这个问题换一个问题来侧面回答这个问题。新的问题假设向量是通过独立同分布的数据采样出来的 $d_k$ 个数据, 那么这 $d_k$ 个数的最大值的期望是多少? 因为分布很多, 这里只给出最常用的正态分布的证明, 详细证明见:http://www.gautamkamath.com/writings/gaussian_max.pdf 这里只给出结论如下:Theorem 1. Let $Y=\\max _{1 \\leq i \\leq n} X_i$, where $X_i \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)$ are i.i.d. random variables. Then$$\\frac{1}{\\sqrt{\\pi \\log 2}} \\sigma \\sqrt{\\log n} \\leq \\mathbf{E}[Y] \\leq \\sqrt{2} \\sigma \\sqrt{\\log n} .$$ 从期望的下界, 可以看出, 方差越大, 最大值的期望越大。同时还有个结论就是 $d_k$ 越大, 最大值的期望也越大。由于正太分布是对称的, 最小值就是最大值取负号。 所以方差变大, 数据分布的最大最小值的差值变大了, 也就从侧面证明了向量元素之间的差值变大了。 softmax 退化为 argmax对于 softmax 函数中的每个分量 $\\operatorname{softmax}\\left(x_i\\right)$, 我们可以写成:$$\\operatorname{softmax}\\left(x_i\\right)=\\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$$ 当 $x_k$ 是最大的元素时, $e^{x_k}$ 会显著大于其他 $e^{x_i}$ (其中 $i \\neq k$ ), 尤其是当这些 $x_i$ 和 $x_k$ 之间的差距变得非常大时。为了更清楚地看出这一点, 我们将 $x_i$ 的每个元素表示成最大元素 $x_k$ 减去一个差值 $\\Delta_i$, 即 $x_i=x_k-\\Delta_i$, 其中 $\\Delta_k=0$ 。 因此, softmax 函数可以重写为:$$\\operatorname{softmax}\\left(x_i\\right)=\\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}=\\frac{e^{x_k-\\Delta_i}}{\\sum_{j=1}^n e^{x_k-\\Delta_j}}$$ 由于 $e^{x_k}$ 是公因子, 根据 $\\exp$ 的性质, 可以提出来并且约掉:$$\\operatorname{softmax}\\left(x_i\\right)=\\frac{e^{x_k} e^{-\\Delta_i}}{e^{x_k} \\sum_{j=1}^n e^{-\\Delta_j}}=\\frac{e^{-\\Delta_i}}{\\sum_{j=1}^n e^{-\\Delta_j}}$$ 当 $\\Delta_i$ 非常大（即 $x_i$ 远小于 $x_k$ ）时, $e^{-\\Delta_i}$ 会接近于 0。因此, 除了 $\\Delta_k=0$ 以外的所有项,其他项 $e^{-\\Delta_j}$ 都会非常小, 可以忽略不计(其实都不用非常小, $e^{-5}=0.0067379 \\ldots, e^{-10}=4.534-05$, 只要差值大于 10 , 就可以忽略不计了)。于是, 对于 $i=k$ :$$\\operatorname{softmax}\\left(x_k\\right) \\approx \\frac{1}{1}=1$$ 而对于 $i \\neq k$ :$$\\operatorname{softmax}\\left(x_i\\right) \\approx 0$$ 所以说当输入向量 $\\mathbf{x}$ 的方差变得非常大时, softmax 函数将会趋近于将最大的元素赋值为 1 , 而其他元素赋值为 0 , 也就是是 argmax 函数。用公式表示的话:$$\\lim _{\\operatorname{var}(\\mathbf{x}) \\rightarrow \\infty} \\operatorname{softmax}(\\mathbf{x})=\\operatorname{argmax}(\\mathbf{x})$$ 所以方差变大时， softmax 函数会退化为 argmax 函数。这里我们可以做个实验看一下: 1234567891011121314151617181920import numpy as npn = 10x1 = np.random.normal(loc=0, scale=1, size=n)x2 = np.random.normal(loc=0, scale=np.sqrt(512), size=n)print('x1最大值和最小值的差值:', max(x1) - min(x1))print('x1最大值和最小值的差值:', max(x2) - min(x2))def softmax(x): return np.exp(x) / np.sum(np.exp(x), keepdims=True)def softmax_grad(y): return np.diag(y) - np.outer(y, y)ex1 = softmax(x1)ex2 = softmax(x2)print('softmax(x1) =', ex1)print('softmax(x2) =', ex2) 其结果为： 1234567x1最大值和最小值的差值: 1.8973472870218264x1最大值和最小值的差值: 66.62254341144866softmax(x1) = [0.16704083 0.21684976 0.0579299 0.05408421 0.16109133 0.14433417 0.03252007 0.05499126 0.04213939 0.06901908]softmax(x2) = [4.51671361e-19 2.88815837e-21 9.99999972e-01 3.02351231e-17 3.73439970e-25 8.18066523e-13 2.78385563e-08 1.16465424e-29 7.25661271e-20 3.21813750e-21] 可以看出, 在方差为 $\\sqrt{512}$ 的时候, softmax 只有第三个元素接近 1 , 其他都几乎为 0 . softmax 什么情况下会梯度消失我们来对 softmax 函数进行求导。定义 softmax 为$$\\operatorname{softmax}\\left(z_i\\right)=\\frac{\\exp \\left(z_i\\right)}{\\sum_j \\exp \\left(z_j\\right)}$$ 假设我们有一个向量 $\\mathbf{z}=\\left[z_1, z_2, \\ldots, z_n\\right]$, softmax 函数的输出是一个向量 $\\mathbf{y}=\\left[y_1, y_2, \\ldots, y_n\\right]$ , 其中:$$y_i=\\operatorname{softmax}\\left(z_i\\right)=\\frac{\\exp \\left(z_i\\right)}{\\sum_{j=1}^n \\exp \\left(z_j\\right)}$$ 我们需要计算 softmax 函数的导数, 即 $\\frac{\\partial y_i}{\\partial z_k}$, 分为两种情况: 当 $i=k$ 当 $i \\neq k$ 首先, 计算 $y_i$ 对 $z_k$ 的导数： 当 $i=k$ 时$$\\frac{\\partial y_i}{\\partial z_i}=\\frac{\\partial}{\\partial z_i}\\left(\\frac{\\exp \\left(z_i\\right)}{\\sum_j \\exp \\left(z_j\\right)}\\right)$$ 使用商的导数法则, 我们得到:$$\\frac{\\partial y_i}{\\partial z_i}=\\frac{\\exp \\left(z_i\\right) \\sum_j \\exp \\left(z_j\\right)-\\exp \\left(z_i\\right) \\exp \\left(z_i\\right)}{\\left(\\sum_j \\exp \\left(z_j\\right)\\right)^2}$$ 化简得到:$$\\frac{\\partial y_i}{\\partial z_i}=\\frac{\\exp \\left(z_i\\right)\\left(\\sum_j \\exp \\left(z_j\\right)-\\exp \\left(z_i\\right)\\right)}{\\left(\\sum_j \\exp \\left(z_j\\right)\\right)^2}=\\frac{\\exp \\left(z_i\\right)}{\\sum_j \\exp \\left(z_j\\right)}\\left(1-\\frac{\\exp \\left(z_i\\right)}{\\sum_j \\exp \\left(z_j\\right)}\\right)$$ 即:$$\\frac{\\partial y_i}{\\partial z_i}=y_i\\left(1-y_i\\right)$$2. 当 $i \\neq k$ 时$$\\frac{\\partial y_i}{\\partial z_k}=\\frac{\\partial}{\\partial z_k}\\left(\\frac{\\exp \\left(z_i\\right)}{\\sum_j \\exp \\left(z_j\\right)}\\right)$$ 同样使用商的导数法则, 我们得到:$$\\frac{\\partial y_i}{\\partial z_k}=\\frac{0 \\cdot \\sum_j \\exp \\left(z_j\\right)-\\exp \\left(z_i\\right) \\exp \\left(z_k\\right)}{\\left(\\sum_j \\exp \\left(z_j\\right)\\right)^2}=-\\frac{\\exp \\left(z_i\\right) \\exp \\left(z_k\\right)}{\\left(\\sum_j \\exp \\left(z_j\\right)\\right)^2}$$ 即:$$\\frac{\\partial y_i}{\\partial z_k}=-y_i y_k$$ 两种情况合并一下 将两种情况合并, softmax 的导数可以表示为：$$\\frac{\\partial y_i}{\\partial z_k}=y_i\\left(\\delta_{i k}-y_k\\right)$$ 其中, $\\delta_{i k}$ 是 Kronecker delta 函数, 定义为:$$\\delta_{i k}= \\begin{cases}1, &amp; \\text { if } i=k \\ 0, &amp; \\text { if } i \\neq k\\end{cases}$$ 最终可以用 Jacobian 矩阵表示, Jacobians 矩阵的第 $i$ 行和第 $k$ 列元素是 $\\frac{\\partial y_i}{\\partial z_k}$ :$$\\mathbf{J}=\\left[\\begin{array}{cccc}y_1\\left(1-y_1\\right) &amp; -y_1 y_2 &amp; \\cdots &amp; -y_1 y_n \\-y_2 y_1 &amp; y_2\\left(1-y_2\\right) &amp; \\cdots &amp; -y_2 y_n \\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\-y_n y_1 &amp; -y_n y_2 &amp; \\cdots &amp; y_n\\left(1-y_n\\right)\\end{array}\\right]$$ 然后与第三步的世界线交汇，好玩的来了 在第三步中我们证明了当方差变大的时候, softmax 退化成了 argmax, 也就是变成一个只有一个 1 其他全为 0 的向量。 这个向量带入到上面的雅可比矩阵会发生什么? 我们发现对于任意的 $y_k=1, y_{j \\neq k}=0$ 的向量来说, 雅可比矩阵变成了一个全 0 矩阵。 也就是说梯度全为 0 了。到这里才算是证明了为什么 $q \\cdot k^{\\top}$ 的方差不能太大, 太大了就梯度消失。 梯度实验 我们同样做个实验，看看梯度到底为多少。 123456789101112131415161718192021import numpy as npn = 10x1 = np.random.normal(loc=0, scale=1, size=n)x2 = np.random.normal(loc=0, scale=np.sqrt(512), size=n)print('x1最大值和最小值的差值:', max(x1) - min(x1))print('x1最大值和最小值的差值:', max(x2) - min(x2))def softmax(x): return np.exp(x) / np.sum(np.exp(x), keepdims=True)def softmax_grad(y): return np.diag(y) - np.outer(y, y)ex1 = softmax(x1)ex2 = softmax(x2)print('softmax(x1) =', ex1)print('max of gradiant of softmax(x1) =', np.max(softmax_grad(ex1)))print('softmax(x2) =', ex2)print('max gradiant of softmax(x2) =', np.max(softmax_grad(ex2))) 其结果为： 123456789x1最大值和最小值的差值: 1.8973472870218264x1最大值和最小值的差值: 66.62254341144866softmax(x1) = [0.16704083 0.21684976 0.0579299 0.05408421 0.16109133 0.14433417 0.03252007 0.05499126 0.04213939 0.06901908]max of gradiant of softmax(x1) = 0.1698259433168865softmax(x2) = [4.51671361e-19 2.88815837e-21 9.99999972e-01 3.02351231e-17 3.73439970e-25 8.18066523e-13 2.78385563e-08 1.16465424e-29 7.25661271e-20 3.21813750e-21]max gradiant of softmax(x2) = 2.7839373695215386e-08 可以看出，在方差为 的时候，长度仅仅为10的向量x2，其梯度就已经快没有了，最大值为2.78e-8。 而如果将方差控制在1，则最大的梯度为0.1698 scale 的值为什么是 $\\sqrt{d_k}$, 有更好的值么?从上一节的第一步的证明, 可以发现, scale 的值为 $\\sqrt{d_k}$ 其实是把 $q \\cdot k^{\\top}$ 归一化成了一个均值为 0 , 方差为 1 的向量。 至于是不是最好呢? 不好说, 因为参数的分布我们不太清楚。苏神曾经试图求解了一些常用分布的最佳 scale 值, 感兴趣的可以看下：https://spaces.ac.cn/archives/9812","link":"/2024/06/02/Attention%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E9%99%A4%E4%BB%A5%E6%A0%B9%E5%8F%B7d/"},{"title":"Buffer of Thoughts - Thought-Augmented Reasoning with Large Language Models","text":"摘要: 本文介绍了Buffer of Thoughts（BoT），这是一种新颖且多功能的思想增强推理方法，用于提高大型语言模型（LLMs）的准确性、效率和鲁棒性。具体来说，我们提出了meta-buffer，用于存储一系列从不同任务的问题解决过程中提取的有用高级思想，即thought-template。然后，对于每个问题，我们检索相关的thought-template，并自适应地用具体的推理结构实例化它，以进行高效推理。 为了保证可扩展性和稳定性，我们进一步提出了buffer-manager，用于动态更新meta-buffer，从而随着更多任务的解决，增强meta-buffer的容量。 我们在10个具有挑战性的推理密集任务上进行了广泛的实验，取得了显著的性能提升：在24点游戏中提高了11%，在几何图形中提高了20%，在一步将军中提高了51%。进一步的分析表明，我们的BoT具有卓越的泛化能力和模型鲁棒性，而其成本仅为多次查询提示方法（例如，tree/graph of thoughts）的12%。 值得注意的是，我们发现我们的Llama3-8B + BoT有可能超过Llama3-70B模型。 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 1 Introduction 2 Related Work 2.1 检索增强语言模型 2.2 基于提示的大型语言模型推理 2.3 类比推理 3 Buffer of Thoughts 3.1 Problem Distiller 3.2 Thought-Augmented Reasoning with Meta Buffer 3.2.1 动机 3.2.2 思想模板 3.2.3 模板检索 3.2.4 实例化推理 3.3 Buffer Manager 3.3.1 模板提炼 3.3.2 Meta-Buffer的动态更新 4 Experiments 4.1 数据集和任务 4.2 实现和基线 4.3 BoT Achieves Better Accuracy, Efficiency and Robustness 4.3.1 推理准确性 4.3.2 推理效率 4.3.3 推理鲁棒性 5 Model Analysis 5.1 Thought-Templates的分布分析 5.2时间成本的分布分析 5.3 模型大小与性能的更好权衡 6 Ablation Study 6.2 Problem-Distiller的影响 6.3 Meta-Buffer的影响 6.4 Buffer-Manager的影响 7 Discussion 8 Conclusion 1 Introduction〓 ReTURN 〓 一系列大型语言模型（LLMs）如GPT-4、PaLM和LLaMA在各种推理任务中展现了出色的表现。除了通过扩大模型规模来提高推理性能外，还有更多有效的提示方法进一步增强了LLMs的功能和性能。 我们将这些方法分为两类： （i）单次查询推理：这些方法通常集中于提示工程，其推理过程可以在单次查询内完成。例如，CoT方法在输入查询后附加“Let’s think step by step”来生成推理理由，以提高推理准确性；以及少量示例提示（Few-shot Prompting），它提供与任务相关的示例以辅助答案生成。 （ii）多次查询推理：这些方法通过利用多次LLM查询来引导不同的合理推理路径，从而将复杂问题分解为一系列较简单的子问题，如Least-to-Most、ToT和GoT。 然而，这两类方法都面临一些限制： （1）单次查询推理通常需要预先假设或相关的推理过程示例，这使得手动逐任务设计变得不切实际，因此缺乏普适性和泛化性； （2）由于推理路径的递归扩展，多次查询推理在为每个特定任务寻找独特的内在结构时通常需要大量计算； （3）单次查询和多次查询推理过程都受限于其设计的示例和推理结构，未能从已完成的任务中提取通用的高级指导思想，这些思想对于解决类似问题时提高效率和准确性非常有用。 为了解决这些限制，我们提出了Buffer of Thoughts（BoT），这是一种新颖且多功能的思想增强推理框架，旨在提高LLMs在各种任务中的推理准确性、效率和鲁棒性。具体来说，我们设计了meta-buffer，这是一个轻量级库，包含一系列从不同问题解决过程中提取的通用高级思想（thought-template），这些思想可以跨任务共享。然后，对于每个问题，我们检索相关的思想模板并用具体的推理结构实例化它，以实现高效的思想增强推理。 为了保证BoT的可扩展性和稳定性，我们进一步提出了buffer-manager，用于动态更新meta-buffer，随着更多任务的解决，meta-buffer的容量得到有效增强。 我们的方法有三个关键优势： （i）准确性提升：通过共享的思想模板，我们可以自适应地实例化高级思想来处理不同任务，消除了从头构建推理结构的需求，从而提高了推理准确性； （ii）推理效率：我们的思想增强推理可以直接利用有用的历史推理结构进行推理，无需复杂的多次查询过程，从而提高了推理效率； （iii）模型鲁棒性：从思想检索到思想实例化的过程就像人类的思维过程，使得LLMs能够以一致的方式处理类似问题，从而显著增强我们方法的模型鲁棒性。 我们的实验证明，Buffer of Thoughts在各种具有挑战性的推理密集任务中显著提高了精度、效率和鲁棒性。以下是我们工作的总结： 我们提出了一种新颖的思想增强推理框架Buffer of Thoughts（BoT），以提高基于LLM的推理的准确性、效率和鲁棒性； 我们提出了meta-buffer，用于存储从不同问题中提取的有用高级思想，并自适应地实例化每个思想模板以解决具体任务； 我们设计了buffer-manager，用于从各种解决方案中提取思想模板，并随着更多任务的解决不断提高meta-buffer的容量； 我们在10个具有挑战性的推理密集任务上进行了广泛的实验。我们的BoT在多个任务上取得了显著的性能提升：在24点游戏中提高了11%，在几何图形中提高了20%，在一步将军中提高了51%，而其成本仅为多次查询提示方法的12%。 2 Related Work〓 ReTURN 〓 2.1 检索增强语言模型检索增强型（大型）语言模型作为一种解决幻觉现象并提升语言模型输出质量的方案被提出。当面对一个输入问题时，检索增强型LLM首先查询一个包含数十亿级别tokens的外部数据库，以检索出一个子文本语料库来帮助生成最终答案。值得注意的是，检索增强型LLM在使用更少参数的情况下，实现了比传统LLM更优越的问答性能，并已在多种下游任务中得到了应用，包括多模态生成和生物医学应用。 在本文中，我们构建了一种新型的检索数据库，称为meta-buffer，它包含一系列高级思想而非具体实例，旨在普遍解决基于LLM的推理任务。 2.2 基于提示的大型语言模型推理提示技术显著增强了LLM的算术和常识推理能力。Chain-of-Thought（CoT）提示及其变体（如Least-to-Most、Decomposed Prompting和Auto-CoT）促使LLM将复杂问题分解为更简单的子任务，并系统地解决它们后总结出最终答案。许多研究表明，这些提示方法在广泛的任务和基准测试中表现出了有效性。 诸如Tree-of-Thought和Graph-of-Thought的创新进一步推动了该领域的发展，通过探索动态、非线性的推理路径来扩展LLM的启发式能力。然而，它们面临着资源需求增加和时间复杂度更高的问题，依赖于手动提示设计，并且通常针对特定任务类型。最近的meta提示方法采用相同的任务不可知形式的提示用于各种任务，并递归地引导单个LLM自适应地处理不同的输入查询。然而，这种长meta提示可能需要相当大的上下文窗口，并且这些方法未能利用历史上的有用指导思想来解决潜在的类似任务。 2.3 类比推理类比推理是一种对自然语言推理非常有用的技术。最近的研究表明，LLM可以像人类一样进行类比推理。例如，Analogical Prompting和Thought Propagation提示LLM自生成一组类比问题，然后利用这些类比问题的结果来为输入问题生成解决方案。然而，对自探索问题的具体解决方案可能引入额外的噪音并导致错误积累。最近的Thought-Retriever利用解决过去用户问题时生成的中间思想来处理类似查询，但它仅关注文本理解/生成，而不是一般的推理问题。因此，仍然缺乏一种更高级和通用的类比方法来处理LLM复杂推理。 3 Buffer of Thoughts〓 ReTURN 〓 Overview of Buffer of Thoughts. 在本节中，我们详细介绍了我们的Buffer of Thoughts，并在图2中展示了我们的核心思想增强推理过程。 对于特定任务，我们利用problem-distiller（第3.1节）提取关键的任务特定信息以及相关约束。基于提取的信息，我们在meta-buffer（第3.2节）中搜索，meta-buffer包含一系列高级思想（thought-template），并检索出与该任务最相关的thought-template。 随后，我们用更具任务特定的推理结构实例化检索到的thought-template并进行推理过程。最后，我们使用buffer-manager（第3.3节）总结整个问题解决过程，并提炼高级思想以增加meta-buffer的容量。 3.1 Problem Distiller大多数复杂任务包含隐含约束、复杂的对象关系以及其上下文中的复杂变量和参数。因此，在推理阶段，LLM需要克服三个主要挑战：提取重要信息、识别潜在约束以及执行准确推理。这些挑战会对单一的LLM构成显著负担。因此，我们将任务信息的提取和理解阶段与最终推理阶段分开，通过在推理过程中添加一个问题提炼器来实现。 具体而言，我们设计了一个meta prompt $\\phi$，首先提炼并形式化任务信息。提炼后的任务信息可以表示为： $$x_d = LLM(\\phi(x)),$$ 其中$x$是任务陈述。由于篇幅限制，我们将问题提炼器的详细meta prompt放在附录A.2中。 问题凝练与翻译 我们使用问题提炼器从输入任务中提取关键元素，重点关注：（1）问题解决所需的基本参数和变量；（2）输入任务的目标及其相应的约束。然后，我们将这些提炼的信息重新组织成一个清晰、易于理解的格式，以便后续推理阶段使用。 接着，我们将具体问题翻译成高级概念和结构。这个翻译过程将复杂的现实问题（如复杂的数学应用场景）分解为更简单的多步计算，从而便于后续高级思想的检索。 3.2 Thought-Augmented Reasoning with Meta Buffer3.2.1 动机人类在解决问题时，通常会总结并归纳出高级指导原则，然后将其应用于相关问题。受此启发，我们提出了meta-buffer，这是一个包含一系列高级思想（thought-template）的轻量级库，用于解决各种类型的问题。与传统方法不同，我们的高级thought-templates可以在解决不同问题时自适应地实例化，从而增强LLMs的精度和灵活性。 3.2.2 思想模板作为一种高级指导原则，我们的thought-template存储在meta-buffer中，并由buffer-manager从各种问题解决过程中获取。获取thought-templates的详细信息将在第3.3节介绍。由于我们的BoT旨在为各种任务提供通用的推理方法，我们相应地将thought-templates分为六类：文本理解、创意语言生成、常识推理、数学推理、代码编程和应用调度。我们在附录A.1中提供了一些示例thought-templates。这种thought-templates的分类可以方便模板检索，以找到最适合解决不同问题的方法。这里我们将thought-template、模板描述及其对应的类别表示为($T_i, D_{T_i}, C_k$)，其中$i$表示meta-template的索引，$k \\in \\mathbb{Z}^{+}$且$1 \\leq k \\leq 6$，表示$C_k$属于六个类别之一，而$D_{T_i}$是thought-template的描述。 3.2.3 模板检索对于每个任务，我们的BoT通过计算描述$D_{T_i}$与提炼问题$x_d$之间的嵌入相似性，检索出一个与提炼问题$x_d$高度相似的thought-template $T_i$。检索过程可以表述为： $$j=\\operatorname{argmax}i\\left(\\operatorname{Sim}\\left(f\\left(x_d\\right),\\left{f\\left(D{T_i}\\right)\\right}{i=1}^N\\right)\\right), \\quad \\text { where } \\quad \\operatorname{Sim}\\left(f\\left(x_d\\right),\\left{f\\left(D{T_i}\\right)\\right}_{i=0}^n\\right)&gt;=\\delta,$$ 其中$N$是meta-buffer的大小，$f(\\cdot)$是一个常规的文本嵌入模型，$T_j$表示检索到的thought-template。我们设置一个阈值$\\delta$（推荐范围为0.5至0.7）来确定当前任务是否为新任务。因此，如果$\\operatorname{Sim}\\left(f\\left(x_d\\right),\\left{f\\left(D_{T_i}\\right)\\right}_{i=0}^n\\right)&lt;\\delta$，我们将任务$x$识别为新任务。 3.2.4 实例化推理对于每个具体任务，我们根据当前任务是否为新任务讨论两种情况：第一种情况是我们成功为任务检索到一个thought-template $T_j$。在这种情况下，如图2所示，我们的思想增强推理将通过我们设计的实例化提示（见附录A.3）自适应地实例化为适当的推理结构。例如，在一步将军问题中，我们实例化更新棋盘状态的模板，以逐步解决问题。因此，我们使用提炼信息$x_d$和检索到的模板$T_j$对任务$x$进行实例化推理，并生成其解决方案$S_x$： $$S_x=L L M_{\\text {instantiation }}\\left(x_d, T_j\\right),$$ 其中$L L M_{\\text {instantiation }}$表示带有LLM的实例化推理器。 第二种情况是任务被识别为新任务。为了实现适当的实例化推理，我们准备了三个通用的粗粒度thought-templates供使用。基于提炼的任务信息$x_d$，我们的BoT将自动为推理过程分配一个合适的thought-template。详细的预定义thought-templates包含在附录A.3中。 3.3 Buffer Manager我们提出了buffer-manager，用于总结从每个问题解决过程中获得的高级指导原则和思想。它能够将每个具体解决方案推广到更多问题中，并以thought-templates的形式将关键提炼知识存储在meta-buffer中。与为每个问题临时生成示例或指令的方法相比，我们的buffer-manager可以确保基于LLM的推理在准确性、效率和鲁棒性方面的持久进步。 3.3.1 模板提炼为了提取一个通用的thought-template，我们提出了三步法：（1）核心任务总结：识别并描述问题的基本类型和核心挑战；（2）解决步骤描述：总结解决问题的一般步骤；（3）通用回答模板：基于上述分析，提出一个可以广泛应用于类似问题的解决模板或方法。此外，为了增强模板提炼的泛化能力和稳定性，我们精心设计了两种类型的上下文示例，分别用于生成task内和cross-task的thought-template。Cross-task示例意味着我们选择从一个任务中提炼的模板来解决其他任务的问题，例如用与代码相关的thought-template解决数学问题。从输入任务$x$中提炼的新模板可以表示为： $$T_{\\text {new }}=LLM_{\\text {distill }}\\left(x_d, S_x\\right),$$ 其中$LLM_{\\text {distill }}$是基于LLM的模板提炼器，其初始化提示如下图所示。 3.3.2 Meta-Buffer的动态更新在模板提炼之后，我们需要考虑是否将提炼的模板更新到meta-buffer中。如果我们初始化一个空的meta-buffer或遇到一个没有适当thought-template的问题，提炼的thought-templates将直接存储在meta-buffer中。如果我们使用检索到的thought-template解决问题，在某个thought-template的实例化过程中可能会产生新的见解。因此，为了避免meta-buffer的冗余，同时保持新生成的有用思想，我们将计算$D_{T_{\\text {new }}}$和$\\left{D_{T_i}\\right}_{i=0}^n$的嵌入向量之间的相似性，并按以下规则更新meta-buffer： $$\\operatorname{Max}\\left(\\operatorname{Sim}\\left(f\\left(D_{T_{\\text {new }}}\\right),\\left{f\\left(D_{T_i}\\right)\\right}_{i=0}^n\\right)\\right)&lt;\\delta.$$ 否则，这意味着meta-buffer已经具备解决该任务所需的知识，无需进行更新。我们的动态更新策略有效地减少了模板检索的计算负担，同时确保了meta-buffer的轻量化特性。我们在第6节进一步进行消融研究以分析这一策略。 4 Experiments〓 ReTURN 〓 4.1 数据集和任务为了评估我们提出的Buffer of Thoughts的有效性并与以往方法进行比较，我们考虑了一组多样化的任务和数据集，这些任务需要不同程度的数学和算法推理、领域特定知识和文学创作能力： (a) Game of 24：从ToT中获取，目标是使用四个给定数字各一次形成等于24的算术表达式； (b) 三个BIG-Bench Hard (BBH)任务：几何图形、多步算术二和单词排序； (c) 三个直接从BIG-Bench套件中获取的推理任务：一步将军、企鹅任务（根据给定表格和额外的自然语言信息回答关于企鹅属性的问题），以及日期理解任务（从自然语言描述中推断日期，对日期进行算术运算，并利用全球知识如二月份的天数）； (d) **Python编程谜题(P3)**：一组用Python编写的具有不同难度级别的挑战性编程谜题； (e) **多语种小学数学(MGSM)**：GSM8K数据集的多语种版本，其中包括翻译成包括孟加拉语、日语和斯瓦希里语在内的十种类型多样的语言的示例子集； (f) 莎士比亚十四行诗写作：来自meta-prompting的一个新颖任务，目标是按照严格的押韵方案”ABAB CDCD EFEF GG”写一首十四行诗，并逐字包含三个提供的单词。 4.2 实现和基线为了与以前的方法进行公平比较，我们使用GPT-4作为BoT的基础模型，包括主要实验和消融研究（在第6节）。我们还在NVIDIA A100-PCIE-40GB GPU上使用了Llama3-8B和Llama3-70B进行分析。我们将Buffer of Thoughts与以下提示方法进行比较： 标准提示：这是我们的最基本基线，LLM直接从输入查询生成响应，不包含任何特定的指导输入输出示例或任务描述之外的额外指令。 单次查询方法：包括Zero-shot CoT和PAL，它们使用LLM分析自然语言问题并生成中间推理步骤。我们还包括Expert Prompting，它创建了一个根据输入查询的特定上下文量身定制的专家身份，并将该专家简介整合到输入中，以生成信息丰富的响应。 多次查询方法：包括ToT和GoT，它们使LLM通过考虑多种推理路径和自我评估选择来做出深思熟虑的决策，并确定下一步行动。这些方法还允许在必要时前瞻或回溯以做出全局决策。此外，我们还包括Meta Prompting，它采用了一种有效的支架技术来增强LLM的功能。 4.3 BoT Achieves Better Accuracy, Efficiency and Robustness4.3.1 推理准确性如表1所示，我们的BoT在多个具有挑战性的基准测试中始终优于所有先前的提示方法，特别是在复杂推理任务如Game of 24和Checkmate-in-One中表现尤为突出。以GPT-4为基线，我们的方法在Game of 24中取得了惊人的79.4%的准确性提升，而相比于在此任务上表现良好的ToT，我们也实现了8.4%的准确性提升。此外，相较于最近的Meta-prompting方法，我们在Game of 24上提升了23%，在Geometric Shapes上提升了20%，在Checkmate-in-One上提升了51%。现有方法需要复杂的、迭代的和启发式的搜索策略来逐一解决这些问题。相比之下，我们的BoT利用了thought-templates中的历史见解和信息性指导，并进一步自适应地实例化出更优的推理结构，以解决这些复杂问题。 4.3.2 推理效率除了在准确性上取得显著提升外，作为一种多次查询方法，我们的BoT在各种任务中实现了与单次查询方法相当的推理时间，同时明显少于传统的多次查询方法如ToT。如图3所示，在Game of 24中，单次查询和多次查询方法都需要迭代和启发式搜索来确定可行的解决方案，这一过程特别耗时且效率低下，尤其是多次查询方法，它涉及多次查询搜索和回溯阶段。相比之下，我们的BoT直接以代码格式检索到一个thought-template，从而实例化一个程序来遍历数字和符号的组合，消除了从头构建推理结构的需要。这使得在调用problem-distiller后，只需一次查询就能解决问题，显著减少了复杂推理所需的时间。值得注意的是，我们的BoT平均仅需多次查询方法（如tree of thoughts和meta-prompting）成本的12%。 4.3.3 推理鲁棒性为了更好地评估我们的BoT，我们设计了一种新的评估指标：成功率，用于评估推理鲁棒性。我们从各种基准中随机抽取1000个示例作为测试子集，并在该子集上评估不同方法的表现。 如图4所示，我们重复这一评估过程10次，并取平均准确率作为各基准上不同方法的成功率。与其他方法相比，我们的BoT在各种任务中始终保持更高的成功率，平均成功率超过第二好的方法10%。我们将出色的鲁棒性归因于我们在不同任务推理过程中提炼的thought-templates的强大泛化能力。通过提供来自适当thought-templates的高级思想，我们的方法在不同任务中的稳定性得到了极大增强。 5 Model Analysis〓 ReTURN 〓 5.1 Thought-Templates的分布分析如图5左图所示，我们选择了六个不同的基准，每个基准抽取100个不同任务。从零开始更新meta-buffer，在完成所有抽取的任务后，我们展示了派生出的thought-templates数量。可以观察到，我们的BoT在包含更多多样化场景的MGSM任务中生成了更多的thought-templates。在相对简单的任务中，如Checkmate-in-One和Penguins，BoT产生了更多针对这些特定问题的固定thought-templates。模板的分布表明，我们的BoT能够有效地为不同基准发现适当的thought-templates。 5.2时间成本的分布分析如图5所示，我们测量了BoT推理框架中各组件在不同任务上的平均时间成本。提炼任务信息和模板检索所需时间相对较短，而实例化推理所需时间较长。总体而言，考虑到不同组件的复杂性，我们的BoT实现了相对平衡的时间成本分布，展示了BoT框架的效率。 5.3 模型大小与性能的更好权衡如图6所示，在Game of 24、单词列表排序和Checkmate-in-One任务中，Llama3-8B和Llama-70B模型可能表现不佳。然而，配备我们的BoT后，这两种模型的准确性显著提升。值得注意的是，BoT+Llama3-8B有可能超越单独的Llama3-70B模型。我们的BoT使得较小的模型能够展现出接近甚至超越较大模型的能力，显著缩小了它们在推理能力上的差距。此外，它极大地减少了大型语言模型在处理复杂问题时所需的推理成本。 6 Ablation Study〓 ReTURN 〓 6.2 Problem-Distiller的影响如图7所示，当problem-distiller被禁用时，Llama3-70B和GPT-4的准确性均出现一定程度的下降。对于更复杂的问题，如Game of 24和Checkmate-in-One，准确性下降更为显著，而相对简单的问题如单词列表排序和MGSM则表现出较小的下降。这是因为在简单任务中，LLMs更容易提取关键信息，使得problem-distiller的影响不太明显。相反，在复杂问题中提取关键信息和潜在约束更具挑战性，使得problem-distiller的作用更为突出，这解释了图中所展示的差异。 6.3 Meta-Buffer的影响如图8所示，当meta-buffer被禁用时，Llama3-70B和GPT-4模型在性能上均表现出显著下降，尤其是在需要复杂推理的基准测试中，如Game of 24和Checkmate-in-One。这进一步强调了meta-buffer在解决复杂问题中的优越性。 6.4 Buffer-Manager的影响在这项消融研究中，我们将整个过程分为四轮。在每一轮中，我们从每个基准中随机抽取50个问题并进行推理。在随后的轮次中，我们继续从每个基准中随机抽取另一个50个问题。 如图9所示，随着轮次的增加，配备buffer-manager的模型不断扩展meta-buffer，同时利用先前解决问题中获得的thought-templates来帮助解决后续的类似问题。因此，我们可以观察到BoT的准确性在每一轮中稳步提高。相反，没有buffer-manager的模型未能表现出上升趋势。 此外，我们还测量了推理时间，如图10所示。随着轮次的增加，配备buffer-manager的模型推理效率不断提高。这是因为随着meta-buffer的不断扩展，检索到合适thought-templates的可能性也增加。因此，模型可以避免从头构建推理结构，从而相应地提高推理效率。 7 Discussion〓 ReTURN 〓 局限性与未来方向 尽管我们的方法在提高准确性、保持推理效率和鲁棒性方面取得了显著进展，但在解决需要人类创造力的问题时，我们的方法提升有限，因为这些问题往往不依赖于特定的thought-template。此外，如果我们的BoT使用较弱的模型初始化meta-buffer，由于较弱模型的推理能力和指令遵循能力有限，派生的thought-templates质量可能会不理想。 总体而言，我们的BoT带来了以下未来研究方向： 将外部资源与BoT整合，构建一个类似代理模型的开放域系统。 使thought-templates的提炼过程可优化，这可能显著提升其在处理更复杂任务时的模板质量。 8 Conclusion〓 ReTURN 〓 在这项工作中，我们引入了Buffer of Thoughts，这是一种新颖的缓冲推理框架，利用LLMs来使用先前任务中的预积累经验和方法，并将其作为thought-templates存储在meta-buffer中。我们进一步设计了buffer-manager，以持续改进问题解决过程并动态提炼thought-templates，从而逐步提升LLM的推理能力。我们的BoT在10个具有挑战性的任务中展示了SOTA性能，并为未来的研究和应用提供了有希望的前景。","link":"/2024/06/11/Buffer-of-Thoughts/"},{"title":"Conda Pack","text":"摘要: 移植python环境一般用于以下两种情况： 别人要运行你的代码，但是没有你的python环境，如果要一个一个python包进行安装，则非常麻烦，并且有时候还安装出现问题。 有一个包始终安装不上，但是在你的另一个python环境下有，或者在别人的环境中存在，这个时候就可以将python环境打包，移植到你的电脑上，然后激活该环境，运行对应的项目。 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 conda pack 流程讲解 安装Conda-Pack 进行python环境打包 报错调试 包不一致 无法pack源码编译的包 conda pack 流程讲解〓 ReTURN 〓 安装Conda-Pack在打包之前如果没有conda-pack包的话，需要安装pip install conda-pack 进行python环境打包123456789conda info -e# conda environments:#base * /miniconda38IMavatar /miniconda38/envs/IMavatarIMavatar_v2 /miniconda38/envs/IMavatar_v2deca-env /miniconda38/envs/deca-envflare /miniconda38/envs/flareflare2 /miniconda38/envs/flare2 1conda pack -n $环境名字 -o $压缩包名字.tar.gz 1234# 运行时样例Collecting packages...Packing environment at '/miniconda38/envs/deca-env' to 'deca-env--imavatar-data-prepro-env.tar.gz'[################################## ] | 87% Completed | 2min 19.6s 接下来，传输这个压缩文件&nbsp; $压缩包.tar.gz&nbsp;到你所需要的服务器，并放到anaconda目录的envs文件夹路径下。 12345# 解压这个conda pack的环境tar -zxvf $压缩包.tar.gz -C $环境名字# 实例mkdir /miniconda38/envs/nhatar -zxvf nha.tar.gz -C /miniconda38/envs/nha &nbsp;注意&nbsp; conda pack方法无法跨操作系统进行环境迁移。只能linux到linux，windows到windows，mac到mac。 报错调试包不一致12345678910111213141516171819202122232425ollecting packages...CondaPackError: Files managed by conda were found to have been deleted/overwritten in thefollowing packages:- pyyaml 6.0: lib/python3.7/site-packages/PyYAML-6.0-py3.7-linux-x86_64.egg-info/PKG-INFO lib/python3.7/site-packages/PyYAML-6.0-py3.7-linux-x86_64.egg-info/SOURCES.txt lib/python3.7/site-packages/PyYAML-6.0-py3.7-linux-x86_64.egg-info/dependency_links.txt + 5 others- dataclasses 0.8: lib/python3.7/site-packages/dataclasses-0.8.dist-info/INSTALLER lib/python3.7/site-packages/dataclasses-0.8.dist-info/LICENSE.txt lib/python3.7/site-packages/dataclasses-0.8.dist-info/METADATA + 5 others- pytorch 1.7.0: lib/python3.7/site-packages/torch-1.7.0-py3.7.egg-info/PKG-INFO lib/python3.7/site-packages/torch-1.7.0-py3.7.egg-info/SOURCES.txt lib/python3.7/site-packages/torch-1.7.0-py3.7.egg-info/dependency_links.txt + 3 othersThis is usually due to `pip` uninstalling or clobbering conda managed files,resulting in an inconsistent environment. Please check your environment forconda/pip conflicts using `conda list`, and fix the environment by ensuringonly one version of each package is installed (conda preferred). 首先，进入你想要打包的那个环境，运行以下命令： 1conda list 之后再进行pack，这个报错自己就消失了 无法pack源码编译的包报错信息 123456Collecting packages...CondaPackError: Cannot pack an environment with editable packagesinstalled (e.g. from `python setup.py develop` or `pip install -e`). Editable packages found:- /data/lijiawei421/eskandar/projects/3Drecon/point_avatar_deps/pytorch3d pack方式变动 1conda pack -n point-avatar -o pointavatar.tar.gz --ignore-editable-packages 解压缩使用 1234mkdir ~/ananconda/envs/point-avatartar -xf pointavatar.tar.gz -C ~/ananconda/envs/point-avatarcd ~/ananconda/envs/point-avatarsource ./bin/activate","link":"/2024/06/03/Conda-Pack-%E5%B7%A5%E5%85%B7/"},{"title":"DSF服务器cuda驱动","text":"摘要: 在实验室环境中安装CUDA驱动 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 500px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 问题起源 解法 安装流程 环境配置项 常见问题 显示segment error Existing package manager installation of the driver found 问题起源〓 ReTURN 〓 Q： zxcpu1-4机器上，cuda安装位置是啥地方？ 当我想指定一下cuda_home，发现/usr/local没有任何默认的cuda驱动。 解法A: 都是自己安装的，管理员没有安装默认版本。 安装cuda是可以本地操作的， 先找到nvidia的对应版本driver的cuda 的地址，下载文件，下载之前用google搜索一遍检查是否是最新的： 123wget https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda_12.3.0_545.23.06_linux.runwget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run 然后直接执行即可 1sh cuda_12.3.0_545.23.06_linux.run 然后就会进入配置页面，如果不是sudo，记得一定要修改安装的位置，不是在options里面改，一定是toolkits里面修改 安装流程〓 ReTURN 〓 环境配置项12345678910# CUDAexport PATH=&quot;/{你的路径}/cuda-12.3/bin:$PATH&quot;export LD_LIBRARY_PATH=&quot;/{你的路径}/cuda-12.3/lib64:$LD_LIBRARY_PATH&quot;export CUDA_HOME=&quot;/{你的路径}/cuda-12.3&quot;export CUDA_TOOLKIT_ROOT_DIR=$CUDA_HOMEexport LD_LIBRARY_PATH=&quot;$CUDA_HOME/extras/CUPTI/lib64:$LD_LIBRARY_PATH&quot;export LIBRARY_PATH=$CUDA_HOME/lib64:$LIBRARY_PATHexport LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATHexport CFLAGS=&quot;-I$CUDA_HOME/include $CFLAGS&quot;export LD_LIBRARY_PATH=&quot;$CUDA_HOME/include:$LD_LIBRARY_PATH&quot; 常见问题〓 ReTURN 〓 显示segment error运行安装程序后出现报错： 12log file not open.Segmentation fault(core dumped) 原因： 文件/tmp/cuda-installer.log没有删除， 删除了就好了。 Existing package manager installation of the driver found如下图所示。 选择continue即可。","link":"/2024/06/02/DSF%E6%9C%8D%E5%8A%A1%E5%99%A8cuda%E9%A9%B1%E5%8A%A8/"},{"title":"FID图像相似距离","text":"摘要: .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 [toc] 简介〓 ReTURN 〓 FID指标，全称为Fréchet Inception Distance，是一种用于评估生成式模型效果的度量指标。 章节2〓 ReTURN 〓","link":"/2024/06/03/FID%E5%9B%BE%E5%83%8F%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"},{"title":"FREEREG &#96;&#96;利用预训练扩散模型和单目深度估计器的图像到点云配准","text":"摘要: 图像与点云之间的跨模态特征匹配是图像到点云配准的基础问题。然而，由于图像和点云之间的模态差异，现有的度量学习方法在特征匹配上难以学习到稳健且具有区分性的跨模态特征。我们提出了一种方法，首先通过预训练的大规模模型统一图像和点云的模态，然后在相同模态内建立稳健的对应关系。我们展示了通过深度到图像的扩散模型提取的中间特征（称为扩散特征），在图像和点云之间具有语义一致性，这使得建立粗略但稳健的跨模态对应关系成为可能。进一步地，我们提取了由单目深度估计器生成的深度图上的几何特征。通过匹配这些几何特征，我们显著提高了由扩散特征生成的粗略对应关系的准确性。大量实验表明，在没有进行任何I2P配准任务训练的情况下，直接利用这两种特征可以实现准确的图像到点云配准。在三个公共室内和室外基准数据集上，所提方法在Inlier Ratio上平均提升了$20.6%$，Inlier Number提高了$3.0 \\times$，Registration Recall提升了$48.6%$，优于现有的最先进方法。代码和附加结果可在 https://whu-usi3dv.qithub.io/FreeReq/ 上获取。 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 FREEREG: 利用预训练扩散模型和单目深度估计器的图像到点云配准 摘要 引言 相关工作 方法 初步介绍：稳定扩散与 ControlNet 跨模态数据上的扩散特征 跨模态数据上的几何特征 融合特征用于 I2P Registration 实验 实验协议 三个基准上的结果 更多分析 消融扩散特征提取 消融特征融合权重 局限性 结论 引言图像到点云（I2P）配准需要估计图像与点云之间的像素到点的对应关系，以确定图像相对于点云的 $\\mathrm{SE}(3)$ 姿态。这是许多任务的前提条件，例如同时定位与地图构建（Zhu et al, 2022）、三维重建（Dong et al, 2020）、分割（Guo et al., 2020）和&nbsp;视觉定位&nbsp;（Sarlin et al., 2023）。 为了建立像素到点的对应关系，我们必须匹配图像和点云之间的特征。然而，为图像和点云学习稳健的跨模态特征是困难的。大多数现有方法（Feng et al., 2019; Wang et al., 2021; Fham et al., 2020; Jjang &amp; Saripall, 2022; Li et al., 2023）依赖于对比损失、三元组损失或 InfoCE 损失等度量学习方法，强制对齐同一对象的2D和3D特征。然而，由于图像捕捉的是外观而点云表示的是结构，直接对齐跨模态数据不可避免地导致收敛效果差。因此，跨模态度量学习面临特征鲁棒性差（Wang et al., 2021）和泛化能力有限（Li et al., 2023）的问题。 图 1：左图：FreeReg 统一了图像和点云的模态，从而使得单模态匹配可以建立跨模态对应关系。右图：FreeReg 不需要在 I2P 任务上进行任何训练，即可实现对室内和室外场景中 RGB 图像与点云的配准，甚至在小重叠、大视角变化和稀疏点密度等挑战性情况下也能有效配准。 本文提出了一种新方法，称为 FreeReg，通过利用最新的大规模扩散模型（Rombach et al., 2022; Zhang &amp; Agrawala, 2023; Mou et al., 2023）和单目深度估计器（Bhat et al., 2023; Yin et al., 2023），在图像和点云之间建立稳健的跨模态对应关系。FreeReg 避免了困难的跨模态度量学习，并且不需要在 I2P 任务上进行训练。如图 1 所示，其核心思想是通过这些大规模预训练模型统一图像和点云的模态，从而在相同模态内进行稳健的对应关系估计，实现跨模态匹配。 为了将点云转换为图像模态，一种简单的方法是将点投影到图像平面上以获取深度图，然后通过深度到图像的扩散模型 ControlNet（Zhang &amp; Agrawala, 2023）将深度图转换为图像。然而，如图 22(I) 所示，深度图可能对应多个可能的图像，使得从点云生成的图像与输入图像的外观完全不同，这会导致即使使用最先进的图像匹配方法（Sarlin et al, 2020; Delone et al, 2018; Sun et al, 2021），也会出现匹配结果不准确的问题。为了解决这个问题，我们提议匹配生成图像与输入图像之间的语义特征，因为生成图像尽管外观不同，但在语义上与输入图像高度一致。受最近基于扩散的语义对应关系估计方法（Tang et al., 2023; Zhang et al., 2023）的启发，我们利用深度到图像 ControlNet 中的中间特征图，在深度图和图像之间进行匹配。如图 [2 (II) 所示，我们可视化了深度图和 RGB 图像的扩散特征。然后，我们使用具有互检的最近邻（NN）匹配器（Wang et al., 2022a）在它们之间建立对应关系。我们发现这些语义特征表现出强一致性，即使它们是分别在深度图和图像上提取的，这使得建立稳健的跨模态对应关系成为可能。然而，语义特征涉及图像的较大区域，这种大感受野导致了粗粒度特征和特征匹配中的稀疏对应关系。 我们进一步利用单目深度估计器（Bhat et al., 2023）提高了跨模态对应关系的准确性。单目深度估计器的最新进展使得对单视角图像进行度量深度估计成为可能。然而，直接匹配点云与从输入图像中估计的深度图之间的特征，如图2 (III) 所示，导致了较差的性能。主要原因在于预测的深度图虽然看起来合理，但与输入点云相比仍存在较大扭曲，这些扭曲阻碍了稳健对应关系的估计。尽管全球性扭曲导致了噪声匹配，但估计深度图的局部几何信息仍提供了准确定位关键点和密集估计细粒度对应关系的有用信息。因此，我们将从估计深度图中提取的局部几何特征（Choy et al., 2019）与从扩散模型中提取的语义特征结合起来，作为跨模态特征，从而实现图像和点云之间的密集且准确的对应关系估计，如图 2](IV) 所示。 综上所述，FreeReg 具有以下特点： FreeReg 结合了来自扩散模型的粗粒度语义特征和来自深度图的细粒度几何特征，实现了精确的跨模态特征匹配。 FreeReg 无需在 I2P 任务上进行训练，这避免了点云与图像局部特征对齐过程中不稳定且声名狼藉的度量学习问题。 FreeReg 显著优于现有的全监督跨模态配准基线（例如 Pham et al., 2020; Li et al., 2023）。具体而言，在室内 3DMatch 和 ScanNet 数据集以及室外 KITTI-DC 数据集上，FreeReg 在 Inlier Ratio 上实现了超过 20% 的改进，Inlier Number 提升了 3.0 倍，Registration Recall 提高了 48.6%。 相关工作图像到点云配准：为了建立图像和点云之间的对应关系以恢复姿态，大多数现有方法（Li et al., 2015; Xing et al., 2018; Feng et al., 2019; Lai et al., 2021; Wang et al., 2021; Pham et al., 2020; Liu et al., 2020; Jiang &amp; Saripalli, 2022; Li et al., 2023）依赖于度量学习来对齐图像和点云的局部特征（Feng et al., 2019; Pham et al., 2020; Lai et al., 2021; Jiang &amp; Saripalli, 2022），或深度图（Liu et al., 2020; Wang et al., 2021; Li et al., 2023）。然而，这些方法通常需要跨模态配准训练数据（Pham et al., 2020; Wang et al., 2021; Jiang &amp; Saripalli, 2022; Li et al., 2023; Kim et al., 2023），并且由于跨模态度量学习的困难，显示出有限的泛化能力（Pham et al., 2020; Wang et al., 2021; Li et al., 2023; Ren et al., 2022; Yao et al., 2023）。相比之下，FreeReg 不需要在 I2P 任务上进行任何训练和微调，并且在室内和室外场景中表现出强的泛化能力。 其他一些方法将图像到点云配准直接作为优化问题进行解决（David et al., 2004; Campbell et al., 2019; Arar et al., 2020; Wang et al., 2023a; Zhou et al., 2023），通过逐步对齐关键点（Li &amp; Lee, 2021; Ren et al., 2022; Campbell et al., 2019）、杆状结构（Wang et al., 2022b）、语义边界（Liao et al., 2023）或 RGB 图像和深度图的代价体积（Wang et al., 2023a）来回归姿态。然而，这些方法在优化过程中严重依赖准确的初始姿态（Wang et al., 2021; Liao et al., 2023）以避免局部最小值。FreeReg 不需要如此严格的初始化，因为 FreeReg 通过特征匹配来建立对应关系，从而处理大的姿态变化。 扩散特征提取：最近，一类被称为扩散模型的研究（Ho et al., 2020; Song et al., 2020a; b; Karras et al., 2022; Song &amp; Ermon, 2019; Dhariwal &amp; Nichol, 2021; Liu et al., 2023）展示了令人印象深刻的生成能力。在此基础上，随着无分类器引导（Ho &amp; Salimans, 2022）和数十亿文本到图像的训练数据（Schuhmann et al., 2022）的出现，特别是稳定扩散（Rombach et al., 2022）展现了卓越的文本到图像生成能力。基于此，现有方法展示了稳定扩散内部表示（Diffusion Feature）（Kwon et al., 2022; Tumanyan et al., 2023）在各种领域如分割（Amit et al., 2021; Baranchuk et al., 2021; Chen et al., 2022b; Jiang et al., 2018; Tan et al., 2022; Wolleb et al., 2022）、检测（Chen et al., 2022a）、深度估计（Duan et al., 2023; Saxena et al., 2023b; a）的卓越表现。这些方法仅在 RGB 图像上利用稳定扩散提取扩散特征。我们的方法基于最近微调的扩散模型在 RGB 图像和深度图上提取扩散特征。 图 3：FreeReg 流程。给定一个点云（PC）和一个部分重叠的 RGB 图像，FreeReg 提取点云和图像的扩散特征和几何特征。这两个特征被融合并匹配，以建立像素到点的对应关系，然后计算图像与点云之间的 $\\mathrm{SE}(3)$ 相对姿态。 模型 ControlNet（Zhang &amp; Agrawala, 2023）或 T2IAdaptor（Mou et al., 2023）高效利用深度、语义图和草图来引导图像生成中的稳定扩散。 扩散特征匹配。一些近期的研究利用扩散特征进行表示学习（Kwon et al., 2022）和语义匹配（Luo et al., 2023a; Tang et al., 2023; Hedlin et al., 2023; Zhang et al., 2023），在 RGB 图像中捕捉对象的不同实例和类别。相比之下，我们的方法展示了扩散特征在学习图像到点云配准的跨模态特征中的有效性。 单目深度估计器。单目深度估计固有地存在尺度模糊问题（Chen et al., 2016, 2020; Xian et al., 2018, 2020）。随着单目深度训练数据的不断增加（Guizilini et al., 2023; Antequera et al., 2020; Wilson et al., 2023），最近的研究（Bhat et al., 2021; 2022; Jun et al., 2022; Li et al., 2022; Yang et al., 2021; Yin et al., 2021; 2019; Yuan et al., 2022; Guizilini et al., 2023; Yin et al., 2023）学习场景先验，以在真实度量空间中回归深度值，并展示了令人印象深刻的结果。我们使用一种最先进的度量深度估计器 Zoe-Depth（Bhat et al., 2023）来恢复与 RGB 图像相对应的点云。 方法设 $I \\in \\mathbb{R}^{H \\times W \\times 3}$ 为 RGB 图像，$P \\in \\mathbb{R}^{N \\times 3}$ 为点云。我们首先将 $P$ 投影到一个深度图 $D \\in \\mathbb{R}^{H^{\\prime} \\times W^{\\prime}}$ 上，该投影基于从深度或 LiDAR 传感器的中心和方向计算的相机姿态。关于此投影的更多细节见补充材料。FreeReg 的目标是匹配在 $I$ 和 $D$ 上提取的跨模态特征，以建立对应关系并解决它们之间的相对姿态。FreeReg 的流程如图 3 所示。具体来说，我们提取扩散特征（第 3.2 节）和几何特征（第 3.3 节）用于特征匹配，然后根据匹配结果估计 I2P 转换。我们首先简要回顾扩散方法，这些方法用于提取跨模态特征。 初步介绍：稳定扩散与 ControlNet所提议的跨模态特征基于 ControlNet（Zhang &amp; Agrawala, 2023）（CN），因此我们在本节中简要回顾 ControlNet 的相关细节。扩散模型包含前向过程和反向过程，两者都是马尔可夫链。前向过程在多个步骤中逐渐向输入图像添加噪声，最终得到纯粹的无结构噪声。相应的反向过程逐步去噪，以逐渐恢复结构并生成图像。稳定扩散（Rombach et al., 2022）（SD）是一种广泛使用的扩散模型，主要包括一个 UNet，它以有噪声的 RGB 图像作为输入并预测噪声。原始的扩散模型仅支持文本到图像生成。近期的 ControlNet（Zhang &amp; Agrawala, 2023），如图 4（b）所示，添加了一个额外的编码器来处理深度图，并利用提取的深度特征引导 SD 的反向过程，使 SD 能够从纯高斯噪声中生成与输入深度图一致的图像。在 FreeReg 中，我们利用 CN 和 SD 提取跨模态特征用于特征匹配。 图 4：扩散特征提取示意图：（a）图像上的扩散特征，（b）深度图上的扩散特征，（c）扩散特征的可视化。 跨模态数据上的扩散特征直接从输入深度图生成图像会出现外观不一致的问题，这会导致特征匹配不准确。我们不生成显式图像，而是利用稳定扩散模型的中间特征图进行跨模态特征匹配。概述如图 4 所示。 RGB 扩散特征。如图 4（a）所示，我们执行稳定扩散（SD）的前向过程（Rombach et al., 2022），在预定义的步骤 $t$ 上向输入 RGB 图像添加噪声。噪声图像被输入到 SD 的 UNet 中，UNet 解码器的中间特征图被用作输入 RGB 图像的扩散特征。 深度扩散特征。对于深度图，我们首先使用传统的腐蚀和膨胀操作（Ku et al., 2018）来使其更加稠密。如图 4（b）所示，我们将深度图作为条件输入到 ControlNet（Zhang &amp; Agrawala, 2023）中，以引导 SD 的反向过程。在这种条件下，SD 逐步去噪直至达到预定义的步骤 $\\hat{t}$，然后我们使用 SD UNet 解码器中的特征图作为深度扩散特征。另一种方法是直接将深度图作为 RGB 图像进行扩散特征提取，但这种方法效果较差，如补充材料中所示。 层选择。剩下的问题是选择用于特征提取的层。图 4（c）中展示了 RGB 图像和深度图上的扩散特征可视化。可以观察到，前几层的上采样特征（层索引 $l \\leq 6$）在 RGB 和深度数据之间表现出强一致性。而后几层（索引大于 6）的特征则显示出更多细粒度的细节，如纹理，但一致性较差。因此，我们选择早期层 $0, 4, 6$ 的特征作为扩散特征。为了减少每层的特征维度，我们应用主成分分析（PCA）将特征维度降至 128。RGB 图像 $I$ 和深度图 $D$ 的扩散特征分别为 $F_d^I$ 和 $F_d^D$，这两个特征通过将不同层的特征拼接后进行 $L_2$ 归一化得到。 跨模态数据上的几何特征上述扩散特征是从图像的大区域中提取的，这使得它难以捕捉细粒度的局部细节，并且只估计稀疏的对应关系，如图 5（b/e）所示。为了提高这些对应关系的准确性，我们引入了所谓的几何特征，利用单目深度估计器 Zoe-Depth（Bhat et al., 2023）。 具体而言，我们利用 Zoe-Depth 为输入 RGB 图像 $I$ 生成每像素深度 $D^Z$，并从生成的深度图中恢复点云。然后，我们使用预训练的点云特征提取器 FCGF（Choy et al., 2019）提取每个点的特征，这些特征作为图像 $I$ 中对应像素的几何特征。我们以相同的方式为深度图 $D$ 的像素构建几何特征。如图（c/f）所示，仅匹配几何特征会因单视图深度图中的大幅扭曲而产生许多离群对应。 图 5：特征及估计对应关系的可视化。（a）输入图像和点云。（b）、（c）和（d）分别展示了扩散特征、几何特征和融合特征图的可视化。（e）、（f）和（g）分别展示了通过最近邻（NN）匹配器使用扩散特征、几何特征和融合特征估计的像素到点的对应关系。扩散特征估计出可靠但稀疏的对应关系。几何特征则提供了密集的匹配，但伴随更多离群点。融合特征在准确性和保留细粒度细节之间取得平衡，从而实现了准确且密集的匹配。 融合特征用于 I2P Registration融合特征。在本节中，我们提出融合两种特征以实现准确的对应关系估计，如图所示。我们在深度图和图像上均匀采样一组密集的关键点。然后，在这些关键点上提取上述扩散特征和几何特征。融合前，两个特征都通过其 $L_2$ 范数进行归一化。具体而言，我们按照（Zhang et al., 2023）的方式，在 $I$ 或 $D$ 上的每个关键点融合这两种特征，计算方式为 $$F = \\left[w F_d, (1-w) F_g\\right]$$ 其中，$w$ 为融合权重，$[\\cdot, \\cdot]$ 表示在特征维度上的拼接，$F$ 是得到的 FreeReg 特征。 像素到点的对应关系。给定 RGB 图像 $I$ 上的两组融合特征 $F^I$ 和深度图 $D$ 上的 $F^D$，我们进行最近邻（NN）匹配，并使用互相最近检查（Wang et al., 2022a）来找到一组可能的对应关系。需要注意的是，深度图 $D$ 中的每个像素在匹配中对应于点云 $P$ 中的一个 3D 点。 图像到点云Registration。为了求解 RGB 图像 $I$ 相对于 $P$ 的 $\\mathrm{SE}(3)$ 姿态。通常的方法是对建立的像素到点的对应关系执行透视-n-点（PnP）算法（Lepetit et al., 2009）。然而，我们已经使用 Zoe-Depth（Bhat et al., 2023）估计了与 RGB 相对应的深度图。因此，我们可以将像素到点的对应关系转换为 3D 点到点的对应关系，并使用 Kabsch 算法（Kabsch, 1976）估计 $\\mathrm{SE}(3)$ 相对姿态。在补充材料中，我们实证表明，使用 PnP 算法可以获得更准确的姿态估计，但在许多情况下失败，而 Kabsch 算法适用于更多情况，但估计的变换误差较大。 实验实验协议数据集。我们在三个广泛使用的数据集上评估了提出的方法：（1）3DMatch（Zeng et al., 2017）测试集包括来自 8 个室内场景的 RGB 图像和点云（称为 $I2P$ 对）。这里使用的点云由 Asus Xtion 深度传感器采集。我们手动排除了重叠很小的 I2P 对，结果得到 1210 对 I2P 对，重叠率超过 $30%$。（2）ScanNet（Dai et al., 2017）测试集包含来自 31 个室内场景的 4,660 对 I2P 对，重叠率超过 $30%$。为了进一步增加难度，我们使用 3 cm 的体素大小对输入点云进行了下采样，从而得到高度稀疏的点云。（3）Kitti-DC（Uhrig et al., 2017）测试集包含来自 4 个精选户外场景的 342 对 I2P 对。稀疏点云来自 64 线 LiDAR 扫描。每对 I2P 对之间的距离小于 10 米。 表 1：不同方法的跨模态配准性能。“InvCP.” 代表逆摄像机投影（Inverse Camera Projection）。 指标。根据（Choy et al., 2019；Wang et al., 2023c;b），我们采用四种评估指标：（1）特征匹配召回率（FMR）是具有超过 $5%$ 正确估计对应关系的 I2P 对的比例。如果对应关系的真实 3D 距离小于 $\\tau_c$，则认为其匹配正确。对于 3DMatch/ScanNet，$\\tau_c$ 设置为 0.3 米；对于 Kitti-DC，$\\tau_c$ 设置为 3 米。（2）内点比例（IR）是所有 I2P 对中的正确对应关系比例的平均值。（3）内点数量（IN）是每对 I2P 对中正确对应关系的平均数量。（4）配准召回率（RR）是旋转和位移误差小于 $\\tau_R$ 和 $\\tau_t$ 的正确对齐 I2P 对的百分比。对于 3DMatch/ScanNet，($\\tau_R, \\tau_t$) 设置为 ($20^\\circ, 0.5 \\text{m}$)；对于 Kitti-DC，设置为 ($10^\\circ, 3 \\text{m}$)。我们在补充材料中提供了不同阈值条件下的额外结果。 基线。我们将 FreeReg 与完全监督的配准基线进行比较。图像配准方法 SuperGlue (SG)（Sarlin et al., 2020）被修改以匹配 RGB 图像和点云。LCD（Pham et al., 2020）通过度量学习来构建 I2P 跨模态描述符。DeepI2P（Li &amp; Lee, 2021）通过优化准确的初始姿态来解决 I2P Registration。我们实现了一种跨模态特征提取方法 I2P-Matr，参考了并行工作的 2D3D-Matr（Li et al., 2023），但官方代码尚未发布。同时，我们在补充材料中按照实验协议（Li et al., 2023）比较了 FreeReg 与 P2-Net（Wang et al., 2021）和 2D3D-Matr（Li et al., 2023），其中 FreeReg 也获得了最佳配准性能。我们还采用了图 2 中提到的基线方法，该方法首先使用 ControlNet（Zhang &amp; Agrawala, 2023）从目标点云生成 RGB 图像，然后使用 SuperGlue（Sarlin et al., 2020）来匹配输入图像和生成的图像（CN+SG）。对于我们的方法，我们报告了仅使用扩散特征（FreeReg-D，即 $w=1$）、仅使用几何特征（FreeReg-G，即 $w=0$）以及融合特征（FreeReg，即默认 $w=0.5$）的匹配结果。更多实现细节和分析请参见补充材料。 三个基准上的结果表 1 展示了 FreeReg 和基线方法在三个跨模态配准基准上的定量结果。一些定量结果如图 6 所示。 对应关系质量由 FMR、IR 和 IN 反映。对于 LCD 和 I2P-Matr，直接对齐跨模态特征的方法表现较差。$\\mathrm{CN}+\\mathrm{SG}$ 由于生成图像与输入图像之间的外观差异，未能建立可靠的对应关系。对于 FreeReg，仅使用扩散特征（FreeReg-D）或几何特征（FreeReg-G）已能获得优于基线的方法的结果。利用这两种特征，FreeReg 实现了最佳的对应关系质量，并在 FMR 上超越基线 $54.0%$，IR 超越 $20.6%$，IN 高出 $3.0 \\times$。需要注意的是，与基线方法不同，FreeReg 甚至没有对 I2P 任务进行训练。 配准质量由 RR 指示。得益于高质量的对应关系，FreeReg 在 RR 上显著超越了基线方法，FreeReg-D/G 分别超越 $48.6%$ 和 $22.9% / 16.4%$ 的 RR。此外，FreeReg 使用 Kabsch 算法在室内 3DMatch/ScanNet 上显著超过 图 6：对应关系可视化。(a) 输入 RGB 图像和用于配准的点云。(b) 来自 I2P-Matr 的估计对应关系。(c/d/e) 分别来自 FreeReg-D、FreeReg-G 和 FreeReg 的估计对应关系。 平均误差 3.4 米。在补充材料中，我们进一步提供了更多分析，发现 PnP 方法在准确性上表现更佳，而 Kabsch 在更多情况下提供了合理的结果。 更多分析我们对 FreeReg 进行了全面的消融实验。补充材料中提供了有关如何微调 FreeReg 以提升性能、FreeReg 的运行时间分析及加速策略、不同阈值下的 FreeReg 性能、更多定性比较和消融研究的详细信息。 消融扩散特征提取在本节中，我们在未包含在测试集中的验证场景 “bundlefusion-office 0”（BFO）上进行评估，以调整扩散特征层选择和扩散步 $\\hat{t}$ 选择的超参数。随后，我们报告了这些参数在 3DMatch 数据集上的性能。 扩散层选择。在表 $Z(a-i)$ 中，我们报告了 Stable Diffusion 的 UNet 中 8 层输出特征图的大小。特征图大小分为三个级别：小组（$8 \\times 11$，层 0 - 2），中组（$16 \\times 22$，层 3 - 5），和大组（$32 \\times 44$，层 6 - 8）。我们从每个级别中选择了在 BFO 上具有最佳配准性能和合理匹配质量的层，具体为层 0、4 和 6，用于构建我们的扩散特征。然后，在表 $22(\\mathrm{j}-\\mathrm{m})$ 中，我们对构建扩散特征的层选择进行了消融实验。结果表明，连接层 0、4、6 的特征显著提高了对应关系质量和配准性能。3DMatch 上的结果进一步验证了我们选择的有效性。有关扩散层选择的更多消融研究见补充材料。 扩散步选择。在表 3 中，我们旨在确定扩散步 $\\hat{t}$。实验结果表明，$\\hat{t}=150$ 的扩散特征在 BFO 上实现了最佳的配准性能。3DMatch 上的结果确认了这一选择的有效性。 消融特征融合权重表 2：扩散特征提取中的层选择。”Feature map” 指特征图的尺寸，格式为 “channel $\\times$ width $\\times$ length”。 表 3：确定扩散特征提取中的 $\\hat{t}$。 我们在表 4 中对融合权重 $w$ 进行了消融实验，以融合扩散特征和几何特征，基于基准模型 FreeReg。结果显示，当 $w$ 设置为 0.5 时，FreeReg 实现了最佳的配准性能。此外，我们发现依赖更多扩散特征（即 $w=0.6$）的结果与默认的 FreeReg 相似。相比之下，依赖更多几何特征（即 $w=0.4$）会导致性能显著下降，IR 下降了 $8.7%$，RR 下降了 $2.5%$。这证明了所提出的扩散特征的鲁棒性。 表 4：确定融合扩散特征和几何特征的权重。 局限性主要限制在于 FreeReg 在 4090 GPU 上匹配单个 I2P 对时需要约 9.3 秒和 12.7G 的 GPU 内存，这导致比基线方法（如 LCD（0.6秒，3.5G）、I2P-Matr（1.7秒，2.7G）和 CN+SG（6.4秒，11.6G））使用更多时间但具有更高的 RR（Inlier Ratio）。原因在于，我们需要运行多个 ControlNet 的反向处理步骤，以去噪纯噪声，达到特定的步骤 $\\hat{t}$ 进行特征提取。在补充材料中，我们展示了如何通过约 50% 的加速仅带来约 1.4% 的 RR 降低。同时，尽管我们展示了使用扩散特征进行 I2P Registration的优越性能，但我们在扩散特征提取中手动选择层和去噪步骤，未来工作可以改进为自动选择优质特征。 结论我们提出了一种 I2P Registration框架，称为 FreeReg。FreeReg 的关键思想是利用扩散模型和单目深度估计器进行跨模态特征提取。具体而言，我们利用扩散模型的中间表示来构建多模态扩散特征，这些特征在 RGB 图像和深度图之间显示出强的一致性。我们进一步引入所谓的几何特征，以捕捉 RGB 图像和深度图上的独特局部几何细节。大量实验表明，FreeReg 在 I2P 任务中表现出强大的泛化能力和鲁棒性。无需在 I2P Registration任务上进行任何训练或微调，FreeReg 在三个公共的室内和室外基准测试中分别实现了 20.6% 的 Inlier Ratio 改进、3.0 倍的 Inlier Number 提升和 48.6% 的 Registration Recall 改进。","link":"/2024/09/04/FreeReg/"},{"title":"大牛谏言 Instructions for PhD Students","text":"摘要: Dimitris Papadias是港科大的教授，陶宇飞的导师。他写过一个slides，叫作《Instructions for PhD Students》。虽然是这位大牛是做数据库方向的，但是他的谏言对于每一个博士生都非常有用。置顶，每天欣赏一遍。 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 Instructions for PhD Students Goals of PhD Students in Databases Motivation How to Find and Keep Motivation How to Present Ideas (1) – for oral presentations and papers How to Present Ideas (2) How to Present Ideas (3) How to write papers PhD Thesis Topic About Laziness About “Stupidity” About Democratic Procedures Uncountable Qualifications of a PhD Student Countable Qualifications of a PhD Student Concrete Advice Conclusion Instructions for PhD StudentsDimitris Papadias, HKUST A presentation to my PhD students Goals of PhD Students in Databases THE CAKE SIGMOD, VLDB, (+PODS) THE ICING TODS, TOIS, VLDBJ, TKDE, ICDE (because sometimes the cake is not enough) Submission is most important I will determine if the paper is good enough for submission. Acceptance is often random – although it increases significantly your chances to find a job, it does not necessarily make you a better person (or scientist). Rejection is part of the game – do not start questioning the meaning of life. If you are lucky you will get rejected again in the future. Acceptance builds CVs Rejection builds men Motivation You are mainly responsible for it I already have &gt;100 papers. +1 does not make a big difference in my CV – it makes a huge difference in yours. I am a full professor – nothing makes a difference in my CV anymore. You have to come after me – not the other way around I am tired of trying to motivate students. You have to compete with the other students for my attention. I can always find one to work with anyway. How to Find and Keep Motivation Motivation is the most important qualification for a PhD student Read the recent SIGMOD, VLDB, (+PODS) Proceedings You can ask to present material that you find interesting Initiate discussions Get involved in other (especially older) students’ work Always try to write something – some of the best ideas will come to you by writing Writing means clarifying your mind a draft is something concrete – otherwise you may have done nothing as far as I am concerned. How to Present Ideas (1) – for oral presentations and papers Only present topics that you understand CLEARLY The most important problem is that sometimes people “&nbsp;do not understand that they do not understand&nbsp;”. If you do not understand a point, do not be afraid to admit it. Understanding that you do not understand, is the most important step towards understanding. Once you understand something it becomes simple in which case you should be able to present it clearly if it still seems complicated, probably you still do not understand it well enough How to Present Ideas (2) Describe the problem in detail Most students fail at this point and the rest of their presentation is useless Present the related work Give the abstract idea of your solution Explain why it is better than previous work Only if steps 1-4 are clear go into the details Most people are not as knowledgeable as you think they are. Do not miss the point among the specifics of the solution. A presentation is like a class – if the audience does not understand it is your fault, not theirs. Present only things that you understand clearly – if something still looks complex, skip it. How to Present Ideas (3) Be very careful about the notation a good notation can make your life easy. a bad notation is inexcusable. Given that most of you make a lot of English errors (excusable), you should make sure that at least your notation is carefully chosen. Spend a lot of time on good examples Read a lot of our previous papers and try to learn the writing style it has been very successful How to write papers All the above about presentations, plus LOVE your papers Papers are forever. LOVE shows in a paper. Even if a paper gets rejected, eventually it always makes it somewhere. Presentation is as important (if not more) than the actual work. Presentation is easier to improve. Read and refine your draft as many times as possible. Then repeat the same process again (and again). PhD Thesis Topic The topic is not important for the quality of a thesis a really good student will produce results in any topic. but a hot topic may help you find a good job later. a topic in the supervisor’s area is likely to increase your chances (in case you get stuck on your own). You do not necessarily need a concrete topic for your PhD thesis Some of the best theses I have seen are collections of papers on a general topic. If you are good enough assembling a thesis should take a few days of copy/paste work. Write the papers first and decide the topic later The most difficult paper is the first one - the remaining ones usually come easy. Normally, the problem and not the solution passes the paper. Try to think of innovative problems. About Laziness It is very easy for PhD students to get lazy they do not have office hours. the supervisors are sometimes lazy. It is very easy for Professors to get lazy they do not have office hours. often they have tenure. The less you work, the less you want to work which creates a vicious circle About “Stupidity” Have not found yet an accurate definition: Lack of common sense Repeating the same mistake many times The really stupid person does not know that he is stupid if he knew he would not be Intelligence (and competence) has limits. Stupidity has none When I think that I have seen everything, there is always something new to surprise me. I will be the judge of your stupidity. About Democratic Procedures The supervisor is always right (at least as far as you are concerned). We follow democratic procedures You are encouraged to give your opinion. I will decide. &nbsp;Your opinion is not very important before your first SIGMOD paper.&nbsp; Uncountable Qualifications of a PhD Student Motivation Programming Skills Background (e.g., papers read, mathematical background) Writing and Presentation Skills Creativity (ability to come up with new and interesting problems) Durability (“do not crack under pressure”) Countable Qualifications of a PhD Student You are as good as the number of CAKE papers (SIGMOD, VLDB, PODS) that you have co-authored. the number of ICING papers (TODS, TOIS, VLDBJ, TKDE, ICDE, INFORMATION SYSTEMS) that you have co-authored. most important are the papers where you are the first author. You get bonus points if I can count on you for good reviews. class tutorials, conference presentations. Concrete Advice Read papers Bother me with questions, ideas etc. Ask to give presentations Write drafts of your ideas, or summaries of the papers that you have read and seem important Never miss a deadline e.g., if you have a task that requires 1 week, ask for 2 to be on the safe side. Pay attention to the detail Conclusion During your studies you will have on the average 3 SIGMOD deadlines. Missing one is important. I will be here to help and guide you. Do not crack under pressure.","link":"/2024/06/06/Instructions-for-PhD-Students/"},{"title":"智源大会2024 LLM Agent - Current and Future","text":"摘要: 2024智源大会 Stefano V. Albrecht 对目前LLM Agent研究的吐槽 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 [toc] 原视频资料〓 ReTURN 〓 2024智源大会 Stefano V. Albrecht 对目前LLM Agent研究的吐槽 一些想法〓 ReTURN 〓","link":"/2024/08/13/LLMAgent-current-and-future/"},{"title":"MacOS安全弹出移动硬盘的方法","text":"摘要: MacOS外接大容量移动硬盘的时候， 弹出设备时遇到问题无法成功， 那么该怎么安全unmount该硬盘呢？ .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 首先，查看目前Mac上所有的磁盘 1df -lh 查看之后，就会得到当前Mac上所有磁盘的列表： 123456789Filesystem Size Used Avail Capacity iused ifree %iused Mounted on/dev/disk3s1s1 460Gi 9.4Gi 61Gi 14% 394k 636M 0% //dev/disk3s6 460Gi 20Ki 61Gi 1% 0 636M 0% /System/Volumes/VM/dev/disk3s2 460Gi 5.5Gi 61Gi 9% 1.1k 636M 0% /System/Volumes/Preboot/dev/disk3s4 460Gi 40Mi 61Gi 1% 45 636M 0% /System/Volumes/Update/dev/disk1s2 500Mi 6.0Mi 483Mi 2% 1 4.9M 0% /System/Volumes/xarts/dev/disk1s1 500Mi 6.1Mi 483Mi 2% 29 4.9M 0% /System/Volumes/iSCPreboot/dev/disk1s3 500Mi 536Ki 483Mi 1% 56 4.9M 0% /System/Volumes/Hardware/dev/disk3s5 460Gi 384Gi 61Gi 87% 1.6M 636M 0% /System/Volumes/Data 一般情况下，这里会出现自己的移动磁盘的名字，在最后一个（我这里没有插移动硬盘，因此图中都是Mac自带的磁盘名）。另外可以看到，每个磁盘前面都对应一个代号（如/dev/disk3s1s1）。随后执行如下代码，将移动磁盘推出： 1diskutil unmount 移动硬盘对应的Filesystem名称(带上/dev) 注意：1）unmount与/dev之间有一个空格；2）A与B对应的编号是df -lh命令中移动磁盘对应的编号，比如df -lh命令执行后，移动磁盘对应的编号是disk2s2，则此处的diskAsB就写disk2s2。 由于磁盘被某个进程占用，无法推出，因此终端会提示：[磁盘代号] failed to unmount: dissented by PID XXXX。此时再打开活动监视器这个app，在CPU菜单下，在PID一栏中找到对应的PID XXXX，点击左上角叉号，选择强制退出即可。","link":"/2024/06/02/MacOS%E5%AE%89%E5%85%A8%E5%BC%B9%E5%87%BA%E7%A7%BB%E5%8A%A8%E7%A1%AC%E7%9B%98%E7%9A%84%E6%96%B9%E6%B3%95/"},{"title":"When LLMs step into the 3D World A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models","text":"摘要: 本文重点介绍了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和广泛的世界知识，强调它们在提升空间理解和交互方面的巨大潜力。我们的研究涵盖了各种3D数据表示，从点云到神经辐射场（NeRFs），并探讨了它们与LLMs在3D场景理解、描述、问答和对话等任务中的整合，以及基于LLM的空间推理、规划和导航代理。 本文还简要回顾了其他整合3D和语言的方法。本文的荟萃分析显示了显著的进展，但也强调了需要新的方法来充分利用3D-LLMs的潜力。因此，通过本文，我们旨在为未来的研究指明方向，探索和扩展3D-LLMs在理解和互动复杂3D世界中的能力。 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 摘要 介绍Introduction 背景 数据的3D表示 大型语言模型（LLM） 关于LLM架构 分词（Tokenization） 大型语言模型的涌现能力 大型语言模型的微调 关于2D视觉语言模型 视觉基础模型（VFMs） 任务和指标介绍(TASKS AND METRICS) 任务 3D标注（3D to 文本） 任务 3D定位（3D加文本 to 3D位置） 任务 3D对话（3D 加 文本 to 文本） 任务 3D具身代理（3D 加 文本 to 行动） 任务 文本到3D生成（Text to 3D） 关于3D TASKS WITH LLMS 关于LLMs如何处理3D场景信息？ 使用LLMs提升3D任务性能 知识增强方法 推理增强方法 使用LLMs进行3D多任务学习 多任务学习的数据 多任务3D模型的训练 LLMs作为3D多模态接口 LLMs用于具身智能体 3D任务规划 3D导航 3D物体操作 LLMs用于3D生成 对象级生成 场景级生成 程序生成与操控 视觉语言模型与三维任务系列(3D TASKS WITH VLMS) 利用VLMs进行3D任务 开放词汇3D场景理解 文本驱动的3D生成 三维视觉与语言的端到端架构 数据集 Cap3D Text2Shape SceneVerse nu-Caption nu-Grounding ScanRefer ReferIt3D Multi3DRefer Chat-3D v2 EmbodiedScan ScanEnts3D WildRefer RIORefer ARKitSceneRefer ScanERU DenseGrounding ScanQA ScanQA (另一个) 3DMV-VQA NuScenes-QA CLEVR3D SQA-3D 3D-LLM ScanScribe M3DBench GPT4Point LAMM 机会与挑战(CHALLENGES AND OPPORTUNITIES) 结论 摘要〓 ReTURN 〓 随着大型语言模型（Large Language Models, LLMs）的发展，它们与3D空间数据的整合（3D-LLMs）取得了快速进展，为理解和互动物理空间提供了前所未有的能力。本文综述了使LLMs能够处理、理解和生成3D数据的方法。 我们重点介绍了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和广泛的世界知识，强调它们在提升空间理解和交互方面的巨大潜力。我们的研究涵盖了各种3D数据表示，从点云到神经辐射场（NeRFs），并探讨了它们与LLMs在3D场景理解、描述、问答和对话等任务中的整合，以及基于LLM的空间推理、规划和导航代理。 本文还简要回顾了其他整合3D和语言的方法。本文的荟萃分析显示了显著的进展，但也强调了需要新的方法来充分利用3D-LLMs的潜力。因此，通过本文，我们旨在为未来的研究指明方向，探索和扩展3D-LLMs在理解和互动复杂3D世界中的能力。 为支持本次综述，我们建立了一个项目页面，组织和列出了与我们主题相关的论文：https://github.com/ActiveVisionLab/Awesome-LLM-3D。 介绍〓 ReTURN 〓 大型语言模型（Large Language Models, LLMs）的出现标志着自然语言处理领域的一个变革时代，使机器能够以前所未有的方式理解、生成和互动人类语言。然而，物理世界本质上是三维的，理解空间3D环境对于涉及感知、导航和互动的许多现实应用至关重要。随着最近的进展，LLMs的应用已经远远超出了文本的范畴。 将LLMs与3D数据融合，提供了一个独特的机会，可以增强计算模型对物理世界的理解和互动，从而在多个领域引领创新，包括自主系统、增强现实、机器人导航和机器人操作。近期的研究表明，整合LLMs与3D数据可以通过利用LLMs的固有优势，如零样本学习、先进推理和广泛知识，在复杂的3D环境中进行解释、推理或规划。 然而，LLMs与3D数据的整合并非易事。3D数据表示、模型可扩展性和计算效率等问题仍是重大障碍。此外，确保模型能够在现实世界环境中运行，需要克服与数据多样性和环境复杂性相关的挑战。解决这些问题对于充分实现LLMs在3D应用中的潜力，创造动态和情境感知的人工智能系统至关重要。 本文综述了LLMs与3D数据的交叉领域，提供了当前方法、应用和挑战的详尽概述。首先，介绍了常见的3D表示形式、LLMs的简要介绍以及视觉语言模型（VLMs）和视觉基础模型（VFMs）的概述。在第三部分，详细描述了当前方法旨在解决的3D视觉语言任务，并概述了当前的评估指标和协议。 接下来，在第四部分中，我们分析了数据格式、处理技术和在增强3D理解方面显示出前景的模型架构，展示了LLMs与3D数据成功结合的多个领域，如：利用LLMs的世界知识和推理能力增强3D任务性能，将LLMs作为多模态接口和具身代理，或使用LLMs生成复杂场景。除了LLMs，还有一些研究提出了统一3D感知与语言能力的端到端架构。广泛的研究还探讨了通过现成的2D视觉语言模型（VLMs）知识进行开放词汇3D场景理解以及文本驱动的3D生成。 本文在第五部分全面概述了这些方法，展示了3D+语言领域的全貌。然后，我们在第六部分列出了用于训练和评估这些方法的数据集。最后，第七部分强调了该领域的挑战和未来可能的研究方向。 背景〓 ReTURN 〓 本部分提供了关于3D表示、Large Language Models（LLMs）、2D Vision-Language Models（VLMs）和Vision Foundation Models（VFMs）的基本背景知识。 数据的三维表示选择一种3D表示形式来描述、建模和理解我们的世界，是一个至关重要的课题，它有助于理解3D-LLMs的当前进展。这也是计算机视觉中的一个基础研究领域。由于深度学习、计算资源和3D数据可用性的进步，该领域最近得到了显著增长。我们简要描述了目前使用的最常见的3D表示形式。 点云 通过空间中的一组数据点表示3D形状，存储每个点在3D笛卡尔坐标系中的位置。除了存储位置外，还可以存储其他信息（例如颜色、法线）。基于点云的方法以其低存储占用而闻名，但缺乏表面拓扑信息。获取点云的典型来源包括LiDAR传感器、结构光扫描仪、飞行时间相机、立体视图和摄影测量等。 体素网格 由3D空间中的单元立方体组成，类似于2D中的像素表示。每个体素至少编码占用信息（以二进制或概率形式），但也可以编码到表面的距离，例如签名距离函数（SDF）或截断签名距离函数（TSDF）。然而，当需要高分辨率细节时，内存占用可能会变得过高。 多边形网格 由顶点和表面组成，以紧凑的方式描述复杂的3D形状。然而，其非结构化和不可微分的性质在将其与神经网络集成以实现端到端可微管道时提出了挑战。为解决这一问题，一些基于梯度近似的方法只能使用手工计算的梯度，其他解决方案如可微光栅化器可能导致渲染结果不精确，如内容模糊。 神经场 在3D研究社区中逐渐受到关注，与依赖几何原语的传统表示形式不同。神经场是一种从空间坐标到场景属性（如占用、颜色、辐射等）的映射，与体素网格中从离散单元到该体素值的映射不同，神经场的映射是一个学习函数，通常是多层感知机（MLP）。这种方式使神经场能够隐式地学习紧凑、连续和可微的3D形状和场景表示。 一组神经场方法专注于隐式表面表示。占用网络通过神经网络编码形状，使用3D点位置和来自点云、低分辨率体素或图像的特征来估计占用概率。与此同时，Deep SDF网络使用神经网络从3D坐标和潜在向量估计SDF。最近的方法如NeuS显著提高了静态和动态物体表面重建的保真度和效率。 另一组方法称为Neural Radiance Fields（NeRF），在3D世界的逼真渲染能力方面表现出色。这些方法使用位置编码技术并利用MLPs预测沿摄像机光线的辐射值（颜色和不透明度）。然而，为每个空间采样点（包括空旷空间）推断颜色和占用细节需要大量计算资源，因此，减少NeRF的计算开销以实现实时应用是一个强烈的需求。 混合表示 尝试将NeRF技术与传统的体积基础方法结合，促进高质量、实时渲染。例如，将体素网格或多分辨率哈希网格与神经网络结合，大大减少了NeRF的训练和推理时间。 3D高斯溅射 是点云的一种变体，每个点包含代表该点周围空间区域发出的辐射的额外信息，作为各向异性3D高斯“斑点”。这些3D高斯通常从SfM点云初始化，并使用可微渲染进行优化。3D高斯溅射通过利用高效的光栅化而非光线追踪，实现了比NeRF计算量小得多的状态-of-the-art新视图合成。 大语言模型传统的自然语言处理（NLP）涵盖了广泛的任务，旨在使系统能够理解、生成和操作文本。早期的NLP方法依赖于基于规则的系统、统计模型和早期的神经架构，如循环神经网络。最近引入的大型语言模型（LLMs），采用transformer架构并在庞大的文本语料库上进行训练，达到了前所未有的性能，并在该领域引发了新的热潮。鉴于本文聚焦于3D LLMs，我们在此提供LLMs的相关背景知识。关于LLMs的深入探讨，请参阅该领域的最新综述。 关于大语言模型架构在LLMs的背景下，“编码器-解码器”和“仅解码器”架构在NLP任务中被广泛使用。 编码器-解码器架构 由两个主要组件组成：编码器 $f_{\\text {enc }}$ 和解码器 $f_{\\text {dec }}$。编码器和解码器组件通常使用transformer实现，transformer利用注意力机制捕捉输入和输出序列中的长距离依赖关系。编码器接收输入序列 $X=\\left(x_1, x_2, \\ldots, x_N\\right)$，并将其映射为捕捉上下文信息的潜在表示序列 $H=\\left(h_1, h_2, \\ldots, h_N\\right)$。解码器则基于 $H$ 生成输出序列 $Y=\\left(y_1, y_2, \\ldots, y_T\\right)$。数学上，编码过程可以表示为 $H=f_{\\text {enc }}(X)$，整个潜在序列 $H$ 是从 $X$ 一次性生成的。而解码器则是按顺序生成输出序列 $Y$： $y_t=f_{\\text {dec }}\\left(y_{&lt;t}, H\\right)$，其中 $y_{&lt;t}=\\left(y_1, y_2, \\ldots, y_{t-1}\\right)$。 仅解码器架构 是transformer架构的一种变体，仅使用解码器组件。它特别适合语言建模任务，其目标是根据前面的标记预测下一个标记。仅解码器架构在数学上可以表示为 $y_t=f_{\\text {dec }}\\left(y_{&lt;t}\\right)$。 分词分词是一种预处理方法，将输入文本分解为一系列标记（tokens），这些标记是语言模型中的基本数据单位。标记的数量是有限的，每个标记可以对应一个单词、子词或单个字母。在推理过程中，输入文本被转换为标记序列并输入到模型中，模型预测输出标记，然后这些标记再转换回文本。分词对语言模型的性能有很大影响，因为它决定了模型如何感知文本。常用的分词技术包括词级分词、子词分词（如字节对编码、WordPiece、SentencePiece）和字符级分词。 大型语言模型的涌现能力大型语言模型（LLM）与传统非LLM方法的一个主要区别在于大型模型中出现的涌现能力，而这些能力在小型模型中不存在。“涌现能力”指的是随着LLM规模和复杂性的增加而产生的新型复杂能力。这些能力使LLM能够高级地理解和生成自然语言，跨领域解决各种问题而无需特定训练，并通过上下文学习适应新任务。以下介绍LLM的一些常见涌现能力。 上下文学习 是指LLM根据提示中的上下文理解和响应新任务或查询的能力，而无需显式再训练或微调。里程碑式的论文（如GPT-2/GPT-3）展示了几次示例学习的方式，在提示中提供一些任务示例，然后要求模型处理不同的示例而无需事先明确训练。最新的LLM如GPT-4展现了卓越的上下文学习能力，能够理解复杂指令，并基于提示中的上下文执行从简单翻译到生成代码和创意写作的广泛任务。 推理 在LLM的背景下，通常被称为“思维链提示”，涉及模型在处理复杂问题或问题时生成中间步骤或推理路径。这种方法允许LLM将任务分解为更小的可管理部分，促进更结构化和易于理解的解决过程。为了实现这一点，训练包括各种问题解决任务、逻辑谜题和模拟不确定性推理的数据集。当前最先进的LLM通常在模型参数超过60亿到1000亿时展示出高级推理能力。 指令遵循 是指模型理解和执行用户指定命令或指令的能力。这包括解析指令，理解其意图，并生成适当的响应或动作。将这种能力适应新任务的方法可能需要从包含各种指令及其正确响应或动作的数据集中进行指令调优。监督学习、人类反馈强化学习和交互学习等技术可以进一步增强性能。 大型语言模型的微调在3D-LLMs的背景下，LLMs要么直接使用预训练状态，要么经过微调以适应新的多模态任务。然而，微调LLM的全部参数会带来巨大的计算和内存挑战，因为涉及的参数数量非常庞大。因此，参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）逐渐流行起来，通过更新模型的一小部分参数而不是重新训练整个模型来适应特定任务。以下列出了LLMs中常用的四种PEFT方法。 低秩适配（Low-Rank Adaptation, LoRA）及其变体 通过低秩矩阵更新参数。在微调过程中，LoRA的前向传播可以表示为：$h=W_0 x + B A x$。其中，$W_0$是LLM的冻结权重，$B A$是由新引入的矩阵$A$和$B$参数化的低秩矩阵，这些矩阵在微调阶段更新。这种方法有几个显著优势。在微调期间，仅优化$B$和$A$，大大减少了与梯度计算和参数更新相关的计算开销。一旦微调完成并且权重合并，与原始模型相比，不会有额外的推理成本，如方程所示：$h=(W_0 + B A)x$。此外，不需要为不同任务保存多个LLM副本，可以保存多个LoRA实例，从而减少存储占用。 层冻结 在训练过程中冻结预训练模型的选定层，同时更新其他层。这通常适用于模型输入或输出更接近的层，具体取决于任务的性质和模型架构。在3DLLM方法中，例如，除输入和输出嵌入层外的所有层可能会被冻结，以减少任务特定数据集的过拟合风险，保留预训练的通用知识并减少需要优化的参数。 Prompt Tuning通过在提示中构建任务，引导大型语言模型（LLMs）执行特定任务，调整模型输入，而不是像传统微调那样调整模型参数。手动提示工程是最直观的方法，但即使对经验丰富的提示调优工程师来说，找到最佳提示也很困难。另一类方法是自动提示生成和优化。一种常见的方法是搜索精确的最优输入提示文本，称为硬提示。或者，可以使用优化方法来优化提示的嵌入（软提示）。 自适应微调通过添加或移除层或模块来定制模型架构，以适应特定任务。这可能包括整合新的数据模态，如视觉信息和文本数据。自适应微调的核心思想是在预训练模型的层之间插入小型神经网络模块。在自适应微调期间，只更新这些适配器模块的参数，而原始模型的权重保持不变。 关于二维视觉语言模型视觉-语言模型（Vision-Language Models, VLMs）是一类旨在捕捉和利用文本与图像/视频之间关系的模型，能够在这两种模态之间执行交互任务。大多数VLMs采用基于transformer的架构。&nbsp;通过利用注意力模块，视觉和文本内容相互条件化，实现互相作用&nbsp;。以下段落简要介绍了VLMs在判别任务和生成任务中的应用。 判别任务 涉及预测数据的某些特征。VLMs，如CLIP和ALIGN，在图像分类的零样本可转移性方面表现出色。这两个模型包括两个模块：视觉编码器和文本编码器。给定一张图像及其类别，CLIP和ALIGN通过最大化图像嵌入与句子“a photo of a {image category}”的文本嵌入之间的相似性进行训练。在推理过程中，通过将“{image category}”替换为可能的候选项，并搜索与图像最佳匹配的句子，实现零样本转移性。这两项工作激发了许多后续研究，进一步提升了图像分类准确性。模型还可以将学习到的知识应用于其他任务，包括目标检测、图像分割、文档理解和视频识别。 生成任务 利用VLMs从输入数据生成文本或图像。通过利用大规模训练数据，单个VLM通常能够执行多个图像到文本的生成任务，如图像描述和视觉问答。著名的例子包括SimVLM、BLIP和OFA等。更强大的VLMs，如BLIP-2、Flamingo和LLaVA，能够基于输入图像处理多轮对话和推理。随着扩散模型的引入，文本到图像生成也成为研究社区的焦点。通过在大量图像-文本对上训练，扩散模型能够根据文本输入生成高质量图像。这一能力还扩展到了生成视频、3D场景和动态3D物体。除了生成任务，还可以通过文本提示编辑现有图像。 视觉基础模型〓 ReTURN 〓 视觉基础模型（Vision Foundation Models, VFMs）是大型神经网络，旨在提取足够多样和表达能力强的图像表示，能够直接部署在各种下游任务中，类似于预训练的LLMs在下游NLP任务中所起的作用。 一个显著的例子是DINO，它使用自监督的教师-学生训练范式。所学习的表示在图像分类和语义图像匹配上都取得了良好效果。DINO中的注意力权重也可以作为观测场景中语义组件的分割掩码。后续的工作如iBOT和DINOv2通过引入掩码图像建模（MIM）损失进一步改进了表示。 SAM是一个基于transformer的图像分割模型，在包含11亿张带有语义掩码的图像数据集上训练，展示了强大的零样本迁移能力。DINO（Zhang等人）——不要与DINO（Caron等人）混淆——采用类似DETR的架构和混合查询选择进行目标检测。后续工作Grounding-DINO引入了文本监督以提高准确性。 Stable Diffusion，一个文本到图像生成器，通过对干净或人为噪声图像进行单次扩散步骤并提取中间特征或注意力掩码，被用作“真实”图像的特征提取器。这些特征最近被用于分割和图像匹配任务，原因在于扩散模型使用的训练集的规模和多样性，以及观察到的扩散特征的涌现属性，例如跨图像的零样本对应性。 任务和指标介绍〓 ReTURN 〓 任务 3D标注（3D to 文本）在给定场景或物体的3D数据后，3D标注任务是生成相应的简短自然语言描述。根据标注的数据类型和生成的标注类型，我们将该任务分解为几种常见的变体。 物体级标注 要求模型生成单个3D物体的简短自然语言描述。此描述应关注物体的关键特征，包括其形状和语义特征。 场景级标注 是为整个3D场景生成简短自然语言描述。此类描述通常关注全局场景信息（如房间类型和风格）、场景中的关键物体及其关系。我们将“基础标注”视为场景标注的一种变体，其中模型输出场景中物体之间关系的描述，并可能附带这些物体的位置信息。 3D密集标注 是在3D场景中定位物体实例并使用自然语言描述它们的联合任务。在这种情况下，输出还可能包含被标注物体的位置信息。通常，3D定位数据集中的参考描述用于生成3D密集标注所需的标注和位置信息。例如，Scan2Cap中的标注是使用ScanRefer中的参考表达生成的。 3D标注的评价指标需要将生成的标注与测试样本的真实标注进行比较。 精确匹配（Exact Match, EM） 要求生成的标注与真实标注完全匹配。精确匹配有不同的准确性阈值，表示为EM@K，意思是正确答案在模型生成的前K个答案之内。常用的阈值是EM@1和EM@10。然而，具有相同语义的自然语言标注可以有多种表达方式，因此标注的主要指标是自动文本生成指标，这些指标旨在测量匹配的n元语法或语义相似性，而不是完整句子的匹配。 BLEU 匹配预测标注和真实标注之间的n元语法，”BLEU@x”指的是匹配长度为”x”的n元语法（典型值在1-4范围内）。这仍然需要匹配确切的词，但对措辞的重排更为鲁棒。 ROUGE 同样旨在匹配n元语法，常用的ROUGE-L关注句子的结构相似性。 METEOR 基于单词匹配的精确度和召回率，“匹配”也存在于同义词和形态变体的单词之间。 CIDEr 根据n元语法的频率加权，频率越高的n元语法权重越低。 由于上述指标依赖于n元语法匹配，它们无法处理语义相似但不同的词。因此，引入了各种衡量语义内容重叠的指标，如SentenceSim和BERT Score。 对于3D密集标注，其中标注定位到场景的部分，需调整基准。通常仍使用BLEU、ROUGE、METEOR和CIDEr分数，但如果预测边界框与物体的交并比（IoU）低于阈值”k”，则分数设为零。常用的”k”值为0.25和0.5。然而，这些指标关注标注的召回率而忽略了误报。这一点在最近的工作中得到解决，后者还测量生成标注相对于BLEU、ROUGE、METEOR和CIDEr指标的精确度和F1分数。 任务 3D定位（3D加文本 to 3D位置）〓 ReTURN 〓 在给定一个3D场景和一个描述场景中物体相对其他物体的“参考表达”后，3D定位涉及生成目标物体的位置、边界框或分割掩码。 单物体定位 涉及在场景中定位单个查询物体，使用参考信息，如语言描述或附加手势。 多物体定位 涉及使用参考表达定位多个物体。这种定位有两种主要变体。第一种涉及单句描述，这可能是模糊的，可能指代3D场景中同一类别的零个、一个或多个目标物体。第二种变体使用段落长度的参考表达，描述多个可能属于不同类别的物体及其之间的空间关系。 3D定位的评价指标需要将预测位置（通常以边界框的形式）与测试样本中的物体的真实位置进行比较。Acc@K IoU是3D视觉定位中广泛使用的指标，测量正预测的百分比，其交并比（IoU）与真实值大于阈值K，通常设为0.25或0.5。值得注意的是，一些数据集在不同场景中评估性能。例如，ScanRefer将数据集分为唯一/多个/总体部分。一些方法测量平均IoU，而其他方法测量边界框中心之间的平均距离。对于多物体定位，使用F1分数作为指标。首先，根据IoUs在预测和真实边界框之间进行一对一匹配，然后将IoUs高于阈值的对视为真正例。 任务 3D对话（3D 加 文本 to 文本）〓 ReTURN 〓 自然地，我们可以考虑在3D场景中进行提问的任务，无论是在单轮对话设置中还是在更自然的多轮对话设置中。 3D问答（3D-QA） 是一个任务，要求模型在给定3D场景的情况下，生成用户提出问题的答案。问题的主题范围广泛，模型必须理解3D场景和问题才能生成正确的回答。问题包括简单任务，如确定物体的存在，以及更复杂的任务，如空间推理。由于有几个成熟的基准测试，而且基准中的大多数问题都是具有唯一答案的事实性问题，3D-QA是评估多任务模型能力的热门任务。 3D情境问答（3D-SQA） 是3D-QA的一个特例。关键区别在于3D-QA要求模型从旁观者的角度回答问题，旁观者可以获取所有关于场景的信息，而3D-SQA则需要从预定义情境中玩家的角度回答问题。例如，3D-SQA可能会在“站在餐桌后面并面对餐桌”的情境下问“我面前有多少把椅子？”。 3D对话 要求模型与用户进行连贯且自然的多轮对话，而不是单轮问答。例如，用户可能想了解一个房间，因此他们会不断地询问关于房间各部分的问题，而模型需要正确且连贯地回应。 评价指标涉及将模型的响应与测试样本的真实响应进行比较。对于3D-QA和3D-SQA，主要指标是精确匹配（Exact Match, EM），这意味着模型生成的答案必须与正确答案完全匹配。这是因为现有的3D-QA基准中的大多数问题都是事实性问题，只有一个明确的正确答案。 对于3D对话和任务规划，其答案不是唯一的，使用诸如BLEU、ROUGE、METEOR、CIDEr和SPICE等语义指标来评估生成响应与基准提供的参考答案之间的相似性。这些指标也用于3D-QA，特别是ScanQA基准，以在准确性之外测量语义相似性。 任务 3D具身代理（3D 加 文本 to 行动）〓 ReTURN 〓 考虑涉及与3D场景交互的任务，这些任务基于描述所需动作或目标的特定文本提示进行。 3D任务规划 是指用户提供一个高层次目标，模型需要列出实现该目标的低层次步骤。例如，给定一个房间的3D场景，用户可能会询问如何清洁房间，模型需要提供详细的清洁步骤。 3D导航 是指使3D代理（如机器人或虚拟角色）能够在3D空间中移动和定向。此任务涉及理解和解释3D环境，识别障碍物，并规划安全、高效的路径以到达指定目标。 3D操作 是指3D代理与其环境中的物体进行物理交互的能力。这可以包括拾取和移动物体，甚至是更复杂的动作序列，如组装部件或开门。 3D任务规划 的评价指标依赖于将模型的文本/标记输出与测试样本的真实动作进行匹配。使用BLEU、ROUGE、METEOR、CIDEr和SPICE来评估生成响应与真实答案之间的相似性。 3D导航 有两个主要指标来评估性能： 成功率（Success Rate, SR）：衡量3D代理是否在预定的距离阈值内到达目标位置。 按路径长度加权的成功率（SPL）：计算为SR乘以真实路径长度与实际路径长度的比率，旨在反映模型实现目标的效率。其他指标包括Oracle成功率（OSR）、轨迹长度（TL）和目标进度（GP）。 除了上述指标外，还需要考虑代理路径与语言指定路径的匹配程度（当使用语言指定详细路径时）。一个这样的指标是按标准化动态时间规整加权的成功率（SDTW），它结合了SR与代理路径和真实路径之间的差异。 3D操作 的关键指标是成功率，定义为成功操作的次数除以任务样本总数。不同的数据集对于如何使用文本表示其动作有不同的约定，例如使用结构化输出、使用标准化数值评分或引入新标记。 以上讨论专注于3D-LLMs方法中使用的指标。建议读者参考Gu等人的工作以了解导航指标的总结。 任务 文本到3D生成（Text to 3D）〓 ReTURN 〓 除了使用文本描述和与现有3D场景进行交互，还可以通过语言描述生成3D物体和场景。这里我们简要总结这一领域，详细综述参见Lee等人的工作。 3D物体生成 涉及根据文本描述生成单个物体的3D模型。文本输入可以提供关于物体类别、属性、部件结构和其他应反映在生成3D形状中的特性。 3D场景生成 是根据文本场景描述创建完整3D环境的任务，如房间或户外空间。这涉及生成文本中指定物体的3D模型，并根据文本中指定的约束（如物体类别、数量、空间关系和场景属性）智能地排列和组合多个3D物体模型。 3D编辑 指的是根据文本指令修改现有3D资产，如形状或场景。这可能涉及添加、移除或变换物体，改变材料或颜色，或根据给定文本改变高层次的场景属性。 3D生成任务的评价指标评估生成形状/场景的质量及其与输入文本的匹配程度。常用的几何生成质量衡量指标包括Chamfer Distance（CD）和Mesh-Volume/Surface Distance（MVD）。CD通过对与真实3D数据的点对点距离平方求和计算，而MVD则通过计算两个网格之间的体积/表面距离来衡量几何误差。 为了评估整体质量，分类准确性检查语义属性是否被保留，而Frechet Inception Distance（FID）则捕捉真实感和多样性。为了检查生成形状是否与输入文本匹配，通常测量文本与3D形状对齐嵌入（如ULIP）或渲染图像（如CLIP）的相似性。人类研究也常用于评估。然而，最近的研究表明，使用类似GPT-4的LVLMs可以作为人类评判的替代方案。 对于基于文本的3D编辑，CD和IoU评估指令编辑在输入几何体上应用的效果，确保没有过度变形。 关于3D TASKS WITH LLMS〓 ReTURN 〓 3D场景理解任务已被广泛研究。其核心在于识别和分类指定3D环境内的所有物体，即语义理解或实例级理解。这一阶段至关重要，因为它为更细致的解读奠定了基础。随后，场景理解的更高层次集中于空间理解，即构建空间场景图和对象关系的语义。此外，还可以预测潜在的交互，例如可供性、场景变化以及理解场景的更广泛背景，如功能和美学风格。3D数据还带来一些2D数据所没有的独特挑战，如获取和标注3D数据的相对高成本、稀疏的3D数据结构以及需要协调同一物体的多个（可能被遮挡的）视角。为此，研究人员利用语言的力量，将3D世界的语义和关系嵌入其中。 最近将大型语言模型（LLMs）与3D数据集成的努力显示出在实现多层次理解和交互方面的潜力，利用LLMs固有的优势，如零样本学习、上下文学习、逐步推理和广泛的世界知识。 在第4.1节以及图2中，我们简要描述LLMs如何处理3D场景信息，重点介绍如何将3D特征与语言对齐，以便通过LLMs解释和推理这些信息，这是后续部分的基础。本节其余部分按照图3中呈现的分类结构展开，描述LLMs在解决3D任务中所扮演的角色。我们首先在第4.2节展示LLMs的世界知识（有时称为“常识知识”）和推理能力如何增强3D任务的表现。第4.3节详细介绍如何将多个3D任务整合到一个LLM中以实现多任务学习。第4.4节探讨如何使用LLMs作为统一界面结合其他模态。随后在第4.5节中描述LLMs如何作为具身代理与3D世界进行交互。最后，第4.6节展示LLMs如何作为助手生成语义多样的3D物体和场景。此外，我们提供表1，以对比3D-LLMs方法在三个轴上的差异：3D组件、LLMs组件以及3D视觉与语言的对齐，旨在提供对这一不断发展的领域的高层次见解。 关于LLMs如何处理3D场景信息？〓 ReTURN 〓 传统LLMs仅限于文本作为输入和输出，使得能够接收3D信息成为所有3D-LLM方法的首要关注点。总体思路是将3D物体或场景信息映射到语言空间，使LLMs能够理解和处理这些3D输入。具体而言，通常包括两个步骤：（i）使用预训练的3D编码器处理相应的3D表示，生成原始3D特征；（ii）采用对齐模块将这些3D特征转换为LLMs可以处理的3D标记，类似于第2.2.1节中提到的标记化过程。预训练的LLMs随后可以在生成输出时使用这些对齐的3D标记。 鉴于3D表示的多样性，如第2.1节所述，有多种方法可获取3D特征。如表1的3D几何列所示，点云因其简单性和与各种预训练3D编码器的兼容性最为常见，这使其成为多任务和多模态学习方法的热门选择。多视图图像也经常使用，因为2D特征提取的研究已很成熟，这意味着3D特征提取仅需要一个额外的2D到3D提升方案。通过深度摄像头获取的RGB-D数据通常用于3D具身代理系统，以提取导航和理解所需的视点相关信息。3D场景图是一种更抽象的3D表示，在建模物体的存在及其关系以及捕捉场景的高层次信息方面表现优越，常用于3D场景分类和规划任务。NeRFs在3D-LLM方法中使用较少，我们认为这是由于其隐式性质，使其更难以标记和与前馈神经网络集成。 当前方法使用不同的架构和模块来将3D特征与LLM输入空间对齐。如表1的3D+LLM列所示。对于仅接受3D输入的模型（图2a），使用线性层或MLP作为对齐模块将3D特征转换为LLM输入空间。接受3D和文本输入的模型通常使用两个独立的分支来对齐3D特征和文本（图2b）。一些工作采用单层vanilla transformer，使3D物体特征在对齐过程中相互关注。其他工作创建基于transformer的对齐模块，调整标准transformer架构以更好地适应不同类型的3D数据，如密集点云和稀疏LiDAR扫描。文本则使用预先存在的LLM文本嵌入表进行编码。 其他工作遵循Q-Former风格的方法，引入固定长度的查询标记作为额外输入，并采用基于BERT的结构，以促进对齐过程中3D和文本特征之间的交互。大多数上述三种类型的架构通过利用3D标注数据集实现对齐，其中使用标注损失，即LLMs生成的标题与场景简要描述的交叉熵损失，微调对齐模块，同时冻结预训练的3D特征提取器和LLM。 最后，一些模型使用闭源模型如ChatGPT，根本不训练对齐模块。相反，将3D数据直接生成文本描述，如描述3D边界框、位置和关系，或使用现有标题。这些文本描述输入到ChatGPT。这些工作中未提出额外的对齐模块，因此不需要训练。 使用LLMs提升3D任务性能经过大量数据训练的LLMs已被证明能够获得关于世界的常识知识。LLMs的世界知识和推理能力在提升3D场景理解和重构若干3D任务流程方面展现了潜力。本节重点介绍利用LLMs提升现有3D视觉语言任务性能的方法。 在应用LLMs于3D任务时，可以将其用法分为两大类：知识增强和推理增强。知识增强方法利用LLMs中嵌入的广泛世界知识来提升3D任务性能。这可能提供上下文见解，填补知识空白，或增强3D环境的语义理解。推理增强方法则依赖LLMs逐步推理的能力，从而更好地推广解决更复杂的3D挑战。以下两个小节分别描述这些方法。 知识增强方法有几种方法利用LLMs的世界知识。Chen等人使用LLMs从RGB-D图像进行3D房间分类。这里，LLMs中嵌入的知识用于根据房间内的物体类别信息确定房间类别。首先，该方法从Matterport3D数据创建场景图，节点代表区域和物体，物体节点链接到房间节点。然后，选择关键物体形成每种房间类型的查询。LLMs对选定物体提取的描述进行评分，得分最高的预测房间标签。还可以提供大小或位置等空间信息。 ViewRefer使用LLMs扩展带有视图相关描述的定位文本。例如，给定原始文本“面向沙发前面，右边的桌子”，LLMs生成另一个视角的相似句子，如“背对沙发前面，选择左边的桌子”。通过多次改写输入文本及其相对视角的同义词，模型改进了跨视角定位。还采用了带有视角间注意力的融合transformer，并包括可学习的多视角原型，以捕捉视角间知识，进一步提升3D定位性能。 Abdelreheem等人解决3D形状中的语义对应问题。他们通过将渲染视图输入BLIP2模型生成类别建议列表来分类3D形状。ChatGPT将这些统一为每个形状的单一类别，还生成语义部分名称和成对映射（例如，手臂→翼）。然后，一个3D分割器基于语义区域分割形状，利用部分映射生成稀疏对应图。 上述知识增强策略在零样本场景中表现出色，尤其是当没有针对特定物体或场景类型的标注3D数据时。这允许关于物体部分、关系和语义的开放性推理，如Chen等人生成空间和语义物体描述，ViewRefer描述多视角物体关系，Abdelreheem等人跨形状生成和匹配物体部分语义。 推理增强方法除了世界知识，LLMs的推理能力也有助于解决其他3D任务，特别是在具有详细几何和多个物体的复杂3D场景中的视觉定位。在这种情况下，物体的文本描述应包括它们的外观和与周围物体的空间关系。普通的定位方法在这种设置中往往表现不佳，因为它们无法理解详细的文本描述。 LLM-Grounder、Transcribe3D和零样本3DVG通过利用LLMs的推理能力分析文本描述，并生成定位物体的指令序列来解决这个问题。具体而言，LLM首先从文本描述中识别锚点和目标物体。然后，它基于定位工具返回的坐标分析多个候选物体之间的空间关系（或描述的属性），选择与文本描述最匹配的候选物体。 此外，Transcribe3D和LLM-Grounder采用多轮互动问答过程帮助用户澄清意图，促使他们提供更多信息以获得更准确的结果。LLM-Grounder包含多种定位工具选择，如OpenScene或LERF，以适应不同的3D表示，如点云或NeRF。这些方法的一个常见缺点是LLM的“盲目性”，因为它只提供3D场景的抽象文本描述，而不是场景的原始点云。这可能导致重要场景细节的丢失。因此，当3D场景包含多个同类物体时，缺乏必要的场景细节意味着无法解决基于文本的引用中的歧义，从而限制整体性能。 除了视觉定位，LLMs的推理能力也促进了其他任务。3DAP利用GPT-4V从2D图像推断物体的3D信息，使用视觉提示技术为输入图像注释3D轴，以增强LLM的3D尺度感知。ConceptFusion使用GPT-3生成指令，利用预定义的基本空间比较模块，使其提议的3D特征图能够进行更复杂的空间推理。 使用LLMs进行3D多任务学习许多研究集中在利用LLMs的指令跟随和上下文学习能力，将多个3D任务统一到单一的语言空间中。通过使用不同的文本提示来表示不同的任务，这些研究旨在让LLMs作为统一的对话界面。实现多任务学习通常包括几个关键步骤，首先是构建3D-文本数据对。这些数据对需要设计任务指令的文本形式，并定义每个不同任务的输出。 接下来，将3D数据（通常是点云）输入3D编码器以提取3D特征。对齐模块随后用于：(i) 在多个层次上（对象层次、关系层次和场景层次）将3D特征与LLMs的文本嵌入对齐；(ii) 将3D特征转化为LLMs可解释的tokens。最后，需要选择合适的训练策略，如单阶段或多阶段的3D-语言对齐训练和多任务指令微调。 在本节的其余部分，我们详细探讨这些方面。此外，我们在表2中总结了本节回顾的每种方法的范围和能力。 多任务学习的数据如表2所示，我们将任务分为四类：描述、定位、问答（QA）和具身代理任务（如规划、导航和操作）。相应地，每个任务的文本输出遵循预定义格式。对于描述和问答任务，输出是纯文本，不受特定格式约束。定位任务的输出是一个3D边界框，通常是所指物体中心的坐标及其3D尺寸。通常，点和大小的值被标准化到0-255范围内，以限制LLMs需要预测的token范围。 对于规划，模型输出一个以文本形式执行任务的步骤序列；而对于导航，输出是一系列空间坐标；对于操作，输出是以文本形式的动作序列。现有方法遵循这些指南构建多任务指令微调数据集。 一旦确定了文本格式，不同的方法使用不同的策略获取数据集的文本注释。几种方法利用人工标注生成每个样本的“真实”注释，然而这可能是一个昂贵且耗时的过程。另一种方法是使用ChatGPT为每个样本生成文本注释，这是3DMIT、LiDAR-LLM、Chat-3D和Chat-3D v2使用的策略。在这里，3D场景数据被转换为文本（通常通过描述物体边界框和空间关系），并创建任务描述以描述所需输出。 为了引导ChatGPT生成预期的任务输出格式，提供了示例，这允许ChatGPT进行上下文学习，为其他3D场景生成合理的文本注释。或者，其他多任务数据集仅通过合并现有的3D视觉语言（VL）数据集来构建。一些多任务数据集结合了这三种方法，旨在结合人工注释的准确性和使用LLM生成注释的可扩展性。 多任务3D模型的训练训练LLMs以处理多个3D任务的第一步是获取有意义的3D特征，提取方法因3D场景类型而异。对于单个物体点云，Point-LLM、Chat-3D和GPT4Point使用Point-BERT提取3D物体特征。对于室内场景，LEO使用PointNet++进行特征提取，而Chat-3D v2和3DMIT则分割场景，并使用Uni-3D提取每个分割部分的特征。同时，MultiPLY将提取的物体特征整合到场景图中，以表示整个场景。 3D-LLM和Scene-LLM从2D多视图图像中提取特征转换为3D表示。3D-LLM从Mask2Former或SAM中提取2D语义特征。Scene-LLM遵循ConceptFusion的方法，融合全局信息和局部细节，将逐像素的CLIP特征映射到逐点的3D特征。对于户外3D场景，LiDAR-LLM使用VoxelNet提取3D体素特征。 对于对齐模块，如第4.1节所述，使用了各种网络架构。值得注意的是，MultiPLY [24] 使用不同的线性层来对齐每种模态的特征。Chat-3D [171] 和 Chat-3D v2 [172] 采用单层vanilla transformer，以允许3D对象特征在对齐过程中彼此关注。LEO [270] 和 LiDAR-LLM [271] 修改了transformers作为对齐模块，以更好地适应不同类型的3D数据（密集点云与稀疏LiDAR）。LEO [270] 修改了自注意力机制，明确编码点云中对象对之间的空间关系。相反，LiDAR-LLM [271] 使用自注意力和交叉注意力机制来对齐鸟瞰视图（BEV）特征与文本特征。3D-LLM [153] 和 GPT4Point [268] 采用了Q-Former，而LL3DA [19] 在Q-Former基础上增加了一个分支，允许查询令牌与用户提供的视觉提示交互。 LLMs 可以通过不同策略进行微调以结合多种3D任务，这在第2.2.3节中有讨论。LEO [270] 和 3DMIT [269] 使用低秩适应（LoRA）进行微调。因此，包括对齐模块和3D编码器在内的可训练参数总量不到原始LLMs参数的10%，显著提高了训练效率。Chat-3D [171]，LL3DA [19]，Chat-3D v2 [172]，LiDAR-LLM [271] 和 MultiPLY [24] 采用自适应微调。具体来说，这些模型包括模块以对齐3D场景中的空间信息与语言，例如transformer层，以捕捉对象关系。这些模块连同预训练的3D编码器和LLMs一起进行微调以实现对齐。3D-LLM [153]，Scene-LLM [266]，Point-LLM [267] 和 GPT4Point [268] 采用层冻结策略。通过冻结大多数LLM层并微调某些层如嵌入层，这种策略在保留语言能力的同时提高了3D理解能力。最后，Agent3D-Zero [17] 使用提示微调，这是一种不需要训练的方法，用于指导LLMs理解3D任务。该方法使用定制提示，在3D场景的BEV图像上添加网格线和刻度线，帮助2D VLMs理解3D几何。 训练这些模型以进行3D多任务学习还涉及3D-语言特征对齐的微调。Point-LLM [267]，3D-LLM [153]，Scene-LLM [266]，LEO [270] 和 GPT4Point [268] 都采用单阶段对齐方法。具体来说，Point-LLM [267] 仅使用字幕数据训练一个MLP，并另外更新输入嵌入层以适应新添加的标记点云令牌的起始和结束标记（⟨p start⟩，⟨p end⟩）。3D-LLM [153] 使用自定义数据集训练对齐模块，并更新输入和输出嵌入层以适应新添加的位置令牌。Scene-LLM [266] 仅训练一个线性层，使LLMs能够通过相机和世界坐标系中的3D框架语言对字幕任务理解自我中心和场景中心视角。它还更新输入嵌入层以适应新添加的3D令牌的起始和结束标记（⟨3D⟩，⟨/3D⟩）。LEO [270] 也使用字幕任务训练对齐模块，但独特地收集了三种类型的字幕数据：对象级[293]，场景中的对象[228, 294] 和场景级[295]，并用这三种数据集训练其对齐模块。GPT4Point [268] 遵循BLIP2 [195] 的结构和训练策略，通过三个任务实现对齐：点文本对比（PTC），点文本匹配（PTM），和点字幕生成（PTG）。 与这些单阶段对齐方法相反，LiDAR-LLM [271]，Chat-3D [171] 和 Chat-3D v2 [172] 都采用两阶段3D-语言对齐过程。LiDAR-LLM [271] 通过3D字幕任务分两个阶段增强局部和全局场景感知：首先集中于单视图字幕，然后扩展到全景场景描述。他们通过结合字幕和对齐任务开发实例级感知能力。Chat-3D [171] 首先使用3D对象分类数据集[296, 293, 297] 对齐3D对象与文本，旨在通过仅更新对齐模块来最大化映射3D对象特征与对象类别词嵌入之间的余弦相似度。在场景级对齐的第二阶段，它使用ScanRefer [218] 以实现字幕能力，特别更新一个transformer层来建模对象的空间关系。同样地，Chat-3D v2 [172] 综合对象级和场景级对齐，在第二阶段另外训练一个位置嵌入层。为了提高训练效率，LL3DA [19] 和 3DMIT [269] 跳过对齐阶段，专注于以下所述的指令微调阶段。 几乎所有的多任务学习方法最终都需要根据指令完成各种3D任务。因此，作为训练的最后阶段，每种方法通常使用它们自己构建的多任务指令跟随数据集进行指令微调。由于所有任务输出都统一为文本形式，训练损失使用的是LLMs的标准自回归损失。这一阶段通常涉及对齐模块和LLM的联合训练。一个例外是Agent3D-Zero [17]，它通过向GPT4V提供来自不同视点的2D图像完成各种3D任务，因此不需要任何训练。 LLMs作为3D多模态接口〓 ReTURN 〓 除了探索3D多任务学习者，一些最新研究还整合了不同模态的信息，以进一步提高模型的能力并实现新颖的交互。除了文本和3D场景，3D多模态LLMs还可能包括2D图像、音频或触觉信息作为输入。大多数工作旨在构建一个跨不同模态的共同表示空间。由于一些现有工作已经提供了将文本、图像或音频映射到共同空间的预训练编码器，一些工作选择学习一个3D编码器，将3D嵌入对齐到其他模态预训练编码器的嵌入空间。JM3D-LLM [279] 学习一个3D点云编码器，将点云的嵌入空间对齐到SLIP [301] 的文本-图像嵌入空间。它在训练期间渲染一系列点云图像，并构建一个层次化的文本树，以实现详细对齐。Point-Bind [272] 也学习了一个类似的3D编码器，并将其对齐到ImageBind [302]，以统一图像、文本、音频和点云的嵌入空间。这使得可以使用不同的任务头处理不同模态之间的任务，如检索、分类和生成。然而，这种方法的一个显著限制是，由于计算成本高，3D编码器只能处理小规模对象级场景，而不能处理包含数百万点的大场景。此外，大多数预训练多模态编码器如CLIP设计用于单对象场景，不适用于包含多个对象和局部细节的大场景。 大场景需要更细致的设计以整合多种模态。ConceptFusion [18] 构建了一个增强的特征图，融合每个组成图像的全局信息和局部细节。通过使用已对齐不同模态（包括文本和音频）的预训练特征提取器实现这一目标。然后使用传统SLAM方法将特征图映射到场景的点云。MultiPLY [24] 采用类似于ConceptGraph [290] 的表示方法。它识别场景中的所有显著对象，获取每个对象的全局嵌入，最终构建一个场景图。结果表示是与Llama [140] 嵌入空间对齐的场景嵌入。包括音频、温度和触觉在内的其他模态嵌入也可以通过线性投影映射到同一空间。所有嵌入被标记并一次性发送到LLM。与对象级场景方法相比，可以处理大场景的方法通过依赖预训练编码器弥合模态差距，而不是从头学习新的编码器，从而降低成本。 LLMs用于具身智能体〓 ReTURN 〓 LLMs的规划、工具使用和决策能力可以用于创建3D具身智能体。这些能力使LLMs能够在3D环境中进行导航，与物体交互，并选择适当的工具来执行特定任务。本节描述了3D具身智能体在规划、导航和操作任务中的表现。 3D任务规划对于具身智能体，‘任务规划’是指在给定任务描述和3D环境的情况下生成执行特定任务的步骤。任务规划通常是导航和操作任务的先决条件，因为规划的准确性直接影响后续任务的性能。LEO [270] 和 LLM-Planner [12] 利用LLMs生成逐步计划，并根据环境感知动态调整。LEO [270] 强调基于当前场景配置的场景感知规划，而LLM-Planner [12] 采用GPT-3将规划分为高层次的子目标和低层次的动作，并在任务执行过程中遇到障碍时重新规划。3D-VLA [276] 通过生成世界模型整合3D感知、推理和行动。它通过使用其生成模型预测未来状态表示（例如目标图像和点云）来增强规划能力。Agent3D-Zero [17] 引入了Set-of-Line Prompting（SoLP），通过生成多样化的观察视点增强VLM对场景几何方面的理解。具体来说，SoLP在BEV图像上叠加网格线和刻度线，并提示VLM提供更准确的相机位置和方向，从而使VLM能够理解3D空间概念。UniHSI [277] 解决了人与场景交互（HSI）任务，涉及基于输入语言命令在3D环境中生成人与物体之间的交互。它使用LLM作为规划器，将语言命令翻译为任务计划，表示为接触链（CoC），即表示人体关节点与物体位置之间时间关系的序列。 虽然上述方法主要关注单个场景内的规划，SayPlan [275] 可以处理多个房间和楼层，通过使用3D场景图进行语义搜索和将经典路径规划与迭代重规划管道结合以细化计划。 3D导航3D导航是指具身智能体在3D环境中移动和定向的能力，通常基于视觉输入和语言指令。每种方法——LEO [270]，Agent3D-Zero [17]，LLM-Planner [12] 和 NaviLLM [11]——都以不同方式实现3D导航。LEO [270] 处理自我中心的2D图像和以对象为中心的3D点云以及文本指令。它生成对应于可执行导航命令（如‘向前移动’或‘向右转’）的动作令牌序列。LEO采用‘最短路径导航试验’，提供比人类演示更少噪音和更直接的学习环境。Agent3D-Zero [17] 通过持续选择基于环境评估的新视点进行导航。它结合历史视点数据，以优化导航路径达到特定目标，如在办公室环境中找到打印机。LLM-Planner [12] 采用分层方法，首先生成高层次计划作为子目标序列，然后由低层次规划器翻译为一系列原始动作。这使整个过程能够适应当前环境。NaviLLM [11] 使用基于方案的指令将各种具身导航任务转化为生成问题。这些指令包括4个要素：由词序列定义的任务、所有可达视点的观察、过去视觉观察的历史记录和指导动作生成的输出提示（例如选择方向或对象）。 3D物体操作在3D具身智能体的背景下，操作指的是它们与物体物理交互的能力，从移动物体到复杂的操作序列，如组装零件或开门。使LLMs能够执行操作任务的核心思想在于将动作序列标记化。为了让LLMs输出特定动作，首先需要定义动作令牌，使LLMs能够根据任务和3D场景上下文生成这些动作。随后，CLIPort [242] 或机器人手臂中的运动规划模块等平台将这些标记化动作翻译为智能体执行的物理动作。 LEO [270]，MultiPLY [24] 和 3D-VLA [276] 各自使用不同的动作令牌，将口头或书面指令转换为机器人在3D空间中的动作。LEO [270] 使用超过500个特定令牌使机器人动作更加精确。具体来说，对于CLIPort [242] 任务，动作姿态使用516个令牌进行编码：320个令牌用于x轴姿态区间，160个令牌用于y轴，36个令牌用于z轴旋转区间。MultiPLY [24] 引入了如⟨SELECT⟩用于物体交互，⟨NAVIGATE⟩用于移动，⟨OBSERVE⟩用于观察，⟨TOUCH⟩用于触觉反馈，⟨HIT⟩用于听觉反馈，⟨PICK-UP⟩和⟨PUT-DOWN⟩用于操作，⟨LOOK-AROUND⟩用于感知的令牌。这种方法还集成了感官反馈（触觉、温度和听觉），增强了机器人与周围环境的交互。3D-VLA [276] 包含(i) 对象令牌（⟨obj⟩⟨/obj⟩）用于识别操作对象，(ii) 位置令牌（⟨loc0-255⟩）用于空间定位，以及(iii) 专门的机器人动作令牌，如手臂位置/旋转/夹持器状态。令牌结构通过⟨ACT SEP⟩分隔，便于理解和执行复杂3D操作。 虽然这些系统通过将指令映射到动作来使机器人执行复杂任务，但它们忽略了可操作物体的语义理解，通常无法区分适合与不适合操作的部分。为解决这一问题，VoxPoser [13]，LAN-grasp [14] 和 ManipLLM [15] 关注“可操作性”，创建可操作性地图以表示可用于执行特定任务的物体和特征，如可抓取的把手、可按压的按钮或可移动的物体。具体来说，VoxPoser [13] 使用LLM分解自由形式的语言指令，推断可操作性和约束条件，并通过使用代码接口与VLM交互生成3D体素地图。这些地图可以生成对动态变化具有鲁棒性的闭环机器人轨迹，能够在接触丰富的环境中从在线经验中学习。LAN-grasp [14] 通过结合多个模型识别可抓取部分，利用基础模型加深机器人对物体的语义理解，无需重新训练。ManipLLM [15] 通过从文本提示、RGB图像和深度图中识别接触点和夹持器方向的3D坐标来预测操作结果。 LLMs用于3D生成〓 ReTURN 〓 传统上，3D建模是一个复杂且耗时的过程，需要详细关注几何形状、纹理和光照以实现逼真的效果。本节探讨了LLMs与3D生成技术的整合，展示了语言如何提供生成情境化对象的方式，并为3D内容创建和操控提供创新解决方案。 对象级生成Shape-GPT使用特定形状的3D VQ-VAE将3D形状量化为离散的“形状词”令牌。这使得形状数据可以与文本和图像一起整合到T5语言模型的多模态输入中。这种多模态表示使T5能够学习跨模态交互，例如文本到形状的生成和形状编辑/补全。GPT4Point采用双流方法，通过Point-QFormer将点云几何与文本对齐，然后将其输入耦合的LLM和扩散路径，以理解文本并生成高保真度的符合文本输入的3D对象。 相比之下，MeshGPT和PolyGen不依赖文本生成，但它们仍采用类似于LLM中序列建模的自回归方法。MeshGPT使用图卷积将网格几何/拓扑编码为丰富的嵌入，通过残差向量量化压缩，然后输入GPT样式的变压器，以自回归方式预测生成具有期望属性的网格的令牌/嵌入。PolyGen是一种基于自回归变压器的3D网格模型，利用指针网络。它包括一个无条件建模网格顶点的顶点模型和一个基于输入顶点条件建模网格面的面模型，使用自回归网络输出面索引和顶点坐标，以生成多样化的高质量网格。 场景级生成Holodeck和GALA-3D采用多阶段管道，从文本逐步细化初始粗略的3D场景布局为详细的逼真3D环境。Holodeck利用专门的模块创建基本布局，选择材料，并根据GPT-4的空间推理和布局/风格建议融入门窗等元素。然后根据GPT-4的文本描述用Objaverse资产填充布局。一个优化器安排这些对象，遵循从GPT-4获得的空间关系约束，确保真实的对象布局和交互。 GALA-3D首先使用LLM从文本生成粗略布局，然后将其转化为3D高斯表示。此表示作为创建详细3D内容的基础，使用实例级文本到图像扩散先验。它采用组合优化来微调布局引导的高斯参数，确保最终场景在对象放置、规模和交互方面与文本对齐。 两者都利用LLMs的互补优势提取高层语义布局，并使用生成模型/优化将这些布局转化为几何和物理上合理的3D场景。 程序生成与操控LLMR、3D-GPT和SceneCraft采用模块化架构，具有专门的组件/代理用于交互式3D世界创建和从自然语言生成代码。LLMR包括用于生成Unity场景代码的不同组件，理解现有场景对象和属性以实现修改，识别执行指令所需的功能，并评估最终代码质量。同样，3D-GPT包含解释指令和确定所需生成功能的组件，丰富描述以包含详细的建模属性，并将丰富的描述翻译为Blender API的Python代码。总体而言，这些方法展示了LLM组件的任务分解和专业化，以处理指令解释、功能映射和强大的代码生成。 视觉语言模型与三维任务系列(3D TASKS WITH VLMS)〓 ReTURN 〓 利用VLMs进行3D任务虽然第4节讨论了将LLMs整合到3D任务中的方法，但大量研究已经通过2D视觉-语言模型（VLMs）的视角探索了3D理解的各个方面。VLMs包含更丰富的视觉信息，可以直接与3D关联。本节回顾了最近一系列论文的贡献，涵盖了语言驱动的开放世界理解、实例级理解、统一的端到端架构、空间推理、生成等内容。 开放词汇3D场景理解开放词汇3D场景理解旨在使用自然语言描述而非预定义的类别标签来识别和描述场景元素。OpenScene采用零样本方法，通过预测与CLIP的文本和图像像素嵌入在共享特征空间中共嵌入的3D场景点的密集特征，实现任务无关的训练和开放词汇查询，以识别对象、材料、可操作性、活动和房间类型。CLIP-FO3D采用类似方法，修改CLIP以从3D场景中提取密集像素特征，这些特征被投影到点云，然后通过知识蒸馏训练一个3D模型以传递CLIP的知识。 Semantic Abstraction从CLIP中提取相关性图作为抽象的对象表示，以推广到新的语义、词汇和领域。Open-Fusion结合SEEM视觉-语言模型和TSDF 3D映射，用于实时开放词汇场景创建和查询，利用基于区域的嵌入和置信图。 PLA和RegionPLC等方法利用对比学习将字幕与2D和3D数据模式结合起来，以关联视觉和语义信息。PLA使用3D-字幕对和对比学习，将多视图图像与字幕关联，以学习视觉-语义表示，而RegionPLC通过结合从2D模型映射到3D点的区域级字幕提出了区域感知对比学习。OVIR-3D将2D区域提案和来自现成2D检测器的文本对齐特征融合到3D实例中，以实现高效的开放词汇检索。CoDA在其3D新对象发现（3D-NOD）策略中使用带注释的基本类别的3D几何先验和CLIP的2D语义先验。其Discovery-driven Cross-Modal Alignment（DCMA）对齐3D和图像/文本特征，以实现新对象的定位和分类。 实例级场景理解工作如Open-Mask3D和Open3DIS利用预测的类别无关3D实例掩码和2D分段级CLIP嵌入，以实现开放词汇3D实例分割。OpenIns3D在没有对齐图像的情况下通过“Mask-Snap-Lookup”流水线实现开放词汇理解，该流水线预测3D掩码提案，生成合成场景图像，并通过语言模块为掩码分配类别。Rozenberszki等人建议利用CLIP特征为3D语义和实例分割提供支持。 利用NeRFs进行语言落地在开放词汇场景理解中表现出良好结果。DFF、LERF、VL-Fields和3D-OVS等方法通过最小化体渲染特征相对于2D特征的误差，将DINO或CLIP等2D特征提取器的知识蒸馏到3D特征场中，从而实现基于查询的本地编辑和将语言嵌入到神经隐式表示中。LERF通过体渲染CLIP嵌入优化密集的、尺度条件的3D语言场。LangSplat和N2F2通过分层监督和多尺度特征场展示了在3D高斯喷溅表示中进行高效的开放词汇查询和交互。 文本驱动的3D生成第4.6节讨论了使用LLMs进行3D生成的方法。这里，我们调查利用2D VLMs指导和文本到图像扩散模型进行文本到3D生成的方法。早期的工作如DreamFields、CLIP-Mesh、CLIP-Forge和Text2Mesh探索了由CLIP指导的零样本3D生成。DreamFusion引入了得分蒸馏采样（SDS），通过使其从任意视角渲染的图像看起来非常真实来优化3D表示的参数，该方法由预训练的2D扩散模型评估。它使用文本到图像的Imagen模型通过SDS优化NeRF表示。 Magic3D提出了一个两阶段框架：首先使用低分辨率扩散先验和稀疏3D哈希网格生成粗略模型，然后使用高效的可微渲染器和高分辨率潜在扩散模型优化带纹理的3D网格模型。Fantasia3D将几何和外观解耦，使用混合DMTet表示和空间变化的BRDFs。ProlificDreamer引入了变分得分蒸馏（VSD），这是一个基于粒子的框架，将3D参数视为随机变量，以提高保真度和多样性。Dream3D利用显式3D形状先验和文本到图像扩散模型来增强文本引导的3D合成。MVDream采用多视图一致的扩散模型，可以在少量数据上进行个性化训练，以实现个性化生成。Text2NeRF结合NeRF表示和预训练的文本到图像扩散模型，从语言生成多样化的室内/室外3D场景。 除了同时生成几何和外观外，一些研究还探索了基于给定几何形状合成纹理的可能性。 对于人类头像，AvatarCraft使用扩散模型从文本提示指导神经隐式场几何/纹理学习。此外，它通过一个明确的变形场将目标人类网格映射到模板人类网格，实现了这些人类头像的动画化。AvatarCLIP提出了一个零样本的CLIP监督框架，用于从文本生成3D头像、几何雕刻、纹理映射和动作合成。CG-HOI使用扩散模型从文本描述动态的人类-对象交互。GenZI通过预训练的视觉-语言模型蒸馏关于人类交互的信息，从文本提示中生成零样本的3D人类-场景交互合成。 在探索组合生成方面，CG3D通过组合单个对象而不使用边界框，生成可扩展的3D场景，使用显式3D高斯辐射场。Po等人引入了局部条件扩散，通过文本提示和边界框实现细粒度场景控制。GraphDreamer通过将场景图分解为全局-局部描述来优化对象SDFs，从而生成组合场景。总体而言，这些方法结合了扩散模型、视觉-语言模型、神经表示和3D先验，实现了对象、头像和场景的文本到3D生成。 三维视觉与语言的端到端架构预训练于大型3D-文本数据集上的Transformer模型可以学习到强大的联合表示，能够跨越视觉和语言模态。3D-VisTA 是一种Transformer模型，使用自注意力机制共同建模3D视觉和文本数据，有效地在遮蔽语言/对象建模和场景-文本匹配等目标上进行预训练。UniT3D 采用统一的Transformer方法，结合PointGroup 3D检测骨干网、BERT文本编码器和多模态融合模块，在合成生成的3D-语言数据上进行联合预训练。SpatialVLM 采用另一种方法，在大型合成3D空间推理数据集上共同训练VLMs，提升3D空间视觉问答任务的性能，并实现链式推理等应用。Multi-CLIP 预训练了一个3D场景编码器，以将场景特征与CLIP的文本和图像嵌入对齐，旨在通过转移CLIP的知识来改善3D理解。 除了预训练方法，研究人员还探索了将3D感知与语言能力统一在端到端框架中的架构。D3Net 将密集标注和视觉定位与3D对象检测器、从检测中生成标注的发言者以及使用标注区分对象的听者结合起来。Uni3DL 作用于点云，包含文本编码、点编码、语义/掩码预测和多种任务输出（如分割、检测、定位和标注）的模块。InstanceRefer 使用全景分割和语言提示基于语言描述过滤实例候选对象，以进行3D点云中的视觉定位任务，而LanguageRefer 将语言嵌入与3D边界框的空间嵌入结合起来。3DVG-Transformer 也解决了点云中的3D定位问题，具有坐标引导的上下文聚合模块和多重注意力机制以实现有效的特征融合。 数据集〓 ReTURN 〓 我们现在提供用于训练和评估3D视觉-语言模型的数据集的高层概述。在表3中，我们列出了数据集及其用于的任务，以及关于3D扫描和注释的信息。在图4中，我们在时间线上展示了这些数据集，显示每个数据集的3D信息来源。目前的3D视觉-语言数据集几乎完全通过对现有的3D视觉数据集进行人工、模型或模板注释生成。正如在表3中所见，大多数现有数据集专注于真实的室内场景，这可以部分解释为大多数现有数据集使用ScanNet 和3RScan 的3D扫描。许多这里展示的数据集共享相同的3D数据，而主要在其选择的注释策略和设计用于的3D视觉-语言任务上有所不同。 用于语言导航和操作的3D数据集通常围绕特定需求设计，并且与现有的研究主体有很大的重叠。我们建议读者参考现有的综述论文以获得这些数据集的概述。同样，对于文本到3D生成数据集，我们建议读者参考Lee等人 最近的综述。由于之前的广泛覆盖，我们在这里省略了进一步的讨论，因为许多方法使用的是2D视觉-语言数据而不是特定的3D数据集。 Cap3DCap3D 是一个3D对象标注数据集，基于Objaverse 数据集的66万个对象开发。它通过从3D对象的多个视图生成2D图像标注，并使用图像-文本对齐和LLMs进行整合来构建。 Text2ShapeText2Shape 是ShapeNet 中的8,447个桌子和6,591个椅子的人工标注形式，结合了用模板生成标签的原始形状的程序生成数据集。它最初用于生成性文本到3D形状任务。 SceneVerseSceneVerse 是一个大型多用途的注释场景数据集，通过汇编现有3D数据集的68k场景制作。SceneVerse包含总计250万个视觉-语言对，用于对象标注、场景标注和生成相对描述，主要通过使用3D场景图和LLMs生成。 nu-Captionnu-Caption 是一个包含来自nuScenes 数据集的42万个LiDAR扫描的标注版本，使用GPT-4和2D多模态语言模型（MLLMs）进行注释。标注包括一般场景描述、详细的对象及其关系描述以及道路潜在风险的识别。 nu-Groundingnu-Grounding 基于nu-Caption，专注于定位任务，使用nuScenes的注释创建28万对问题和答案，用于视觉定位和定位标注。 ScanReferScanRefer 引入了使用自然语言表达进行3D RGB-D定位的任务，通过在ScanNet 数据集的800个场景中创建51,583个人工注释的“引用表达”，这些表达描述了11,046个对象。输入包括一个扫描的3D场景的点云和一个指定目标对象的自由形式描述，输出是相应对象的边界框。ScanRefer提供了一个评估服务器和在线基准，以便于不同方法之间的比较。 ReferIt3DReferIt3D 引入了一些数据集（Nr3D，Sr3D和Sr3D+），这些数据集包含707个ScanNet场景中的对象。类似于ScanRefer，这些对象用引用表达进行注释，重点是场景中包含多个目标类别实例的查询，引用表达需要在它们之间进行区分。Nr3D包含41,503个人工注释的自由形式表达，以引用3D场景中的对象；Sr3D包含83,572个基于模板的表达；SR3D+是Sr3D的增强表达版本。ReferIt3D还提供了一个评估服务器和在线基准。 Multi3DReferMulti3DRefer 是ScanRefer数据集的修改版本。与始终引用场景中一个对象的引用表达不同，Multi3DRefer包含零目标的6,688个描述，单目标的42,060个描述和多目标的13,178个描述，这些描述针对800个ScanNet场景中的11,609个对象。ChatGPT 用于重新措辞引用表达。 Chat-3D v2Chat-3D v2 是ScanRefer的另一个修改版本，使用ScanNet中的705个场景的引用表达构建描述场景中对象关系的场景标注。这些场景标注通过向GPT-4提供关于对象的真实信息生成。生成的标注包含对直接表示场景中每个对象的“对象标识符”的引用。 EmbodiedScanEmbodiedScan 是Matterport3D、3RScan 和ScanNet 的注释组合，设计为用于3D场景理解的多模态、自我中心的数据集。使用Segment Anything 和其他注释工具提供3D边界框、语义占用和970k基于模板的语言描述，涵盖总计5185个场景。 ScanEnts3DScanEnts3D 扩展了ScanRefer 和ReferIt3D，通过专业注释员将引用句子中提到的每个对象链接到其在3D场景中的实例。在原始论文中，这个数据集仅用于训练目的，发现可以提高模型在其他视觉定位和标注数据集上的性能。 WildReferWildRefer 提出了STRefer和LifeRefer数据集，强调了野外环境中以人为中心的设置，提供了全面的3D和语言人类注释，用于3D定位。STRefer包含来自STCrowd数据集的662个场景的5,458个引用表达，而LifeRefer包含来自新的3D扫描集的3,172个场景的25,380个引用表达。 RIOReferRIORefer 是3RScan数据集的人工注释版本，用于3D定位。它包含1,380个场景的63k对象描述。这个数据集作为测试模型跨数据集泛化能力的一种方式引入，例如提出的“ScanRefer到RIORefer泛化”和“RIORefer到ScanRefer泛化”任务。 ARKitSceneReferARKitSceneRefer 是ARKitScenes数据集的注释版本，强调了现实世界室内环境中小型日常物品的3D定位。它包含1,605个场景中的15k对象描述。 ScanERUScanERU 是ScanNet的修改和人工注释版本，结合了ScanRefer的46k引用表达和包含一个3D人体模型指向引用对象的706个ScanNet场景，位置由人工注释员指定。 DenseGroundingDenseGrounding 类似于Multi3DRefer，旨在扩展3D定位任务以包含多个对象，但每个输入是一个结合多个引用查询的段落，每个查询对应一个对象。这些段落通过从ScanRefer和ReferIt3D的随机对象的最近邻居构建，并结合其引用表达形成段落。 ScanQAScanQA（Azuma等） 是ScanNet的注释版本，生成了800个场景中的41k问题-答案对。问题通过使用ScanRefer中的引用表达自动生成，并由人工注释员精炼，而答案完全由人工注释员提供。这个数据集通常被称为“ScanQA”。 ScanQA (另一个)ScanQA（Ye等） 与ScanQA（Azuma等）同时发布，也是ScanNet的人工注释版本，用作3D问答数据集。Ye等包含806个ScanNet场景的10k问题-答案对。Azuma等最初使用ScanRefer的引用表达生成问题，而Ye等完全依赖人工注释员进行问题创建。 3DMV-VQA3DMV-VQA 是Habitat-Matterport 3D数据集（HM3D）中的5k场景的注释版本，使用HM3DSem中的语义信息生成50k问题，分为“概念”、“计数”、“关系”和“比较”四种类型。这些问题以模板生成，然后转化为自然语言问题。 NuScenes-QANuScenes-QA 由nuScenes数据集的34k场景组成，标注了460k模板样式的问题-答案对，通过构建的场景图生成。问题分为5种类型：“存在”、“计数”、“查询对象”、“查询状态”和“比较”，并可能包含空间推理。 CLEVR3DCLEVR3D 是3RScan数据集的注释版本，设计用于室内3D问答任务。使用3DSSG中的场景图注释生成基于模板的问题和答案。最初为1,333个场景生成了44k问题，但他们使用“组合场景操纵”技术随机替换场景图中的对象，人工生成171k问题，涵盖8,771个场景。 SQA-3DSQA-3D使用相同的虚拟场景，生成53k个不同类型的问答对，用于更复杂的推理任务。 3D-LLM3D-LLM (Wallace et al.)是一个3D问答数据集，生成于一个虚拟环境中，包含生成的3D问答对。 ScanScribeScanScribe是ScanNet的一个注释版本，包含41k个场景描述，标注了800个场景中的场景信息，用于描述任务。 M3DBenchM3DBench是一个多模态多任务基准数据集，汇编了多个现有3D视觉语言数据集，用于评估统一的3D视觉语言模型。 GPT4PointGPT4Point包含25k对3D视觉问答对，基于ScanNet数据集生成，使用大语言模型生成问题和答案。 LAMMLAMM包含25k个3D问答对，基于ScanNet和ReferIt3D数据集生成，使用GPT-4生成问题和答案。 机会与挑战(CHALLENGES AND OPPORTUNITIES)〓 ReTURN 〓 尽管在将大规模语言模型（LLMs）与3D数据集成方面取得了一定进展，但在数据表示、计算效率和基准测试方面仍然存在挑战，亟需创新解决方案。 表示选择对3D视觉语言模型的性能有很大影响。目前，点云主要用于表示室内（如网格顶点）和室外（如LiDAR点云）环境，因其简洁性和与神经网络的兼容性。然而，点云难以捕捉准确、丰富的空间模型所需的细节。开发新的3D场景表示方法，更有效地弥合空间信息与语言之间的差距，可能会解锁新的理解和交互水平。通过找到创新的方法，将语言和语义信息编码到3D表示中，如使用精简的语言和语义嵌入，可以帮助弥合这两种模态之间的差距。 3D数据处理和LLMs的计算需求构成了重大挑战。随着3D环境的复杂性和语言模型规模的增加，可扩展性仍然是一个问题。专为适应性和计算效率设计的LLM架构的进步，可能显著扩大其应用范围。 改进基准测试对于全面评估和提升多模态LLMs在3D任务中的能力至关重要。目前的基准测试范围有限，尤其在3D推理方面，阻碍了对空间推理能力和3D决策/交互系统开发的评估。此外，现有的度量标准未能全面捕捉LLMs在3D环境中的能力。制定更精准测量不同3D任务性能的特定任务度量标准是必要的。最后，当前场景理解基准的粒度过于简单，限制了对复杂3D环境理解的洞察。需要更为多样的任务集合。 在使用LLMs进行3D理解时，安全性和伦理问题也必须考虑。LLMs可能产生虚构和不准确的信息，导致在关键3D应用中出现错误决策。此外，LLMs常以不可预测且难以解释的方式失败，可能会继承训练数据中的社会偏见，在现实3D场景预测中不成比例地影响某些群体。因此，在3D情境中使用LLMs时，必须谨慎，采用策略创建更包容的数据集，建立用于偏见检测和校正的稳健评估框架，并制定机制以减少虚构，确保负责任和公平的结果。 结论〓 ReTURN 〓 这篇综述论文全面探讨了大型语言模型（LLMs）与3D数据的整合。系统性地回顾了LLMs在处理、理解和生成3D数据的方法、应用和新兴能力，强调了LLMs在各种3D任务中具有变革潜力。从增强在3D环境中的空间理解和交互，到推动具身人工智能系统（embodied AI systems）能力的进步，LLMs在推进这一领域中显得尤为关键。 主要发现包括识别出LLMs的独特优势，如零样本学习（zero-shot learning）、高级推理和广泛的世界知识，这些对弥合文本信息与空间解释之间的差距至关重要。论文展示了LLMs与3D数据成功整合的广泛任务种类。对其他3D视觉-语言方法的探讨揭示了一个丰富的研究领域，旨在加深我们对3D世界的理解。 此外，综述还强调了数据表示、模型可扩展性和计算效率等重大挑战，提出克服这些障碍对于充分实现LLMs在3D应用中的潜力至关重要。综上所述，这篇综述不仅提供了LLMs在3D任务中应用的当前状态的全面概述，还为未来的研究方向奠定了基础。它呼吁通过合作努力，探索和扩展LLMs在理解和与复杂3D世界互动方面的能力，为空间智能领域的进一步进展铺平道路。","link":"/2024/06/02/LLMs-step-into-the-3D-World/"},{"title":"MinkowskiEngine安装与配置","text":"摘要: 本文是关于一个巨麻烦的库MinkowskiEngine的安装与配置。在当前时间下，许多新版本的库和系统都用不了这个工具，所以作者甚至直接release了一个docker环境。真的巨麻烦！ .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 MinkowskiEngine简介 update information 功能简介 安装部署流程 安装ME之前先确定cuda版本，建议使用cuda11.1，这是多次尝试之后最靠谱的版本！ 安装cuda之前，具有确认gcc版本， 建议使用gcc-7.5.0 使用conda-forge来安装编译gcc-7.5.0 安装完成后， 更新系统路径 现在可以开始安装cuda-11.1了 完成cuda安装后， 记得要source新的cuda环境 务必先安装pytorch，然后再安装依赖 源码安装Minkowski Engine 安装完成后测试是否能正常导入MinkowskiEngine MinkowskiEngine简介〓 ReTURN 〓 项目主页 The Minkowski Engine is an auto-differentiation library for sparse tensors. It supports all standard neural network layers such as convolution, pooling, unpooling, and broadcasting operations for sparse tensors. update information〓 ReTURN 〓 2021-08-11 Docker installation instruction added 2021-08-06 All installation errors with pytorch 1.8 and 1.9 have been resolved. 2021-04-08 Due to recent errors in pytorch 1.8 + CUDA 11, it is recommended to use anaconda for installation. 2020-12-24 v0.5 is now available! The new version provides CUDA accelerations for all coordinate management functions. 功能简介〓 ReTURN 〓 The Minkowski Engine supports various functions that can be built on a sparse tensor. We list a few popular network architectures and applications here. To run the examples, please install the package and run the command in the package root directory. 安装部署流程〓 ReTURN 〓 &nbsp;仅仅按照官方文档里去安装这个库的坑非常多&nbsp;，这个博主总结的比较全面 安装ME之前先确定cuda版本，建议使用cuda11.1，这是多次尝试之后最靠谱的版本！〓 ReTURN 〓 在Nvidia官网找到cuda-11.1的下载命令：Nvidia官网 1wget https://developer.download.nvidia.com/compute/cuda/11.1.0/local_installers/cuda_11.1.0_455.23.05_linux.run 安装cuda之前，具有确认gcc版本， 建议使用gcc-7.5.0〓 ReTURN 〓 gcc/g++的版本不能过高，否则安装ME库的时候也无法编译,官方要求版本&gt;=7.4.0，实际使用7.5.0到8.0.1都没问题，建议不要高于9.5.0。 我尝试通过常规源码包编译的方式安装编译gcc-7.5.0失败。最终通过conda-forge工具解决了这个问题，这也拓展了我的思路，可能很多项目的c++库都用conda-forge或者conda来管理才是最明智的选择。 使用conda-forge来安装编译gcc-7.5.0〓 ReTURN 〓 虽然conda不直接提供特定版本的gcc，但conda-forge有提供跨平台的编译工具链： 12conda install -c conda-forge gcc_linux-64=7.5.0conda install -c conda-forge gxx_linux-64=7.5.0 这种方式在安装时会处理所有的依赖问题，并且方便在conda环境中进行管理。 安装完成后， 更新系统路径〓 ReTURN 〓 单单是conda安装完成是不够的，现在系统默认还是使用的是原来的gcc，必须将上述conda forge安装的gcc路径放在系统路径的前置位置，确保Conda环境中的gcc优先于系统默认的gcc，这样才会默认使用这个gcc。 步骤: 激活环境conda activate your_env_name 查找GCC和G++的实际路径： 12conda list gccconda list gxx 查找GCC的二进制路径 12find $(conda info --base)/envs/your_env_name -name gccfind $(conda info --base)/envs/your_env_name -name g++ 例如，我找到我的新安装的gcc所在的位置是: /ssddata/lijiawei421/env/miniconda38/envs/openscene/libexec/gcc/x86_64-conda-linux-gnu/7.5.0/gcc， 这是一个可执行文件，通过/ssddata/lijiawei421/env/miniconda38/envs/openscene/libexec/gcc/x86_64-conda-linux-gnu/7.5.0/gcc --version命令可以查看到它的版本是7.5.0 刷新系统路径: 1export PATH=/ssddata/lijiawei421/env/miniconda38/envs/openscene/libexec/gcc/x86_64-conda-linux-gnu/7.5.0/:$PATH 现在可以开始安装cuda-11.1了〓 ReTURN 〓 1sh cuda_11.1.0_455.23.05_linux.run 完成cuda安装后， 记得要source新的cuda环境〓 ReTURN 〓 12345678910# CUDAexport PATH=&quot;/ssddata/lijiawei421/env/lib/cuda-11.1/bin:$PATH&quot;export LD_LIBRARY_PATH=&quot;/ssddata/lijiawei421/env/lib/cuda-11.1/lib64:$LD_LIBRARY_PATH&quot;export CUDA_HOME=&quot;/ssddata/lijiawei421/env/lib/cuda-11.1&quot;export CUDA_TOOLKIT_ROOT_DIR=$CUDA_HOMEexport LD_LIBRARY_PATH=&quot;$CUDA_HOME/extras/CUPTI/lib64:$LD_LIBRARY_PATH&quot;export LIBRARY_PATH=$CUDA_HOME/lib64:$LIBRARY_PATHexport LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATHexport CFLAGS=&quot;-I$CUDA_HOME/include $CFLAGS&quot;export LD_LIBRARY_PATH=&quot;$CUDA_HOME/include:$LD_LIBRARY_PATH&quot; 务必先安装pytorch，然后再安装依赖〓 ReTURN 〓 坑点，官方文档让先安装依赖再安装pytorch，这里务必先安装pytorch，然后再安装依赖，经过该博主多次尝试，最合适的pytorch版本就是1.9.0和1.9.1。最新版本的pytorch也会出问题。 1234conda install pytorch=1.9.0 torchvision cudatoolkit=11.1 -c pytorch -c nvidiapip install ninja #官方文档没说，这里依赖还需要安装ninja库conda install openblas-devel -c anaconda #安装依赖 测试安装 1234python&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.cuda.is_available()True 源码安装Minkowski Engine〓 ReTURN 〓 123git clone https://github.com/NVIDIA/MinkowskiEnginecd MinkowskiEnginepython setup.py install --blas_include_dirs=${CONDA_PREFIX}/include --blas=openblas 安装完成后测试是否能正常导入MinkowskiEngine1234python&gt;&gt;&gt; import MinkowskiEngine as ME&gt;&gt;&gt; print(ME.__version__)0.5.4","link":"/2024/06/05/MinkowskiEngine%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"},{"title":"NeRF原论文及教程","text":"摘要: NeRF原论文以及相关解读材料。 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 原文资源 Abstract Introduction Related Work 神经 3D 形状表示 视图合成和基于图像的渲染 Neural Radiance Field Scene Representation Volume Rendering with Radiance Fields Optimizing a Neural Radiance Field Positional encoding Hierarchical volume sampling Implementation details 5.3 实现细节 Results 数据集 比较 讨论 消融研究 Conclusion 参考资料 原文资源〓 ReTURN 〓 arxiv地址 Abstract我们提出了一种方法，通过优化底层连续体积场函数，利用稀疏输入视图集合，在复杂场景的新视图合成方面实现了最先进的结果。我们的算法使用一个全连接（非卷积）的深度网络表示场景，其输入是一个单一的连续5D坐标（空间位置 (x, y, z) 和视角方向 (θ, φ)），输出是该空间位置的体积密度和视角依赖的辐射亮度。 我们通过在相机射线上查询5D坐标并使用经典的体积渲染技术，将输出的颜色和密度投影到图像中来合成视图。由于体积渲染本质上是可微的，因此优化我们表示&nbsp;所需的唯一输入是一组已知相机姿态的图像。&nbsp; 我们描述了如何有效地优化神经辐射场，以渲染具有复杂几何和外观的场景的照片真实感新视图，并展示了在神经渲染和视图合成方面优于以往工作的结果。视图合成结果最好以视频形式观看，因此我们强烈建议读者观看我们的补充视频以获得有说服力的比较结果。 Introduction〓 ReTURN 〓 在这项工作中，我们通过直接优化连续5D场景表示的参数，以最小化渲染一组捕获图像的误差，从而解决了长期存在的视图合成问题。 我们将静态场景表示为一个连续的5D函数，该函数输出空间中每个点 (x, y, z) 在每个方向 (θ, φ) 上发射的辐射亮度，以及每个点的密度，密度类似于差分不透明度，控制通过 (x, y, z) 的光线累积了多少辐射。我们的方法优化了一个没有卷积层的全连接深度神经网络（通常称为多层感知器或 MLP），通过从单个5D坐标 (x, y, z, θ, φ) 回归到单个体积密度和视角依赖的 RGB 颜色来表示该函数。 为了从特定视点渲染这个神经辐射场（NeRF），我们： 通过场景行进相机光线，生成一组采样的3D点； 使用这些点及其对应的2D视角方向作为神经网络的输入，产生一组颜色和密度输出； 使用经典的体积渲染技术将这些颜色和密度累积成2D图像。由于这个过程本质上是可微的，我们可以使用梯度下降来优化该模型，通过最小化每个观察图像与从我们的表示渲染的对应视图之间的误差。 在多个视图上最小化这个误差鼓励网络通过为包含真实底层场景内容的位置分配高体积密度和准确的颜色来预测场景的连贯模型。 我们发现，优化神经辐射场表示复杂场景的基本实现无法收敛到足够高分辨率的表示，并且每条相机光线所需的样本数量效率低下。我们通过位置编码转换输入5D坐标，使 MLP 能够表示更高频率的函数，并提出了一种层次采样程序，以减少充分采样这种高频场景表示所需的查询数量来解决这些问题。 我们的方法继承了体积表示的优点：两者都可以表示复杂的现实世界几何和外观，并且非常适合使用投影图像进行基于梯度的优化。关键是，我们的方法克服了在高分辨率建模复杂场景时离散体素网格的高存储成本。 总之，我们的技术贡献包括： 一种用5D神经辐射场表示具有复杂几何和材料的连续场景的方法，该场景参数化为基本的 MLP 网络； 一种基于经典体积渲染技术的可微渲染过程，我们使用它来从标准 RGB 图像优化这些表示。这包括一种层次采样策略，将 MLP 的容量分配给具有可见场景内容的空间； 一种位置编码，将每个输入5D坐标映射到更高维度空间，使我们能够成功优化神经辐射场以表示高频场景内容。 我们证明了我们的方法在定量和定性上均优于最先进的视图合成方法，包括拟合神经3D表示的工作和训练深度卷积网络以预测采样体积表示的工作。据我们所知，这篇论文首次提出了能够从自然环境中捕获的 RGB 图像渲染高分辨率照片真实感新视图的连续神经场景表示。 Related Work〓 ReTURN 〓 计算机视觉领域的一个有前途的新方向是将物体和场景编码到一个 MLP 的权重中，直接从 3D 空间位置映射到形状的隐式表示，例如该位置的签名距离。然而，这些方法迄今为止还无法以与使用离散表示（如三角网格或体素网格）相同的保真度再现具有复杂几何形状的真实场景。 在本节中，我们回顾了这两类工作，并将它们与我们的方法进行对比。我们的方法增强了神经场景表示的能力，以产生复杂真实场景渲染的最先进结果。 使用 MLP 从低维坐标映射到颜色的类似方法也被用于表示其他图形函数，如图像、纹理材料和间接照明值。 神经 3D 形状表示近期工作通过优化将 xyz 坐标映射到签名距离函数或占用字段的深度网络，研究了连续 3D 形状的隐式表示。然而，这些模型受到需要访问地面真相 3D 几何的限制，通常从合成 3D 形状数据集（如 ShapeNet）获取。 后续工作通过制定可微渲染函数放宽了对地面真相 3D 形状的要求，从而允许仅使用 2D 图像优化神经隐式形状表示。Niemeyer 等人将表面表示为 3D 占用字段，并使用数值方法找到每条光线的表面交点，然后使用隐式微分计算精确导数。Sitzmann 等人使用一种较不直接的神经 3D 表示，仅在每个连续的 3D 坐标上输出特征向量和 RGB 颜色，并提出了一种可微渲染函数，该函数由一个沿每条光线行进的循环神经网络组成，以确定表面位置。 虽然这些技术可以潜在地表示复杂和高分辨率的几何，但它们迄今为止仅限于具有低几何复杂度的简单形状，导致渲染结果过于平滑。我们展示了另一种策略，即优化网络以编码 5D 辐射场（具有 2D 视角依赖外观的 3D 体积），可以表示高分辨率的几何和外观，以渲染复杂场景的照片真实感新视图。 视图合成和基于图像的渲染在视图密集采样的情况下，可以通过简单的光场采样插值技术重建照片真实感的新视图。对于视图稀疏采样的小说视图合成，计算机视觉和图形学社区通过从观察图像中预测传统几何和外观表示，取得了显著进展。一类流行的方法使用基于网格的场景表示，具有漫反射或视角依赖的外观。可微光栅化器或路径追踪器可以直接优化网格表示，以使用梯度下降重现一组输入图像。然而，基于图像重投影的梯度优化网格通常很困难，可能是因为局部极小值或损失景观的差调。此外，这种策略需要在优化之前提供具有固定拓扑的模板网格初始化，这对于不受限制的真实世界场景通常是不可用的。 另一类方法使用体积表示来从一组输入 RGB 图像中生成高质量的照片真实感视图合成。体积方法能够逼真地表示复杂形状和材料，非常适合基于梯度的优化，并且比基于网格的方法产生的视觉干扰伪影更少。早期的体积方法使用观察到的图像直接为体素网格着色。最近，一些方法使用多个场景的大型数据集训练深度网络，从一组输入图像预测采样体积表示，然后在测试时使用 alpha 混合或沿光线的学习混合来渲染新视图。其他工作优化了每个特定场景的卷积网络（CNN）和采样体素网格的组合，以便 CNN 可以补偿低分辨率体素网格的离散化伪影，或者允许预测的体素网格根据输入时间或动画控制变化。 虽然这些体积技术在新视图合成方面取得了令人印象深刻的结果，但由于其离散采样导致的时间和空间复杂性差，它们在高分辨率图像的扩展能力受到根本限制。我们通过在深度全连接神经网络的参数中编码一个连续体积来规避这个问题，这不仅比以前的体积方法产生显著更高质量的渲染，而且只需要这些采样体积表示的一小部分存储成本。 Neural Radiance Field Scene Representation〓 ReTURN 〓 我们将连续场景表示为一个5D向量值函数，其输入是3D位置 $x = (x, y, z)$ 和2D视角方向 $(θ, φ)$，输出是发射颜色 $c = (r, g, b)$ 和体积密度 $\\sigma$。在实际操作中，我们将方向表示为3D笛卡尔单位向量 $d$。我们用一个MLP网络 $F_Θ : (x, d) → (c, σ)$ 来近似这种连续的5D场景表示，并优化其权重 $\\Theta$ 以将每个输入的5D坐标映射到其对应的体积密度和方向发射颜色。 我们通过限制网络预测体积密度 $\\sigma$ 仅作为位置 $x$ 的函数，同时允许RGB颜色 $c$ 作为位置和视角方向的函数来鼓励表示多视图一致性。为实现这一点，MLP $F_Θ$ 首先用8个全连接层（使用ReLU激活函数和每层256个通道）处理输入的3D坐标 $x$，并输出 $\\sigma$ 和一个256维的特征向量。然后将该特征向量与相机光线的视角方向连接起来，并传递给一个额外的全连接层（使用ReLU激活函数和128个通道），输出视角依赖的RGB颜色。 请参阅图3，了解我们的方法如何使用输入的视角方向来表示非朗伯效应。如图4所示，训练中没有视角依赖（仅以 $x$ 为输入）的模型在表示镜面反射方面存在困难。 Volume Rendering with Radiance Fields〓 ReTURN 〓 我们的5D神经辐射场表示一个场景中的任何空间点的体积密度和方向发射辐射。我们使用经典体积渲染的原理来渲染穿过场景的光线颜色。体积密度 $\\sigma(\\mathbf{x})$ 可以解释为光线在位置 $\\mathbf{x}$ 处终止于一个微小粒子的微分概率。相机光线 $\\mathbf{r}(t)=\\mathbf{o}+t \\mathbf{d}$ 的预期颜色 $C(\\mathbf{r})$ 在近处和远处边界 $t_n$ 和 $t_f$ 之间为： $$C(\\mathbf{r})=\\int_{t_n}^{t_f} T(t) \\sigma(\\mathbf{r}(t)) \\mathbf{c}(\\mathbf{r}(t), \\mathbf{d}) d t$$其中$$T(t)=\\exp \\left(-\\int_{t_n}^t \\sigma(\\mathbf{r}(s)) d s\\right)$$ 函数 $T(t)$ 表示沿光线从 $t_n$ 到 $t$ 的累积透射率，即光线从 $t_n$ 到 $t$ 而不碰到任何其他粒子的概率。从我们的连续神经辐射场渲染一个视图需要估计穿过所需虚拟相机的每个像素的相机光线的这个积分 $C(\\mathbf{r})$。 我们使用求积法数值估计这个连续积分。确定性求积法通常用于渲染离散体素网格，但这会有效地限制我们的表示的分辨率，因为 MLP 只能在固定的离散位置上查询。相反，我们使用分层采样方法，将 $[t_n, t_f]$ 分成 $N$ 个等间隔的区间，然后从每个区间内随机均匀地抽取一个样本： $$t_i \\sim \\mathcal{U}\\left[t_n+\\frac{i-1}{N}\\left(t_f-t_n\\right), t_n+\\frac{i}{N}\\left(t_f-t_n\\right)\\right]$$ 虽然我们使用离散样本集来估计积分，但分层采样使我们能够表示一个连续的场景表示，因为它使 MLP 在优化过程中在连续位置上进行评估。我们使用这些样本按照 Max 的体积渲染综述中讨论的求积规则来估计 $C(\\mathbf{r})$： $$\\hat{C}(\\mathbf{r})=\\sum_{i=1}^N T_i\\left(1-\\exp \\left(-\\sigma_i \\delta_i\\right)\\right) \\mathbf{c}_i$$ 其中 $$T_i=\\exp \\left(-\\sum_{j=1}^{i-1} \\sigma_j \\delta_j\\right)$$。 再其中 $\\delta_i=t_{i+1}-t_i$ 是相邻样本之间的距离。这个函数从一组 $(\\mathbf{c}_i, \\sigma_i)$ 值计算 $\\hat{C}(\\mathbf{r})$ 是可微的，并简化为传统的 alpha 混合，alpha 值为 $\\alpha_i=1-\\exp \\left(-\\sigma_i \\delta_i\\right)$。 Optimizing a Neural Radiance Field〓 ReTURN 〓 在前一节中，我们描述了将场景建模为神经辐射场并从该表示中渲染新视图的核心组件。然而，我们观察到这些组件不足以实现最先进的质量，如第6.4节所示。我们引入了两个改进，以便表示高分辨率复杂场景。首先是输入坐标的位置编码，这有助于MLP表示高频函数；其次是分层采样程序，允许我们高效地采样这种高频表示。 Positional encoding尽管神经网络是通用函数逼近器，但我们发现让网络 $F_{\\Theta}$ 直接操作 $x, y, z, \\theta, \\phi$ 输入坐标，在表示颜色和几何的高频变化方面表现不佳。这与最近Rahaman等人的研究一致，该研究表明深度网络倾向于学习低频函数。他们进一步表明，在将输入映射到更高维空间后再传递给网络，可以更好地拟合包含高频变化的数据。 我们在神经场景表示的上下文中利用这些发现，并表明将 $F_{\\Theta}$ 重新表述为两个函数的组合 $F_{\\Theta}=F_{\\Theta}^{\\prime} \\circ \\gamma$ ，一个是学习的，一个不是，可以显著提高性能。这里 $\\gamma$ 是一个从 $\\mathbb{R}$ 到更高维空间 $\\mathbb{R}^{2L}$ 的映射， $F_{\\Theta}^{\\prime}$ 仍然是一个普通的MLP。我们使用的编码函数形式为： $$\\gamma(p)=\\left(\\sin \\left(2^0 \\pi p\\right), \\cos \\left(2^0 \\pi p\\right), \\cdots, \\sin \\left(2^{L-1} \\pi p\\right), \\cos \\left(2^{L-1} \\pi p\\right)\\right).$$ 该函数 $\\gamma(\\cdot)$ 分别应用于 $\\mathbf{x}$ 的三个坐标值（归一化到 $[-1, 1]$）和笛卡尔视角方向单位向量 $\\mathbf{d}$ 的三个分量（归一化到 $[-1, 1]$）。在我们的实验中，我们设置 $\\gamma(\\mathbf{x})$ 的 $L=10$ 和 $\\gamma(\\mathbf{d})$ 的 $L=4$。 类似的映射也被应用于流行的Transformer架构中，但目的是为序列中的离散位置提供输入，以便于没有顺序概念的架构中使用。相反，我们使用这些函数将连续输入坐标映射到更高维空间，以便我们的MLP更容易逼近高频函数。与研究3D蛋白质结构建模相关的并行工作也利用了类似的输入坐标映射。 Hierarchical volume sampling我们通过在每条相机光线上评估神经辐射场网络的策略是低效的：自由空间和遮挡区域对渲染图像没有贡献，但仍然被反复采样。我们从早期的体积渲染工作中获得灵感，提出了一种分层表示，通过根据样本对最终渲染的预期影响分配样本来提高渲染效率。 我们同时优化两个网络：一个“粗略的”，一个“精细的”。首先，使用分层采样法采样一组 $N_c$ 位置，并在这些位置评估“粗略的”网络。根据“粗略的”网络的输出，我们沿着每条光线生成一个更有信息量的采样点，这些样本偏向于体积的相关部分。为此，我们首先将“粗略的”网络的alpha混合颜色 $\\hat{C}_c(r)$ 重写为沿光线的所有采样颜色 $c_i$ 的加权和： $$\\hat{C}c(r)=\\sum{i=1}^{N_c} w_i c_i$$, $$w_i=T_i\\left(1-\\exp \\left(-\\sigma_i \\delta_i\\right)\\right)$$ 将这些权重归一化为$\\hat{w}i=\\frac{w_i}{ \\sum{j=1}^{N_c}w_j}$，生成沿光线的分段常数PDF。我们使用逆变换采样法从该分布中采样第二组 $N_f$ 位置，并在第一组和第二组样本的联合上评估我们的“精细的”网络，使用所有 $N_c + N_f$ 样本计算最终渲染颜色$\\hat{C}_f(r)$。这个过程为我们预期包含可见内容的区域分配了更多样本。这个目标与重要性采样相似，但我们将采样值视为整个积分域的非均匀离散化，而不是将每个样本视为整个积分的独立概率估计。 Implementation details我们为每个场景优化一个独立的神经连续体积表示网络。这仅需要场景的RGB图像数据集、相应的相机姿态和内在参数以及场景边界（对于合成数据，我们使用真实的相机姿态、内在参数和边界；对于真实数据，我们使用COLMAP结构光恢复包来估算这些参数）。 在每次优化迭代中，我们从数据集中所有像素的集合中随机采样一批相机光线，然后按照第5.2节所述的分层采样，从粗网络中查询 $N_c$ 个样本，从精网络中查询 $N_c+N_f$ 个样本。接着，我们使用第4节描述的体积渲染程序，从这两组样本中渲染每条光线的颜色。我们的损失函数是渲染颜色和真实像素颜色之间的总平方误差： $$\\mathcal{L}=\\sum_{\\mathbf{r} \\in \\mathcal{R}}\\left[\\left|\\hat{C}_c(\\mathbf{r})-C(\\mathbf{r})\\right|_2^2+\\left|\\hat{C}_f(\\mathbf{r})-C(\\mathbf{r})\\right|_2^2\\right]$$ 其中 $\\mathcal{R}$ 是每批次的光线集合， $C(\\mathbf{r}), \\hat{C}_c(\\mathbf{r})$ 和 $\\hat{C}_f(\\mathbf{r})$ 分别是光线 $\\mathbf{r}$ 的真实RGB颜色、粗体积预测颜色和精体积预测颜色。尽管最终渲染来自 $\\hat{C}_f(\\mathbf{r})$，我们也最小化 $\\hat{C}_c(\\mathbf{r})$ 的损失，以便粗网络的权重分布可以用于精网络的样本分配。 在我们的实验中，我们使用4096条光线的批次大小，每条光线在粗体积中采样64个坐标，在精体积中额外采样128个坐标。我们使用Adam优化器，学习率初始为 $5 \\times 10^{-4}$ 并在优化过程中指数衰减到 $5 \\times 10^{-5}$（其他Adam超参数保持默认值 $\\beta_1=0.9, \\beta_2=0.999$ 和 $\\epsilon=10^{-7}$）。单个场景的优化通常在一张NVIDIA V100 GPU上需要大约100到300千次迭代（约1-2天）才能收敛。 Results〓 ReTURN 〓 数据集合成物体渲染 我们首先在两个合成物体渲染数据集上展示实验结果（表1，“Diffuse Synthetic $360^{\\circ}$” 和 “Realistic Synthetic $360^{\\circ}$”）。DeepVoxels 数据集包含四个具有简单几何形状的朗伯对象。每个对象从上半球的视点渲染，分辨率为512x512像素（479个作为输入，1000个用于测试）。我们还生成了包含八个对象的路径追踪图像的数据集，这些对象展示了复杂的几何形状和现实的非朗伯材料。六个对象从上半球的视点渲染，两个对象从全球的视点渲染。每个场景渲染100个视图作为输入，200个视图用于测试，分辨率为800x800像素。 复杂场景的真实图像 我们展示了用大致前向图像捕获的复杂真实世界场景的结果（表1，“Real ForwardFacing”）。该数据集包含用手持手机捕获的8个场景（5个来自LLFF论文，3个由我们捕获），捕获了20到62张图像，并保留1/8的图像用于测试集。所有图像的分辨率为1008x756像素。 比较为了评估我们的模型，我们与当前性能最好的视图合成技术进行了比较。所有方法使用相同的输入视图集来为每个场景训练一个单独的网络，除了Local Light Field Fusion (LLFF)，它在一个大型数据集上训练单个3D卷积网络，然后使用相同的训练网络处理新场景的输入图像。 Neural Volumes (NV) 合成在一个有界体积内并位于独特背景前的对象的新视图。它优化一个深度3D卷积网络来预测包含128^3个样本的离散化RGB alpha体素网格，以及包含32^3个样本的3D变形网格。算法通过在变形体素网格中行进相机光线来渲染新视图。 Scene Representation Networks (SRN) 将连续场景表示为不透明表面，由MLP隐式定义，该MLP将每个 (x, y, z) 坐标映射到一个特征向量。他们训练一个循环神经网络通过使用任何3D坐标处的特征向量来预测沿光线的下一个步长，从而在场景表示中行进光线。最终步长的特征向量被解码为该表面点的单一颜色。SRN是同一作者在DeepVoxels基础上的更好表现的后续研究，因此我们没有包括DeepVoxels的比较。 Local Light Field Fusion (LLFF) 设计用于为采样良好的前向场景生成照片真实感的新视图。它使用训练好的3D卷积网络直接预测每个输入视图的离散化棱锥采样RGB alpha网格（多平面图像或MPI），然后通过alpha混合和混合附近的MPI来渲染新视图。 讨论我们在所有场景中全面超越了NV和SRN基线。此外，我们生成的渲染在定量和定性上优于LLFF（除了一个指标），而仅使用它们的输入图像作为我们的整个训练集。SRN方法生成了高度平滑的几何形状和纹理，其视图合成表示能力受限于每条相机光线仅选择单一深度和颜色。NV基线能够捕获合理详细的体积几何和外观，但其使用的128^3体素网格限制了它在高分辨率下表示细节的能力。LLFF提供了一个“采样指南”，建议输入视图之间的视差不超过64像素，因此在合成数据集中包含400-500像素视差时，常常无法估计正确的几何。此外，LLFF在渲染不同视图时在不同场景表示之间进行混合，导致感知上分散的不一致性。 这些方法之间最大的实际权衡是时间与空间。所有比较的单场景方法每个场景的训练时间至少为12小时。相比之下，LLFF可以在10分钟内处理一个小输入数据集。然而，LLFF为每个输入图像生成一个大的3D体素网格，导致巨大的存储需求（一个“Realistic Synthetic”场景超过15GB）。我们的方法只需要5MB的网络权重（相对于LLFF的3000倍压缩），甚至比我们任何数据集中单个场景的输入图像本身所需的内存还少。 消融研究我们在表2中通过广泛的消融研究验证了算法的设计选择和参数。我们在“Realistic Synthetic $360^{\\circ}$”场景上展示结果。第9行显示了作为参考点的完整模型。第1行显示了没有位置编码（PE）、视角依赖（VD）或分层采样（H）的最小化版本模型。在第2-4行中，我们从完整模型中逐一移除这三个组件，观察到位置编码（第2行）和视角依赖（第3行）提供了最大的定量益处，其次是分层采样（第4行）。第5-6行显示了输入图像数量减少时我们的性能下降。请注意，我们的方法使用仅25个输入图像时的性能仍然超过了NV、SRN和LLFF在提供100个图像时的所有指标（见补充材料）。在第7-8行中，我们验证了用于 $\\mathbf{x}$ 的位置编码的最大频率 $L$ 的选择（用于 $\\mathbf{d}$ 的最大频率按比例缩放）。仅使用5个频率会降低性能，但将频率从10增加到15并不能提高性能。我们认为，随着 $2^L$ 超过采样输入图像中的最大频率（大约1024），增加 $L$ 的益处有限。 Conclusion〓 ReTURN 〓 我们的工作直接解决了使用MLPs（多层感知器）将物体和场景表示为连续函数的前人工作的不足。我们证明了将场景表示为5D神经辐射场（一个以3D位置和2D视角为函数输出体积密度和视角相关发射辐射的MLP）比之前主流的训练深度卷积网络输出离散体素表示的方法产生更好的渲染效果。 尽管我们提出了分层采样策略以提高渲染的采样效率（适用于训练和测试），但在高效优化和渲染神经辐射场的技术研究方面仍有很大进展空间。未来的另一个研究方向是可解释性：如体素网格和网格这样的采样表示允许对渲染视图的预期质量和失败模式进行推理，但当我们将场景编码在深度神经网络的权重中时，如何分析这些问题尚不明确。 我们相信，这项工作在基于真实世界图像的图形管道方面取得了进展，其中复杂场景可以由从实际物体和场景图像中优化的神经辐射场组成。 参考资料〓 ReTURN 〓 https://www.bilibili.com/video/BV1CC411V7oq/?spm_id_from=333.337.search-card.all.click&amp;vd_source=79dd75bdbe89b6c71e9e0bb394ba77d4","link":"/2024/06/11/NeRF%E5%8E%9F%E8%AE%BA%E6%96%87%E5%8F%8A%E6%95%99%E7%A8%8B/"},{"title":"Scalability and Performance of LiDAR Point Cloud Data Management Systems A State-of-the-Art Review","text":"摘要: 本文探讨了LiDAR点云数据管理（PCDM）系统的可扩展性和性能。点云数据由于其体积大且异质性强，亟需高效的管理解决方案。现有研究主要集中于在不同的并行架构和数据模型上开发PCDM系统，已取得一定的成果。特别是共享内存架构和共享磁盘架构下的PCDM系统表现出色。然而，关于共享无架构和宽列NoSQL数据库在PCDM中的应用研究仍不足。本文还指出，现有的PCDM系统在扩展性和性能评估方面存在显著的研究空白。针对未来研究，本文建议在三方面进行深入探索：点云数据量的变化、流量变化及其他系统需求。同时，强调了开发一个可扩展、灵活的框架以系统化地测试和比较不同PCDM系统的重要性。 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 Introduction RelatedWork Background Scalability and Performance of Data-Intensive Systems Scaling Techniques and Parallel Architectures in Data-Intensive Systems Advent of Parallel Data-Intensive Systems Architectures of Data-Intensive Systems (Parallel Data System Architectures) Use of Databases in Data-Intensive Systems Parallel Architectures toward Scalability and Performance Data Models toward Scalability and Performance Analysis of State-of-the-Art PCDM Systems Shared-Memory Based PCDM Systems Shared-Disk Oriented PCDM Systems Shared-Nothing Architecture-Oriented PCDM Systems Database-Oriented Shared-Nothing Architecture-Based PCDM Comparison of Scalability and Performance of Current PCDM Systems Inspect Performance Results of Current PCDM Systems Discussion Guide to Selecting Parallel Architectures and Data Models for PCDM Further Research Avenues 现有PCDM系统： 开发新PCDM系统： Conclusions Introduction〓 ReTURN 〓 随着LiDAR点云数据的日益普及，越来越多的研究致力于开发高效的点云数据管理（PCDM）系统。尽管其中一些系统采用基于文件的方法来存储和查询点云数据，但相当多的系统致力于开发依赖数据库技术的PCDM系统。PCDM研究的核心在于应对大量异构点云数据。因此，依赖数据库技术的PCDM解决方案主要集中于实现更大的可扩展性，同时保持可接受的性能水平。 “可扩展性”(Scalability)是一个相对的术语。在本文中，可扩展性是指开发高度可扩展的数据密集型系统，换句话说，是从数据管理的角度来讨论。因此，可扩展性主要关注的是如何在快速增长的数据需求和查询（即流量）需求下，确保高效的存储和数据检索时间。这一定义来源于Kleppmann（2017）。 更具体地说，点云数据的可扩展管理正在设计和开发数据密集型系统方面进行探索。换句话说，PCDM系统正在被开发，以适应不同负载参数的增长，同时确保相应的性能。这些负载参数包括点云数据量的快速增长、单用户和多用户场景中从数据库中检索的点数量的快速增长，以及在必要时高卷的点云数据摄取，用于测量性能指标（例如，管理10亿点时检索100万点所需的响应时间）。 RelatedWork〓 ReTURN 〓 尽管在科学文献中已经全面介绍了与PCDM相关的多种方法，但许多优秀的过往调查并未完全涵盖这一快速发展的领域中的最新和新兴趋势。因此，本文综合了现有调查、原创研究论文、书籍章节和有关PCDM的学位论文的贡献。这主要可以分为两个领域：(i) 基于文件的PCDM研究，和(ii) 依赖数据库技术的PCDM研究。然后，本文讨论了未来系统选择的问题，并进一步识别出需要解决的主要研究问题。 2.1 基于文件的PCDM Otepka等人的工作是PCDM背景下的早期调查论文之一。这项调查将地理参考的点云数据模型定义为三维笛卡尔空间中与地理空间参考系统（例如，全球横轴墨卡托投影）相关的一组点 $\\mathrm{P} i, i=1, \\ldots, n$。点云特征也被描述为两类：(i) 基本特征，和(ii) 派生特征。基本特征是指在点云测量/调查过程中捕获的特征（例如，$x-, y$-和$z$-坐标）。而派生特征是在点云处理阶段生成的特征（例如，分类）。作者强调了保留原始点云的重要性，而不是从插值数据生成的表面模型（这是十年前的常见做法）。为了保持原始格式，该调查讨论了几种用于保留原始点的先进空间索引，包括kD树、八叉树和R树。该调查还强调了在点云文件中组织几何属性（即坐标）和其他属性（例如强度或颜色）的不同技术，用于管理点云数据。还讨论了与点云文件中各个属性组织有关的点云数据管理问题。然而，这些内容并未涵盖面向数据库的PCDM。 Graham的后续工作概述了LiDAR点云数据的结构以及通过不同编码格式（例如二进制、ASCII和基于标记语言的格式（例如XML））进行点记录的利用和传输。作者还详细描述了美国摄影测量与遥感学会（ASPRS）的LAS数据格式。LAS格式是LiDAR数据的主要共享格式。Graham的描述包括LAS 1.4规范的主要亮点、LAS数据格式与LiDAR数据处理的关系，以及不同点属性和LiDAR项目属性如何在LAS格式中反映出来。最重要的是，Graham强烈支持基于文件的PCDM系统，而不是数据库解决方案，并质疑采用数据库解决方案。作者认为，数据库解决方案只有在需要随机访问LiDAR数据时才具有吸引力，并进一步认为PCDM应该基于文件存储，以支持高吞吐量生产操作，假设采用数据库进行PCDM可能带来的弊大于利。 2.2 依赖于数据库技术的PCDM 大多数依赖数据库技术的PCDM研究，包括[3,5,19-24]，讨论了基于文件的PCDM的局限性。这些局限性包括在查询大量LiDAR点时的不可靠性、不足的临时查询支持、较差的水平和垂直可扩展性，以及缺乏数据集成和数据共享的支持。此外，上述论文还详细讨论了采用数据库解决方案进行LiDAR数据管理的好处：(i) 通过数据隔离实现应用独立性，(ii) 有效支持数据的并发访问，(iii) 高可扩展性（例如采用分布式数据库），(iv) I/O优化，(v) 集成其他点云数据和影像数据，(vi) 通过标准化的声明式查询语言进行数据检索的可能性，(vii) 易于管理和建立安全性，(viii) 设计用于点云数据的特殊形式的空间数据类型，(ix) 通过多层次的细节级点云数据表示改善可视化。 点云数据在数据库环境中的组织的领先研究工作之一是[19]中提出的，该工作提出了多种PCDM解决方案的全面评审和基准测试。基准测试包括两个关系数据库管理系统（DBMS）（即Oracle和PostgreSQL/PostGIS）、一个列式数据存储（即MonetDB）和一种使用LAStools的基于文件的方法。此外，作者还研究了通过使用Oracle Exadata数据库机器（OEDBM）采用高性能并行数据库进行PCDM的可行性。每个PCDM的数据加载时间、数据存储和查询响应时间在三个基准（小型、中型和大型）下使用AHN2数据集进行测量和呈现。使用三种不同的数据集大小提供了一种直接且具体的手段来评估可扩展性。可扩展性和性能也可以从理论上考虑（详细信息参见第4和第5节）。然而，在实验场景中，通常通过获得特定工作负载场景的性能结果来确定系统的可扩展性。 在van Oosterom等人的工作中，作者主要研究了不同数据负载量下的可扩展性。他们的分析包括在Oracle和PostgreSQL中使用块模型、点云数据组织（其中一组点作为一个块存储在一起）以及在Oracle、PostgreSQL和MonetDB中使用平面表模型点云组织（其中每个点存储在一个表行中）的比较。作者还解释了在PostgreSQL和Oracle数据库中使用PC_PATCH数据类型和SDO_PC和SDO_PC_BLK数据类型进行点数据组织的情况。 Vo等人的工作提供了这些Oracle和PostgreSQL内置数据类型的更深入评审。在该工作中，详细介绍了航空LiDAR的起源，重点是其在规模、分辨率和受欢迎程度上的增长。此外，作者还详细讨论了在文件和数据库环境中对航空LiDAR数据建模以及在航空LiDAR数据中的空间索引应用，以及LiDAR点记录如何在基于瓦片的索引、层次索引和集成多索引结构中进行分区和组织。最后，Vo等人指出了数据库支持LiDAR全波形数据管理的缺乏。 此外，分布式非关系数据库解决方案在PCDM中的重要性越来越频繁地出现[3,22,27-30]。虽然在[3,24]中阐明了采用分布式解决方案和非关系数据库的好处（即避免单点故障和更好的可扩展性），但迄今为止，已发表的工作并未审查当前PCDM系统的可扩展性和性能。&nbsp;特别是，是什么使PCDM系统具有可扩展性以及如何实现这一点，是缺乏严格研究的主题。&nbsp;本节中强调的最重要点总结在表1中。 表1展示了对PCDM系统可扩展性和性能的广泛调查的缺乏。第4-6节详细讨论了这一差距。在本综述中，通过涵盖并行架构和数据模型两个基本领域，探索了当前最先进的PCDM系统，这两个领域有助于使数据系统具有可扩展性和高效性。 Background〓 ReTURN 〓 Scalability and Performance of Data-Intensive Systems在当今的数据密集型应用程序开发中，实现更高的可扩展性而不损害相应的性能是一个关键目标。&nbsp;对于数据密集型应用，可扩展性被定义为系统应对数据量、流量和复杂性增加的能力。&nbsp;在设计数据密集型应用程序时，首先需要确定和描述上述负载因素及其随时间的预期增长。通过一系列负载实验来评估可扩展性。虽然性能方面通常因领域和应用领域而异，但常见的性能指标涉及存储、查询和数据加载。存储性能通常通过存储数据所需的字节/千字节来衡量。数据加载性能通过吞吐量进行评估。查询性能通常通过吞吐量或响应时间来衡量。在数据密集型应用中，吞吐量通常定义为单位时间内处理的记录数或执行的工作量。例如，在PCDM环境中，数据加载通常以每秒或千点数数据每秒的速度进行测量。响应时间被定义为查询经过的时间。在PCDM中，查询响应时间通常以毫秒、秒或分钟为单位进行衡量。在提供数据的并发访问时，根据目标，数据密集型系统会在响应时间和吞吐量之间进行权衡。目前在PCDM中，不将查询性能评估为吞吐量。 可扩展性和性能是数据密集型系统（例如PCDM）开发中的两个重要维度。可扩展性和性能是相互关联的。可扩展性差通常导致性能不佳，这意味着在不可扩展的系统中，性能通常会随需求（即负载）而降低。此外，可扩展性为获得更好的性能提供了选择。虽然希望同时具备更高水平的可扩展性和性能，但数据密集型系统通常会在性能和可扩展性之间进行权衡。因此，最先进的数据密集型系统通常旨在吸收更高的负载，同时实现可接受的性能。 Scaling Techniques and Parallel Architectures in Data-Intensive SystemsAdvent of Parallel Data-Intensive Systems传统上，计算机系统遵循“经典”的冯·诺依曼体系结构，因此采用单核、单处理器架构。通常，术语“核心”用于表示单个计算单元。因此，历史上，计算机系统每个都只有一个处理器，在该处理器中只有一个计算单元。如今，这些计算单元也被引入为核心。此外，这些核心在上下文中与中央处理单元（CPU）同义（即，一个核心 = CPU）。随着芯片制造商推出了具有多个计算单元的处理器，多核处理器成为计算机系统中的主流元素。与此同时，诸如线程处理等技术的出现使处理器核心能够同时被利用。因此，整个计算机系统中可用的逻辑计算单元（即，逻辑CPU）增加，从而实现并行计算。 为了实现更高的可扩展性和相应的性能，支持大规模数据管理的最先进计算机系统（即数据密集型系统）通过网络化不同的处理元素进行设计。这些处理元素包括多核处理器、多处理器芯片（单核或多核）、RAMS、磁盘甚至完全自治的处理元素（即节点）。因此，现代数据密集型系统具有更多的可用计算单元（即物理和逻辑CPU）。这使得数据密集型系统中的CPU能够与存储在RAM和/或磁盘中的数据并行交互，并且也能够并行执行与数据管理任务相对应的计算。 Architectures of Data-Intensive Systems (Parallel Data System Architectures)如前所述，当今的数据密集型系统是并行系统。这些最先进的数据密集型系统主要有三种类型：（i）垂直扩展系统（尺度上升系统），（ii）水平扩展系统（尺度外系统），以及（iii）混合采用两者的系统。数据密集型系统可以通过两种方式采用垂直扩展。第一种方法是通过一个快速网络在单个操作系统下连接多个CPU、内存芯片和磁盘。这些系统产生基于共享内存架构的数据密集型系统。第二种方法连接多台计算机，每台计算机都有自己的操作系统、CPU(s)和内存芯片(s)，但使用共享磁盘存储数据，通过快速网络连接。采用这种方法的应用被称为基于共享磁盘架构的数据密集型系统。采用水平扩展的应用被称为基于共享无架构的系统。 Use of Databases in Data-Intensive Systems如今，在数据密集型应用中普遍采用数据库。这些数据库通常部署在共享内存、共享磁盘或共享无架构的并行架构中。它们提供机制来高效地管理系统的底层数据，可以在磁盘上持久保存或者在主存储器中进行管理。与基于文件的应用相比，这些面向数据库的数据密集型应用能够使数据独立于各自的应用（在基于文件的应用中，文件格式与应用紧密耦合）。目前，有许多数据库可供数据密集型系统使用。它们服务于多种目的，因此具有不同的数据模型，并部署在各种并行架构中。数据模型是接收和组织输入数据的格式。常见的数据模型包括关系模型、关系列模型、NoSQL模型（如键值模型、宽列模型、图模型）、数组模型以及混合模型（多模型和NewSQL模型）。遵循关系模型的数据库可以部署在所有三种数据密集型系统架构下。NoSQL模型数据库和混合模型数据库主要部署在共享无架构环境中。虽然数组数据库也可以部署在共享内存架构中，但实际上数组数据库主要用于共享磁盘或共享无架构中。这三种并行架构风格及其在各自架构中的数据库部署都有潜力提供所需的数据管理可扩展性，同时确保相应的性能。然而，在采用数据密集型应用开发（如PCDM）时，理解每种并行架构对可扩展性和性能的影响至关重要。特别是，了解每种并行架构在适应给定的数据密集型任务时的灵活性、技术多样性、可扩展性范围、性能特性和局限性是重要的。这样的理解有助于选择适合当前数据密集型任务的并行架构。同样，还需要理解数据模型对于开发数据密集型应用的影响。因此，第4节和第5节用于讨论架构和数据模型对于PCDM的可扩展性和性能的影响。 Parallel Architectures toward Scalability and Performance〓 ReTURN 〓 现有的共享内存系统和共享磁盘系统有能力应对不断增加的数据量和流量。然而，这些垂直扩展系统的可扩展性始终受限于各自系统的容量。例如，在共享内存系统中，处理和存储元素的现有容量（如可用的CPU、内存单元和磁盘）决定了系统应对增加的数据量和流量的能力。如果需要进一步扩展共享内存系统以应对超出其容量的数据量和流量，则必须迁移到具有更大存储容量、处理能力和内存的高端机器。类似地，在共享磁盘架构中，可扩展性也依赖于系统的具体特性。特别是，为支持不断增加的数据量，可能需要将共享磁盘系统迁移到具有更大磁盘容量的共享磁盘机器。 相比之下，添加更多处理器-内存（或CPU-内存）节点到现有系统可以提高共享磁盘系统的数据处理能力，而无需进行系统迁移。然而，共享磁盘系统中的另一个问题是，数据写入可以在任何节点上执行。这意味着两个或更多节点可能会同时尝试写入一个数据记录（即元组）。因此，为了确保一致性，管理系统必须使用基于磁盘的锁表或通知系统中其他节点锁定该元组的意图。由于这些额外的开销，在共享磁盘架构中添加更多节点并不总能提高大数据量和流量的可扩展性。 相比之下，在基于无共享架构的系统中管理数据时，可以在无需系统迁移的情况下增加存储、处理和内存需求。更具体地说，在无共享架构的系统中，添加完全自主的节点（包含处理器、内存和磁盘）是容易实现的。此外，与共享磁盘架构系统不同，添加节点通常能提高可扩展性。这是因为在无共享架构中，每个节点独立管理其自己的数据。因此，在将数据写入持久存储时，无需在无共享架构中锁定节点。因此，在数据量需求和流量需求不断增加的情况下，无共享架构被认为是一种有效的方法。 值得注意的是，研究表明，在相同的并行度下，基于共享内存架构的系统通常表现最佳，而基于无共享架构的系统表现最差。在所有三种架构中，增加更多节点/核心或更强大的节点/核心通常会提高系统的并行度。因此，可用于数据管理任务的处理器（和CPU）、内存和磁盘单元也会增加。因此，特别是数据加载和查询性能通常会得到改善。虽然增加节点/核心一般会提高性能，但在并行计算数据管理（PCDM）文献中，这种情况仅在基于无共享架构的PCDM工作中有所体现。当前的基于共享内存和共享磁盘架构的PCDM系统并未显示出在现有PCDM系统中添加更多核心/处理器-内存节点时可以实现的性能提升。可能的原因是，一旦系统配置完成，向现有系统添加更多核心/处理器-内存节点是不可行的。 在某些基于无共享架构的PCDM系统中，添加完全自主的节点到现有系统可以改善数据查询时间。在其他基于无共享架构的空间数据管理系统中（如Hadoop-GIS和VegaGIStore），也可以看到查询响应时间和数据加载的性能提升。然而，向无共享架构系统添加节点仅能以次线性方式改善性能。此外，添加节点并不保证性能提升，因为提升仅能在一定数量的节点内实现，超过这一数量后，通信成本将超过性能收益。 在分析最先进的PCDM研究工作时，大多数数据库导向的PCDM研究努力都基于共享内存架构系统。然而，当前的PCDM文献并未提供采用共享内存架构的直接原因。这些架构主要受到实验中使用的数据库的影响。具体而言，这些PCDM系统基于传统关系数据库（如Oracle、PostgreSQL/PostGIS等）。 据我们所知，目前仅存在一个基于共享磁盘架构的PCDM系统，即Oracle Exadata数据库机器PCDM。与基于共享内存架构的PCDM系统类似，仅有一个基于共享磁盘架构的PCDM系统的原因尚不能严格解释。然而，当今大多数知名的基于共享磁盘架构的数据库解决方案，如Oracle Exadata、IBM Parallel Sysplex以及微软和Sybase的类似解决方案，都是商业数据库解决方案。因此，采用基于共享磁盘架构的数据库解决方案可能需要大量的财务投资。这可以被认为是基于共享磁盘架构的PCDM稀少的潜在原因。 一些最近的PCDM系统基于无共享架构。这些系统使用NoSQL数据库进行实验，并设计为在无共享架构下部署。与关系数据库相比，NoSQL数据库的出现是数据管理范式中的一个新事物。因此，在PCDM文献中采用基于无共享架构的NoSQL系统的努力较少。所有引用的系统在管理不同数据量时都展示了可扩展性和相应的性能。这些数据量从数百万LiDAR点到6400亿不等，现有系统在大或密集的航空点云（如AHN2、都柏林LiDAR数据集等）上进行了测试，但未在实时数据增长和流量增长方面进行测试。 随着大规模和密集的LiDAR数据集日益普及，这对使用共享磁盘或共享内存架构的系统提出了挑战。因此，在实时数据和流量增长对PCDM项目变得重要的情况下，基于无共享架构的PCDM可能提供一个有前景的途径。 综上所述，可以得出以下结论：通过并行化可以实现更高的可扩展性和性能；所有三种并行架构都可以通过增加并行度来实现更好的性能；共享内存架构在数据和流量增长方面的可扩展性最差，而无共享架构具有最大的可扩展性。此外，通过向系统中添加更多节点可以提高无共享架构系统的性能，但在节点饱和后，性能可能会受到负面影响。 所有三种并行架构在PCDM研究中都进行了探索。对于静态点云数据集，足够的并行性可以使任何一种架构都能以可扩展的方式运行。相反，在需要PCDM容纳定期数据和流量增长的情况下，基于无共享架构的PCDM系统提供了更大的潜在可扩展性。除了并行架构外，数据模型是另一个显著影响性能和可扩展性的因素。下一节将讨论数据模型。 Data Models toward Scalability and Performance〓 ReTURN 〓 数据模型在软件开发中，尤其是在数据库软件系统中，具有深远的重要性。数据模型提供了一种在数据库中表示现实世界实体及其关系的方式（例如，用户及其姓名、地址等，或LiDAR点之间的时间戳和元数据）。最常见的数据模型是Codd于1970年提出的关系模型。基于关系模型的数据库系统，即关系数据库管理系统（RDBMS），在几乎所有应用中已经成功使用了几十年。根据关系模型，数据被组织成具有固定且精确定义模式的关系（即表格）。许多关系数据库管理系统（例如Oracle和PostgreSQL）提供面向对象的功能。此类系统中的数据模型被称为对象关系数据模型。 传统上，大多数关系数据库依赖于垂直扩展。因此，它们在数据量和流量方面的可扩展性是有限的。然而，今天所有主要的关系数据库都提供水平扩展部署，并能够管理大流量和大数据量。关系模型和对象关系模型都被用于PCDM。例如，通过在表中每行存储一个点记录，每列存储一个点属性，van Oosterom等和Psomadaki等使用关系模型（不含面向对象的功能）来存储点云。此类模型被称为平面模型，并已在Oracle和PostGIS数据库管理系统中实现。平面模型允许使用标准DBMS功能直接访问点云中的每个点。然而，这些平面模型的可扩展性主要受到处理过多数据记录的成本限制（例如，管理数十亿记录的非常大的索引结构）。 除了平面模型，Oracle和PostGIS还允许使用对象关系模型存储点云（即Oracle的SDO_PC和PostGIS的PCPATCH，称为块模型）。顾名思义，点被分组成块，每个块表示为存储在数据库行中的二进制大对象（BLOB）。因此，与等效的平面模型相比，数据记录较少。这些系统中定义了专门的数据类型和操作来处理点云BLOB。Van Oosterom等通过实验表明，块模型比平面模型更具可扩展性，且通常允许更快的查询。 列式关系数据库管理系统是另一类在PCDM文献中被考虑的关系数据库系统。与优化用于存储和检索行数据的典型关系数据库不同，列式关系数据库管理系统优化用于快速检索列数据，因此被称为列式数据库。这些列式RDBMS通常设计为利用底层硬件的并行性。MonetDB和Oracle Exadata数据库机器（OEDBM）是两种著名的列式、大规模并行RDBMS。到目前为止，关于OEDBM的PCDM研究很少。在存储层次上，MonetDB和OEDBM都采用平面表方法存储点记录。 另一类数据模型称为NoSQL。与RDBMS不同，许多NoSQL数据库不需要显式预定义模式。相反，数据可以具有任意结构，并由应用程序逻辑隐式编码。这一特性通常被称为无模式，帮助降低模式演化成本，更好地支持半结构化和非结构化数据。NoSQL数据模型的例子包括键值模型（如Redis）、图模型（如Neo4j）、文档模型（如MongoDB）和宽列模型（如HBase、Cassandra、Big Table）。NoSQL数据模型为表示不同类型的数据提供了广泛的选项。每种模型都针对特定应用进行了优化。与关系模型相比，NoSQL模型允许更高的模式灵活性。 在PCDM研究中已经探索了NoSQL数据库的无模式特性。例如，Boehm等在MongoDB（文档模型）中将点云文件建模为文档对象，而Vo等研究了在HBase中使用宽列模型建模点云的四种不同方式。在这些情况下，NoSQL允许在没有预定义固定模式的情况下存储点云。这种灵活性在点云来自多个来源（如不同类型的传感器）、具有异质点属性并且不需要按照预定义标准（如LAS标准）进行摄取和存储时特别有用。与RDBMS相比，NoSQL数据库更容易容纳更高水平的数据复杂性。此外，许多NoSQL数据模型专为比RDBMS实现更高的可扩展性和性能而开发。许多NoSQL系统围绕无共享系统架构演变，因此本质上具有高度可扩展性。数据分区、分布、复制（以最小化网络通信）以及分布式索引、哈希和缓存的战略性使用是许多NoSQL系统实现高查询性能和可扩展性的技术之一。 大多数NoSQL RDBMS通过牺牲RDBMS中的主要标准功能（如ACID属性）来实现更高的性能和可扩展性。ACID属性在需要低延迟且数据库状态持续变化的应用中至关重要。此类应用常见于在线事务处理系统，如银行系统和在线票务系统。为了保证ACID合规性，数据库管理系统可能会妥协其可扩展性。尽管ACID属性对许多应用很重要，但在PCDM中是否需要ACID合规性尚无一致观点。然而，一些研究（如Vo等和van Oosterom等）认识到，大多数点云数据库在摄取后很少更新、插入或删除点云记录。因此，Vo等认为PCDM不严格要求ACID合规性。因此，在PCDM的背景下，像许多NoSQL系统那样以可扩展性和性能为代价放弃ACID合规性是可以接受的。然而，新型乘客车辆和无处不在的iPhone设备的出现可能为创建需要重新思考LiDAR数据集作为伪静态实体的新应用铺平道路，这可能影响对ACID合规性重要性的看法。 值得注意的是，高度可扩展性和ACID合规性并非必须互相排斥。EarthServer是一个ACID合规且可扩展的数据库系统的例子，能够处理点云数据。EarthServer基于Rasdaman的数组数据模型（raster data manager）。EarthServer主要支持的空间数据类型是高维光栅数据。为了在EarthServer中存储点云数据，需要将其转换为光栅表示。可以说，这种情况并不理想，因为光栅化会剥夺点云数据的丰富性，从而降低其价值。例如，在每点处理应用中，这种价值很容易体现出来。 除了EarthServer的特殊情况，现代SQL数据库如NewSQL属于一类关系数据库系统，它们提供NoSQL系统的高可扩展性和关系数据库的强一致性和可用性。然而，目前只有一个PCDM系统是基于NewSQL数据库构建的。研究人员在其中使用了SAP HANA数据库——一个内存中列式关系数据库管理系统，用于PCDM。 大多数性能基准测试的一个关键缺陷是结果仅涵盖现有数据模型中的少数几种。此外，在跨多个数据模型进行性能评估时，实验设置必须一致。在当前的PCDM文献中，这意味着对象关系数据模型和关系列式模型的性能基准测试必须在相似的实验环境下进行。尽管在PCDM中存在关系型、列式、NoSQL和NewSQL模型的采用，但迄今为止，尚未在相同实验设置下对所有模型进行性能基准测试。因此，点云数据中的所有数据模型尚不可能。 然而，据Oszu和Davoudian等人的研究，NoSQL数据库、NewSQL数据库和其他新型数据库技术表现出更好的性能。因此，在相同测试环境下获得不同NoSQL、NewSQL和传统关系数据库的性能结果，将能更好地了解性能高效的PCDM。 传统的关系型和对象关系模型已被广泛用于PCDM。主要的RDBMS（如Oracle和PostGIS）多年来一直提供PCDM解决方案。此外，硬件优化的列式关系数据库管理系统（如MonetDB、Oracle Exadata）在当前的PCDM研究中也有所应用。具体来说，NoSQL提供了传统关系模型的多种替代方案。目前文献中可以找到几种NoSQL PCDM系统和一个NewSQL PCDM系统。非关系型数据模型允许在没有刚性模式的情况下表示点云数据，因此它们能够更好地适应复杂的异构点云数据集。此外，NoSQL和NewSQL都承诺比RDBMS更高的可扩展性和性能。 许多NoSQL数据库通过牺牲ACID合规性来实现可扩展性和性能。虽然PCDM可能不需要严格的ACID合规性，但在选择数据模型时，必须考虑NoSQL模型的缺点。与NoSQL不同，NewSQL系统在不牺牲ACID的情况下提供高可扩展性。PCDM的数据模型选择应考虑实际的系统需求。例如，如果可扩展性比一致性更重要，NoSQL数据模型可能是一个合适的选择。如果必须将点云数据与当前托管在RDBMS中的现有数据集成，则关系或对象关系模型可能是更有效的选择。 Analysis of State-of-the-Art PCDM Systems〓 ReTURN 〓 如第4和第5部分所述，在讨论可扩展性和性能的理论方面时，可以将并行架构和数据模型视为独立的领域。然而，在分析已经开发的系统时，这种架构和数据模型的分离并不容易实现。这是因为在已实现的数据管理系统中，架构和相应的数据库（包括数据模型）紧密集成。因此，在研究已实现的系统时，必须结合考虑架构和数据模型及其对系统的影响。 为便于理解，本节将最先进的PCDM系统按其各自的并行架构进行分类分析，具体包括：(i) 基于共享内存的PCDM系统，(ii) 基于共享磁盘的PCDM系统，和(iii) 基于无共享架构的PCDM系统。数据模型、相应的数据库以及其他与PCDM系统相关的信息将在每种架构下进行统一展示和讨论。 Shared-Memory Based PCDM Systems表2展示了最受欢迎的基于共享内存架构的PCDM系统。根据表2，目前的共享内存架构PCDM系统成功测试的数据集范围从7400万个LiDAR点到230亿个LiDAR点。在[19]中，作者指出，即使在具有广泛并行性和更强大硬件的情况下，其实验环境中摄入6400亿个LiDAR点的过程也会影响扩展性并生成无法承受的加载时间。相比之下，230亿个点是[19]中基于共享内存的PCDM系统可以测试的最大点数，而6400亿个点则明显大得多（大约是其26.8倍）。因此，在需要管理极大量点云数据（如数十亿点）且预计数据增长会显著增加（例如达到数千亿点）的场景中，使用共享内存PCDM系统进行实验测试可能会因其动态性而变得困难。 鉴于LiDAR数据集更新的稀少性，即使是处理大规模点云数据集（如数千亿点）可能需要的巨大数据摄取时间（例如数天至数周）也可能是可以接受的，但摄取数万亿LiDAR点（如全国范围的LiDAR扫描）可能会进一步加剧数据加载时间。因此，无论在研究还是非研究场景中，都可能出现更为不利的扩展性情况。此外，共享内存系统中的资源和并行性可用性也可能阻碍管理这些数据集。 根据表2，目前的基于共享内存架构的PCDM系统主要建立在两种类型的关系数据库之上：(i) 提供持久存储的对象关系数据库和(ii) 提供内存存储的列式关系数据库。一般来说，当要管理的数据量与系统主内存大小兼容时，内存数据库是有利的。因此，在采用内存数据库进行PCDM时，系统的主内存应能提供所有可用点云数据的存储需求。此外，还应有足够的主内存用于PCDM操作，如数据加载和数据查询。然而，事先确定内存PCDM系统应具备的主内存大小并不容易，因为总体内存消耗需求受到多种因素的影响，如数据编码技术、点数、索引大小、每个点的属性数量以及常见点云操作所需的内存。 尽管如此，当主内存足够容纳点云数据和操作时，内存系统可以提供更好的性能。例如，根据[19,48]，内存PCDM解决方案（如MonetDB）在数据加载时间方面优于其他基于持久存储的共享内存架构PCDM系统。由于MonetDB中的固有并行执行，这种性能提升也得到了增强。然而，由于内存数据库中的主内存瓶颈，查询大数据量可能导致性能差和扩展性低下。例如，在[48]中，作者指出，当从存储22亿个点的MonetDB PCDM系统查询点云数据时，其查询性能优于存储22亿个点的PostgreSQL/PostGIS PCDM系统。然而，当从存储230亿个点的MonetDB PCDM系统查询点数据时，存储230亿个点的PostgreSQL/PostGIS PCDM系统的查询响应时间显著更好。作者因此得出结论，像PostgreSQL/PostGIS这样的数据库，通过在磁盘存储中将点数据组织成一组块，而不是像MonetDB那样在主内存中单独处理每个点，是管理大量点云数据的可扩展解决方案。 MonetDB和SAP HANA是用于基于共享内存架构PCDM的两种内存数据库，也可以部署在无共享架构中。根据作者的最佳知识，目前没有基于MonetDB或SAP HANA的横向扩展PCDM系统。由于横向扩展提供了有利的可扩展性和性能前景，这可以成为面向无共享架构PCDM的潜在未来研究方向。 现有的基于共享内存架构的对象关系PCDM系统主要基于Oracle或PostgreSQL/PostGIS数据库。这些数据库都遵循固定模式。因此，从可扩展性的角度来看，这些PCDM系统将无法支持从不同传感器获取的异构点云数据集的多样性。然而，如果将点云数据摄入并存储在根据LAS标准定义模式的数据库表中，这种限制可以得到缓解。 从理论角度看，共享内存架构和关系数据模型不足以支持PCDM中不断增长的数据量、流量和数据复杂性。除了表2中列出的研究外，一些PCDM研究人员仍然采用共享内存架构和关系（对象关系）数据模型。例如，在[19]中提倡的需要专门的点云数据空间数据类型，并在[68,69]中进一步研究了基于共享内存架构的PCDM系统中的这一需求。[68,69]的工作基于Oracle数据库，主要关注识别PCDM的新共享内存索引策略。类似的尝试也可以在[20,31,46,52]中找到。从可扩展性和性能的角度来看，空间索引起着关键作用。例如，在管理大量点云数据时，高效过滤或检索所需的LiDAR点是至关重要的。因此，由于减少磁盘读取次数或减轻数据库负载的正面影响，空间索引在提高查询吞吐量和查询响应时间方面非常有吸引力。由于[5]中已经对共享内存架构PCDM系统中采用的索引策略进行了相当全面的回顾，本文将不再进一步讨论这些内容。 Shared-Disk Oriented PCDM SystemsOracle Exadata PCDM系统（见[19]）似乎是唯一采用共享磁盘架构的PCDM系统，并且已在超过230亿个点的规模下进行了测试。在[19]中，作者测试了包含6400亿个LiDAR数据点的OEDBM PCDM系统，没有出现不利的数据加载或处理时间。这一示例展示了共享磁盘架构在处理前所未有的数据量方面的能力，相较于共享内存架构具有显著优势。这主要归因于OEDBM固有的大量内存使用和优化的硬件并行处理。因此，目前在PCDM文献中，评估共享磁盘架构与共享内存架构PCDM系统的可扩展性和性能仍然是一个开放的研究问题。 在基于共享内存架构和无共享架构的PCDM系统中，索引的使用以辅助点云数据检索是一个广泛讨论的话题。然而，在[19]中，OEDBM没有为点云数据检索实现任何索引。这是因为OEDBM不实现额外的数据结构或索引进行数据检索。相反，为了数据检索和减少磁盘I/O，OEDBM采用了反向索引的概念。反向索引通常通过元数据管理来实现。通过使用不同列中的最小值和最大值等元数据，OEDBM可以在无需人工干预的情况下执行数据检索。 此外，与大多数基于共享内存架构的PCDM系统类似，OEDBM主要基于关系模型（见第5节；OEDBM使用面向列的关系模型）。因此，当在OEDBM中管理点云数据时，数据必须符合固定的模式。结果是，数据复杂性方面的可扩展性受到限制。 在其计算技术方面，OEDBM利用消息传递接口（MPI）库进行分布式多节点计算。MPI主要用于具有多个节点且计算任务之间需要高度同步的系统。这是通过高效利用底层硬件中的并行性来实现的。因此，采用OEDBM进行PCDM可以充分利用底层硬件中的并行性。然而，这需要在有效分割任务（即数据加载、数据查询等）方面具备广泛的专业知识。因此，这种增加的复杂性可能成为高度可扩展的PCDM研究实验的潜在障碍。 从上述对基于共享内存和共享磁盘架构的PCDM系统的讨论来看，这些系统在最大测试规模分别为230亿和6400亿LiDAR点方面取得了成功。然而，如第4节和第5节所述，与无共享架构的系统相比，共享内存和共享磁盘系统的可扩展性受限。因此，研究基于无共享架构的PCDM系统的可扩展性是一个合乎逻辑的下一步，并且将成为下一个小节的讨论内容。 Shared-Nothing Architecture-Oriented PCDM Systems最早采用无共享架构的PCDM尝试之一在[22]中讨论。作者使用文档导向的NoSQL数据库MongoDB加载和管理了4480亿个LiDAR点。在该实验中，大型LiDAR文件被分割成一系列较小的文件，然后这些较小的文件被存储在部署在GridFS文件系统上的MongoDB集合中。作者声称在大数据量方面具有高可扩展性，因为该系统可以采用Hadoop分布式文件系统（HDFS）进行分布式存储、扩展性以及更高容量的数据管理，并支持MapReduce并行数据处理框架。然而，该解决方案仅限于文件选择，因为数据是在文件系统级别管理的。 Li等人[29]提出了一个用于LiDAR PCDM的通用框架，该框架也建立在HDFS和MapReduce之上，但仅测试了41.7亿个点（相对于之前测试的共享内存架构和共享磁盘架构系统的数据量而言，这只是一个小部分）。作者的主要动机是PCDM固有的大规模存储和数据处理需求。为此，空间组织的LAS文件被存储在HDFS中，并使用MySQL数据库来索引这些LAS文件。其解决方案还在每个节点中集成了LAStools，以生成与MapReduce框架的兼容性，从而实现LAS文件操作并支持机器学习工作。 虽然[22,29]构建了无共享架构的解决方案，但这些解决方案采用了一种混合方法，在同一系统中共存了LAS文件格式和数据库解决方案。因此，完全的数据独立性这一采用数据库进行PCDM的主要目标并未得到保证。因此，本评论的其余部分将专注于纯数据库存储解决方案。 Database-Oriented Shared-Nothing Architecture-Based PCDM一些关于无共享架构的PCDM的重要研究包括[3,33,34]。从理论上讲，这些系统对大数据量具有高度可扩展性。然而，这些基于无共享架构的PCDM系统仅在最多14亿个点上进行了评估。具体而言，文献[3,33,34]分别测试了最多14亿、2.73亿和8.12亿个点，但明确展示了无共享架构PCDM系统的潜在可扩展性。在某些情况下，横向扩展的PCDM相比于基于共享内存架构的PostgreSQL/PostGIS PCDM系统，表现出了更好的可扩展性和性能，这归因于无共享架构中资源的更高可用性。 除上述基于无共享架构的PCDM研究外，GeoWave、EarthServer/RASDAMAN和TileDB也在无共享架构环境中提供PCDM支持。表3概述了现有的基于无共享架构的PCDM解决方案。根据表3，采用无共享架构的PCDM系统主要基于宽列模型或数组模型。此外，基于MPI的高度同步计算主要应用于基于数组模型的PCDM系统。这些系统（如TileDB和EarthServer/RASDAMAN）分别使用HDFS/S3和PostgreSQL作为数据存储介质。相比之下，基于宽列模型的PCDM系统使用HBase和Accumulo数据库。此外，与MPI不同，这些基于HBase和Accumulo的PCDM系统采用Hadoop/MapReduce或Hadoop/Spark数据处理框架作为其多节点计算框架。 与MPI相比，Hadoop/MapReduce和Hadoop/Spark中的计算并非高度同步。重要的是，Hadoop/MapReduce和Hadoop/Spark提供了更高层次的编程抽象，使程序员无需掌握底层硬件的专业知识。然而，了解底层硬件将使Hadoop/MapReduce和Hadoop/Spark程序更高效地利用底层硬件的并行性。因此，从实际角度来看，基于无共享架构的PCDM使用Hadoop/MapReduce和Hadoop/Spark相较于基于MPI的PCDM来说更为简单。 如6.2节所述，MPI可以在关系模型之上使用（例如在共享磁盘架构中的OEDBM）。然而，根据表3，基于MPI和关系模型的PCDM并不普遍。因此，在横向扩展架构中采用MPI和关系数据模型可能是一个具有潜力的方向，特别是在数据复杂性方面可扩展性微不足道的情况下。 如第5节和6.2节所述，采用数组模型需要放弃点云的原生格式。然而，如第5节所述，点云数据必须被视为主要数据集，其保留以及与任何后处理的连接需要保留。这种思维模式的价值已经在太阳能潜力分析的背景下得到验证，其中中间输出是阴影分析所需的最终输出。在数组模型中，原生点云格式被转换为栅格格式。因此，尽管EarthServer/RASDAMAN和TileDB等系统可以横向扩展到多个独立节点以高效管理所需的数据量和流量，但处理数据复杂性仍然具有挑战性。 值得注意的是，由于HBase和Accumulo NoSQL数据库放宽了遵循严格模式的要求，构建在HBase和Accumulo之上的系统可以在适应数据复杂性方面提供更大的可扩展性。因此，[69]认为在HBase或Accumulo基础上的系统，如GeoWave，管理点云数据将有助于满足异构点云数据的需求。此外，基于HBase和Accumulo构建的系统提供了灵活性，可以扩展到大量节点。因此，从理论上讲，采用HBase或Accumulo的系统，如用于PCDM的宽列数据库，可以在数据量和流量以及数据复杂性方面实现更大的可扩展性。 另外，键值数据库和宽列数据库针对写密集型工作负载进行了优化。这些数据库通常建立在基于日志结构合并树（LSM tree）的存储引擎之上，而非通常建立在B树结构上的读优化存储引擎。通常，一旦点云数据被摄取，PCDM系统预计将主要执行读密集型任务。因此，采用写密集型数据库在系统设计目标方面看似矛盾。然而，在LSM树数据库（如HBase）中期待读优化的场景下，可以调整布隆过滤器。然而，现有的采用HBase的PCDM研究尚未探索布隆过滤器调整可能带来的潜在收益。 与基于共享内存架构的PCDM系统类似，基于无共享架构的PCDM系统也致力于实现高效的空间索引策略以便检索点云数据。表3显示，除TileDB，[34]和EarthServer/RASDAMAN系统外，其他使用HBase或Accumulo等数据库的研究工作在点云数据上实现了空间索引。这些努力主要测试了两种变体的空间填充曲线（SFCs）：Hilbert SFC和Morton曲线。在基于宽列数据库的PCDM中实现其他基于空间的点访问技术（如四叉树等层次索引）并未出现在同行评审的PCDM文献中。然而，已经有尝试在MD-HBase上实现四叉树，作者展示了使用磁盘上的持久四叉树数据结构来执行一系列范围和k近邻查询中的2D点数据检索。因此，在宽列数据库上实现如四叉树这样的数据结构用于PCDM可以被视为未来的潜在研究方向，特别是针对3D点云数据的空间查询。 无共享架构的采用也扩展到全波形（FWF）-LiDAR数据管理。与点云数据中的离散点不同，FWF-LiDAR包括原始信号的全波形版本。这包括脉冲组件，即表示激光束位置和方向的线段；波组件，即一系列信号幅度值；以及点组件，即脉冲和波组件处理的衍生物。FWF-LiDAR中的丰富信息提供了对扫描场景的更深入了解，但需要更多的存储和计算能力。为应对这一需求，Vo等人开发了一种新的时空索引技术和可扩展的数据管理解决方案，通过采用HBase数据库管理FWF-LiDAR数据。Vo等人的空间索引策略采用了六维Hilbert索引，该索引基于FWF-LiDAR的脉冲组件的两个边缘的x、y和z坐标。时间索引基于飞行线ID，每个脉冲组件在FWF-LiDAR数据中都是唯一的。 如第4节所述，在采用无共享架构时，增加节点是提高性能的一种手段。例如，在[51,78,80,81]的工作中，增加节点在各自的空间数据管理系统中改善了数据查询和数据加载性能。然而，在PCDM的背景下，到目前为止，只有[33]提供了在3、5和9个节点下这一潜力的充分证据。尽管该实验清楚地描绘了无共享架构在性能提升方面的益处，但仍需通过多数据集、各种标准查询和其他预期操作（如数据加载和并发数据查询）进行更系统的测试。 Comparison of Scalability and Performance of Current PCDM Systems在本节中，将全面概述当前PCDM解决方案的基础架构和数据模型，以及其他相关的关键信息，重点介绍它们在存储、查询和加载方面的可扩展性潜力和显著性能特点。 &nbsp;在PCDM研究中，实现可扩展性并确保可接受的性能是最终目标。&nbsp;到目前为止，几乎所有的PCDM实验都展示了一定程度的数据量可扩展性，但由于使用了不同的硬件、各种数据集和不同的查询，因此不容易进行比较。可以说，最可靠的实验比较方法是进行类似于van Oosterom等人所做的系统基准测试。这样的方法可以直接比较具有不同数据模型和不同索引技术的系统。 目前尚缺乏此类实验，这大大增加了PCDM系统之间进行有意义比较的复杂性。尽管存在这些已知的限制，现有比较的评估仍能提供一些见解。因此，提供了表4来进行参考。 Inspect Performance Results of Current PCDM Systems表4和图1总结了目前主要PCDM系统在存储性能、数据加载性能和窗口查询性能方面的结果。重点放在窗口查询上，因为它们在所有PCDM实验中广泛使用，尽管存在一些差异。例如，参考文献[19]主要基于2D窗口查询和kNN查询，而参考文献[3]则采用3D窗口查询。 由于将所有实验结果汇总在一个表中需要广泛的整合，因此在构建表4时做了具体的考虑，包括： 在测试不同数据集大小的情况下，报告最大的结果。 提供适当的附加信息，如加载时使用的线程数量、窗口查询的性质、使用的空间索引和实施的存储模型。 数据存储主要通过考虑总磁盘/内存使用情况来确定，即在分别提供索引和数据大小的实验中，报告两者的总和（即总存储消耗）。 尽管报告的结果受到数据异质性的影响，但由于没有简单的方法进行表征，因此必须将其视为报告中的不确定性。 关于图1，几个PCDM系统的数据加载速度和存储成本显著较高（即为离群值）。为了在图1中保持空间和清晰度，离群数据点显示在主绘图区域之外。 基于表4和图1的当前实验场景，可以得出以下观察结果： OEDBM PCDM系统（基于多节点共享磁盘架构的关系数据库导向的PCDM系统）具有最高的点/秒比。 OEDBM PCDM系统在字节/点比（即每点的字节数）方面也表现最好，得益于内置的压缩机制（即查询高压缩模式）。 在PostgreSQL/PostGIS导向的PCDM系统中： 参考文献[19]中实现的块模型在字节/点比方面表现最低（使用3000点的块）。 该系统在PostgreSQL/PostGIS系统中具有最高的点/加载时间比。 在Oracle数据库导向的PCDM系统中： 参考文献[19]中实现的Oracle平面模型具有最高的点/加载时间比。 实现的Oracle块模型在字节/点比方面表现最好。 在基于MonetDB的系统中： Morton-relaceXY实现的字节/点比更好。 Imprints索引实现的点/加载时间比更好。 对于LiDAR点云数据，2D窗口查询的研究比3D范围或kNN查询更为普遍。 数据加载和并发数据查询可以并行进行。然而，许多研究人员并未明确探索加载和查询中的并行性。 在汇总表4的数据加载结果、查询结果和存储结果的过程中，发现了一些基于共享无架构的PCDM研究实验报告中的显著差距，这些差距应在未来的报告中出现，包括： 数据库中的复制因子（例如，在HBase中，默认的复制因子为3，即在数据库中存储三份数据）。 数据加载过程中使用的节点数量。 数据检索过程中产生的通信成本（这是一种开销）。 索引大小。 点云数据在节点间的分布。 这些信息非常重要。例如，根据参考文献[3]，PostgreSQL/PostGIS消耗21.0字节存储一个LiDAR点记录，而HBase模型-4消耗28.4字节每点。假设参考文献[3]中的HBase数据库配置了默认的复制因子3，可以认为28.4字节值代表3个点的存储需求，而不是一个点。如果是这样，那么参考文献[3]的最佳场景的存储性能几乎比PostreSQL/PostGIS系统高出100％的效率。然而，由于这些信息不可用，无法在没有进一步实验的情况下得出这样的结论。同样，参与数据加载过程的节点数量、索引大小和数据通信所花费的时间等信息，对于深入分析基于共享无架构的PCDM系统的可扩展性和性能至关重要。 当前最先进的PCDM系统是基于并行架构实现的。迄今为止发表的实验表明，基于共享内存架构的PCDM系统在处理大数据量时可能会导致可扩展性和性能不佳。此外，当前基于共享内存架构的PCDM系统无法支持数据的异质性，因为这些系统基于关系数据模型。同样，基于共享磁盘架构的PCDM系统在面对数据复杂性时展示了有限的可扩展性。然而，在当前的PCDM文献中，基于共享磁盘架构的PCDM系统是唯一经过测试的大规模数据集（即高达6400亿点）的数据库导向的PCDM系统；相比之下，当前基于共享无架构的PCDM系统最多测试了14亿点。 在支持数据复杂性方面，利用HBase和Accumulo的基于共享无架构的系统展示了很大的潜力。对于并发数据访问或流量可扩展性方面的可扩展性，PCDM [19]强调这是一个重要的方面。然而，PCDM可扩展性（关于不同流量量）研究不足，到目前为止的查询都是2D窗口查询，较少尝试3D窗口和kNN查询。 Discussion〓 ReTURN 〓 讨论开发新的可扩展、高性能PCDM系统时出现的问题以及强调PCDM研究领域的关键差距是至关重要的。因此，本节提供了在特定需求出现时选择并行架构和数据模型的指导，同时也提出了未来研究的方向。 Guide to Selecting Parallel Architectures and Data Models for PCDM 图2展示了实现高度可扩展、高性能PCDM系统的途径。从并行架构的角度来看，共享无架构相比共享内存和共享磁盘架构在数据模型选择上提供了更多的灵活性。同样，从数据模型的角度来看，关系数据模型（包括对象-关系、列式关系等）提供了更多的灵活性，因为它们可以部署在任何并行架构中。 此外，如果主要选择共享内存架构，可用的数据模型将是关系模型和NewSQL模型。而共享磁盘架构则限于数组和关系模型。 虽然如前所述，NoSQL模型在可扩展性和性能方面本质上优于关系模型，但NoSQL模型主要设计用于部署在共享无架构中。因此，采用宽列和/或键值数据库将要求PCDM系统开发者坚持使用共享无架构，这可能会限制某些类型的查询。 图2提供了选择PCDM并行架构和数据模型的一些广泛指南。然而，基于本文提供的评审，选择特定的并行架构和数据模型需要仔细考虑，以确保在数据规模和复杂性方面的可接受性能和未来可用性。 具体来说，选择并行架构和数据模型应通过对预期PCDM系统的特定需求进行仔细调查来进行。从决策的角度来看，这些需求可以主要分为三个领域，即：(i) 预期点云数据量的变化，(ii) 预期流量量的变化，以及 (iii) 其他PCDM系统需求（例如，与数据模型/数据库和点云数据固有特性相关的需求）。 图3和图4提供了关于预期流量和点云数据量变化的决策工作流程。从评审来看，所有并行架构都能够为PCDM提供可扩展性和相应的性能。然而，正如评审中所强调的，PCDM系统的可扩展性和性能已经在明确定义的数据规模上进行了测试。因此，每个实验中要管理的最大数据规模是预先已知的。在这种情况下，采用任何并行架构都是可能的。这种灵活性是允许的，因为所需的存储和数据处理能力可以明确地设计到并行架构中，并且可以考虑更广泛的数据模型（如图2所示）。 在预期数据和/或流量未知但预计会有显著变化的情况下，共享无并行架构似乎更为合适。确实，如图3和图4所示，共享内存或共享磁盘并行架构的能力可能成为潜在的障碍。 虽然对并行架构的直觉可以基于PCDM系统中点云数据量和流量需求的预期变化，但其他与系统相关的需求可以为选择最合适的数据模型提供更好的见解。为此，表5概述了可用于特定PCDM系统开发需求的数据模型（请注意，表5中的列名为宽列，而不是NoSQL。原因是，在当前的PCDM阶段，宽列是唯一在完整数据库导向PCDM中实验的NoSQL模型）。例如，如果更高的可扩展性和性能是基本需求，NewSQL模型和宽列NoSQL模型可能提供具有竞争力的解决方案。虽然数组数据模型是可扩展的，但据我们所知，数组数据库的可扩展性尚未直接与NewSQL和NoSQL模型进行比较。同样，如果ACID需求和/或固定模式需求和/或点云数据或数据集之间的关系是优先考虑的，关系、NewSQL和数组模型可能提供更可行的解决方案。另一方面，如果异构点云数据集合是优先需求，应该考虑宽列导向的NoSQL数据模型。图2-4和表5中描述的基本指南将促进高度可扩展高效能PCDM系统的战略决策。 Further Research Avenues除了关于现有PCDM系统的差距和PCDM系统可扩展性与性能评估的讨论外，本综述还确定了PCDM研究可以投入更多精力的方向。以下分别讨论了现有PCDM系统和开发新PCDM系统的研究方向。 现有PCDM系统： 针对不断增长的流量量进行可扩展性和性能的实验。 设计展示数据复杂度下可扩展性和性能的实验。 开发新PCDM系统： 在共享无架构中部署行导向和列导向的关系数据库，并探索其可扩展性和性能。 探索NewSQL数据库在共享无架构中用于PCDM的途径。这包括探索图数据库、文档数据库和键值数据库（宽列之外的NoSQL数据库）的适用性。 如前所述，PCDM系统的可扩展性和性能主要通过不断增长的数据量来评估。在少数情况下，已探讨了并行加载。然而，在开发高度可扩展的数据密集型PCDM系统时，其他维度的可扩展性实验（即流量量和数据复杂度）同样重要。同样，新的数据模型的出现和这些新数据模型在共享无架构中的部署，承诺了更好的可扩展性和性能。因此，在开发新型PCDM系统时（例如，通过结合新数据模型和未探索的并行架构），务实的分析对未来PCDM研究至关重要。 最后，强调需要一个可扩展、灵活的框架来进行系统的测试、评估和比较异构PCDM系统的可扩展性和性能是重要的。如第6节所述，目前还没有系统的方法来评估现有PCDM系统的可扩展性和性能。因此，覆盖所有并行架构和数据模型的灵活可扩展框架是必要的。这样的框架需要涵盖点云数据的特征，以便不同的数据集可以直接比较。此外，还应纳入存储性能、查询性能和数据加载性能等性能维度。我们相信，这样的框架需要仔细分析PCDM实验和数据密集型系统特征化所涉及的步骤。 Conclusions〓 ReTURN 〓 LiDAR点云数据是3D地理空间科学研究的重要来源。目前，这些本质上庞大且异质的数据集正在以前所未有的规模和密度被收集。当前，有越来越多的研究尝试开发高度可扩展且性能高效的PCDM解决方案。这些尝试基于不同的并行架构和特定数据模型，已经产生了展示可扩展性和相应性能的结果。 在容量方面，通过采用符合关系模型和NewSQL模型的数据库，基于共享内存架构的PCDM系统已经成功被探索。另一方面，到目前为止，只有一个系统在管理共享磁盘架构中的点云数据方面展示了可扩展性和性能结果。展示共享磁盘PCDM系统的可扩展性和性能结果可以为PCDM研究提供重要的见解。最近，采用共享无架构和宽列NoSQL数据库的趋势逐渐显现。虽然共享无架构和宽列模型（以及一般的NoSQL模型）在理论上能够实现高性能和高可扩展性，但当前的结果不足以验证这些关于PCDM的理论主张。 对PCDM系统中并行架构和数据模型的可扩展性和性能的回顾，以及对最先进PCDM系统的评估，有助于识别在系统层面和核心研究层面上的关键研究空白。本次调查中确定的主要研究空白与现有PCDM系统的差距以及开发新型PCDM系统的差距有关。 本研究可以作为开发可扩展PCDM系统时战略决策的指南，涵盖三个主要领域：(i) 预期的点云数据量变化，(ii) 预期的流量量变化，(iii) 其他系统要求。最后，本研究确定了一个关键需求，即需要一个可扩展、灵活的框架，用于系统的测试、评估和比较异构PCDM系统的可扩展性和性能，这应在未来的研究中进行探索。","link":"/2024/06/03/PCDM-Scalability/"},{"title":"ToolEMU","text":"摘要: 语言模型（Language Model, LM）代理和工具使用方面的进展显示了丰富的功能，但也放大了潜在的风险——例如泄露私人数据或造成财务损失。 作者引入了一个使用语言模型来模拟工具执行的框架， 检测LLM代理在多场景下的安全性能。 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 800px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 章节1 章节2 Abstract〓 ReTURN 〓 近期在语言模型（Language Model, LM）代理和工具使用方面的进展，以ChatGPT插件等应用为例，展示了丰富的功能，但也放大了潜在的风险——例如泄露私人数据或造成财务损失。识别这些风险需要大量劳动，需要实施工具、手动设置每个测试场景的环境以及发现风险案例。随着工具和代理变得更加复杂，测试这些代理的高成本将使发现高风险、长尾风险变得越来越困难。 为应对这些挑战，我们引入了ToolEmu：一个使用语言模型来模拟工具执行的框架，支持对语言模型代理的多种工具和场景进行可扩展的测试。除了模拟器外，我们还开发了一种基于语言模型的自动安全评估器，用于检查代理故障并量化相关风险。通过人类评估，我们测试了工具模拟器和评估器，发现ToolEmu识别的失败案例中有68.8%是有效的真实世界代理失败。 利用我们精心设计的初始基准，包括36个高风险工具包和144个测试案例，我们对当前语言模型代理进行了定量风险分析，并发现了许多可能导致严重后果的失败。值得注意的是，即使是最安全的语言模型代理，根据我们的评估器，其失败率也达到了23.9%，这突显了开发更安全的语言模型代理以供实际部署的必要性。 Introduction〓 ReTURN 〓 近期在语言模型（Language Models, LMs）和工具使用方面的进展（Brown et al., 2020; Raffel et al., 2020; Ouyang et al., 2022; OpenAI, 2023a）导致了诸如WebGPT（Nakano et al., 2021）、AutoGPT（Richards, 2023）和ChatGPT插件（OpenAI, 2023b）等代理的开发，这些代理在现实世界中半自主地操作工具。 虽然这些方法有望解锁更强大的LM能力，但从通过文本与人类互动的LM转变为使用工具在现实世界中行动的代理，突显了其广泛部署所伴随的风险。 LM代理未能遵循指令可能导致一系列严重风险，从使用银行工具进行交易时的财务损失，到操作与物理环境互动的机器人时的重大财产损失甚至生命危险。鉴于这些失败可能带来的严重现实后果，在部署之前识别即使是低概率的风险至关重要。 然而，由于这些风险的长尾和开放性，以及测试所需的大量工程工作，识别LM代理相关风险非常具有挑战性。通常情况下，专家需要实施特定工具，设置针对指定测试案例的沙盒环境，并检查代理的执行情况。这种劳动密集型程序限制了测试空间，使得难以扩大对各种工具和场景的风险评估并识别长尾风险。 为了克服这些障碍，我们借鉴了在高风险领域（如自动驾驶）中广泛使用的基于模拟器的测试（Dosovitskiy et al., 2017），引入了ToolEmu，一个基于LM的工具仿真框架，旨在通过多种工具和场景检查LM代理，识别长尾场景中的现实失败，并通过自动评估器促进更安全的代理开发。 我们的框架核心是使用LM模拟工具及其执行沙盒。与通常通过编程和静态建立的模拟环境不同，我们利用LM的最新进展（如GPT-4 (OpenAI, 2023a)），仅使用工具规范和工具输入来模拟工具执行，而不需要其具体实现和执行环境。这使得在不同场景中快速原型化LM代理成为可能，同时能够评估缺乏现有API或沙盒实现的高风险工具。 例如，我们的模拟器可以模拟交通控制工具，揭示GPT-4在识别此类关键场景风险时的失败（见图2e）。为了进一步促进风险评估和长尾失败检测，我们引入了一个对抗性模拟器进行红队测试。对抗性模拟器自动为指定测试案例实例化更可能导致LM代理失败的沙盒状态。通过我们的模拟器，我们能够识别当前LM代理的广泛长尾和潜在严重失败（见图2中的示例）。在我们的模拟器中，200个工具执行轨迹中有超过80%被人类评估者判断为现实的。 在这些失败中，我们检查了ChatGPT-3.5在LM模拟终端工具上的7个严重失败，发现其中6个可以在实际bash终端上实例化。值得注意的是，即使有现有的bash终端沙盒，完全实例化这些失败仍需约8小时，而在ToolEmu中不到15分钟。 此外，为了支持可扩展和定量的风险评估，我们设计了一个基于LM的安全评估器来捕捉LM代理可能引起的失败并量化相关风险严重性。自动评估器检查LM代理的模拟轨迹，检测潜在的风险行为，并评估后续后果。我们的自动安全评估器能够识别73.1%由三名人工标注者多数投票确定的失败，相比之下，单个保留的人类标注者平均识别率为78.8%。在我们的模拟器和评估器识别的失败中，68.8%的被人类评估验证为确实存在风险并具有现实的模拟轨迹。我们还通过设计自动有效性评估器来量化LM代理在不牺牲安全性的情况下完成用户指令的有效性。两个评估器都经过仔细验证，以证明其与人工标注一致的速率可与标注者间一致率相媲美。 最后，我们展示了如何使用我们的模拟器和自动评估器建立一个评估基准，定量评估LM代理在各种工具和场景中的表现。我们的基准集中在一个特定的威胁模型上，即用户指令模糊或省略关键信息，LM代理在执行这些指令时未能正确解决这些模糊性，导致风险。利用我们的模拟器的可扩展性，我们策划了一个包含144个此类测试案例的评估数据集，覆盖了9种风险类型（见图5b），涵盖了来自18个类别的36个工具包，其中大多数缺乏现有的沙盒评估或在当前真实环境中难以测试（见表2）。 使用我们的基准，我们定量评估了当前的LM代理并分析了提示对LM代理安全性的影响（见表5）。我们观察到，基于API的LM如GPT-4（OpenAI, 2023a）和Claude-2（Anthropic, 2023）在安全性和有效性方面取得了最佳评估分数，并且提示调优可以进一步提高性能。然而，即使是最安全的LM代理，在我们测试案例中，根据我们的评估器，其失败率也达到了23.9%，这表明在增强LM代理安全性方面仍有很大进展空间。","link":"/2024/06/02/ToolEMU/"},{"title":"ffmpeg 基础","text":"摘要： 本文介绍了从互联网上搜集到的多种对ffmpeg工具应用的知识， 汇总后以便简单的查阅。 § Table of Contents § ffmpeg 基础 用ffmpeg将视频转成gif 基础命令 进阶命令 降低动图的帧率 调整视频的码率 调整画面分辨率 截取其中的一段视频生成gif 图片处理 Resize图片 图片裁剪 图片添加水印 图片添加中文水印 图片旋转指定的角度 图片上下左右镜像 视频裁剪 主代码 裁剪位置测试 查看视频的FPS MacOS安装FFmpeg Linux无Root权限安装FFmpeg 用ffmpeg将视频转成gif返回目录 基础命令1ffmpeg -i 你的视频文件.mp4 生成的文件名.gif 进阶命令有时候30多M的mp4视频在转为gif后可能大小会去到100+M，非常夸张，我们不希望动图那么大，因为我们一般是要放到ppt或者什么地方使用的，因此，我们需要修改一些参数，使得生产的gif更小。通常，我们可以改变gif的帧率或者分辨率来实现数据大小压缩。 降低动图的帧率用-r参数来降低征率，比如原始的input.mp4是30帧的，可以用-r 15来降低成15帧。具体效果也是很明显的，生成的文件大小从原来的86MB下降到56MB。 1ffmpeg -i input.mp4 -r 15 out1.gif 关于ffmpeg的-r参数，有个很小坑，**-r参数一定要放到-i参数后面**，它在前在后的作用是不一样的。 调整视频的码率在压缩视频大小而不影响画质的目标下会比较有用 1ffmpeg -i input.mp4 -b:v 500k output.mp4 这里的 -b:v 表示视频比特率。 调整画面分辨率比如原始视频是1080p的，我们可以将其减低到480p，从而显著降低最终的gif文件大小，这里可以使用-s参数，后面跟具体的分辨率大小比如480x272 具体命令如下： 1ffmpeg -i input.mp4 -ss 00:01:13.500 -t 12 -s 480x272 out2.gif 有些时候，我们不知道原始的视频分辨率，如果贸然指定目标分辨率的话，可能会导致画面被拉伸，观感下降，这时候可以使用一个更高级的参数-vf，-vf其实就比较复杂了，这里我们只看如何用-vf参数实现宽高等比例缩放吧，具体命令如下： 123#scale后面可以指定具体的分辨率宽:高，作用同-s，也可以只指定宽或者高，#另一者用-d代替，ffmpeg就会自动缩放，保持原比例ffmpeg -i input.mp4 -vf &quot;scale=480:-1&quot; out3.gif 从原来的1080p降低到了480p后，文件大小变化更加明显，从原来的86MB降低到了6MB。看来还是将分辨率的效果比较明显，但是6MB作为表情包还是太大了，这时候我们可以将上面两种方式结合到一起，降分辨率的同时降帧率，命令行如下： 1ffmpeg -i input.mp4 -vf &quot;scale=480:-1&quot; -r 15 out4.gif 最后生成的文件只有4MB了。 截取其中的一段视频生成gif使用-ss和-t两个参数，-ss代表的是start time，也就是开始时间，你可以指定开始的具体秒数，也可以用时:分:秒.毫秒的格式，比如00:12:14.500。-t是要截取的时长，单位是秒。假设我们想从input.mp4里的第73.5秒开始，截取12秒的视频内容生成动图，命令行你可以写成如下的方式： 12ffmpeg -i input.mp4 -ss 73.5 -t 12 out.gifffmpeg -i input.mp4 -ss 00:01:13.500 -t 12 out.gif 图片处理返回目录 Resize图片1234ffmpeg -i image_source -vf scale=width:height out_source#width为压缩后的图片宽度#height为压缩后的图片高度#当width或height其中有一个数值为-1时将保持原来图片尺寸比例压缩 图片裁剪123456ffmpeg -i image_source -vf crop=width:height:from_x:from_y out_source#width是要裁剪出的图片的宽度#height是要裁剪出的图片的高度#from_x是裁剪的起始X轴的位置#from_y是裁剪的起始Y轴的位置#如果不指定from_x和from_y表示从原始图片中心位置开始裁剪 图片添加水印1234567ffmpeg -i image_source -vf movie=logo_source,scale=logo_width:logo_height,lut=a=val*opacity_num[watermask];[in][watermask] overlay=from_x:from_y[out] -y out_source#logo_source为水印图片地址#logo_width为水印图片的宽度#logo_height为水印图片的高度#opacity_num为水印图片的透明度#from_x为水印的起始X轴的位置#from_y为水印的起始Y轴的位置 图片添加中文水印123456789ffmpeg -i image_source -vf drawtext=fontfile=font_ttf_path:fontcolor=font_color:fontsize=font_size:text=message_info:x=from_x:y=from_y out_source#font_ttf_path为字体路径，此项必须设置否则会出现字体无法找到的错误;#字体路径要设置绝对路径并且要注意路径正反斜线转义(例如C\\\\:/Windows/Fonts/simhei.ttf)#font_color为字体的颜色#font_size为字体的大小#message_info为水印文字内容#from_x为水印的起始X轴的位置#from_y为水印的起始Y轴的位置#如果水印内容是中文需要设置中文字体否则会文字显示乱码 图片旋转指定的角度123456789ffmpeg -i image_source -vf rotate=route_num*PI/180 -y out_source#route_num是要旋转的角度#当route_num大于0的时候顺时针旋转,当route_num小于0的时候逆时针旋转ffmpeg -i image_source -vf transpose=route_number -y out_source#route_number=0顺时针旋转90度再左右镜像#route_number=1顺时针旋转90度#route_number=2逆时针旋转90度#route_number=3逆时针旋转90度再左右镜像 图片上下左右镜像123ffmpeg -i image_source -vf hflip out_sourceffmpeg -i image_source -vf vflip out_source 视频裁剪主代码123456video_path=''fps=25crop=&quot;1300:1300:0:120&quot;resize=512out_name=&quot;out&quot;ffmpeg -y -i $video_path -vf &quot;fps=$fps, crop=$crop, scale=$resize:$resize&quot; -c:v libx264 $video_folder/&quot;$out_name_cropped.mp4&quot; 裁剪位置测试对于输入的视频，如果不知道如何设置crop参数，不知道怎么设置起始的点位，及长宽，首先我们可以先截取视频的首帧图像来做实验： 【起始点位】：截取框的左上角角点在原图内的坐标位置【长宽】：向下及向右截取的范围 1ffmpeg -i video.mp4 -vf &quot;select=eq(n\\,0)&quot; -vframes 1 first_frame.jpg -i video.mp4：指定要处理的输入视频文件名称。 -vf &quot;select=eq(n\\,0)&quot;：使用视频过滤器选择帧。该过滤器选择第 n 帧，其中 n 的值为 0。因此，此过滤器选择视频的第一帧。 -vframes 1：指定要提取的帧数为 1。 first_frame.jpg：指定输出文件的名称。提取的帧将保存为 JPG 格式的图像文件。 然后我们在首帧图像上实施crop操作，观察结果： 1234crop=&quot;1300:1300:0:120&quot;input_name=&quot;inputs.jpg&quot;output_name=&quot;outputs.jpg&quot;ffmpeg -i $input_name -vf crop=$crop $output_name -y 查看视频的FPS1ffprobe -v error -select_streams v:0 -show_entries stream=r_frame_rate -of default=noprint_wrappers=1:nokey=1 video_file.mp4 | bc MacOS安装FFmpeg返回目录目前直接brew安装即可，但是执行前，首先确保xcode工具已经更新，是最新，然后确保brew工具更新到最新 1brew update 最后直接brew安装ffmpeg即可 1brew install ffmpeg Linux无Root权限安装FFmpeg返回目录首先，我们在http://ffmpeg.org/releases这个网址找到可用的ffmpeg版本，并下载 wget http://ffmpeg.org/releases/ffmpeg-{指定版本号}.tar.bz2 进入文件夹 1234./configure --enable-libmp3lame --enable-libx264 --enable-gpl --prefix={自定义的路径}# 比较关键的在于--prefix，由于我们没有管理员权限，所以设置的安装位置在本地makemake install 安装完成后会在prefix所指示目录下生成bin, include, lib, share几个文件夹，然后开始编辑.bashrc文件，让其可以访问到bin中的内容即可。","link":"/2024/05/30/ffmpeg%E7%94%A8%E6%B3%95/"},{"title":"gcc与g++的编译及配置","text":"摘要: 在本文中，笔者回答了编程环境配置中的常见问题，即在无root权限情况下，在非系统路径中部署一个与系统默认版本不同的gcc或g++，然后只在我们自己使用时显式指定这个高级版本的库。 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 背景 解决方案 因地制宜 寻求外援 下载 编译安装 环境配置 背景很多时候当我们希望在远程服务器上部署某个软件，或者安装某个代码库的时候，会发现服务器默认的gcc或者g++版本并不符合我们的期望，例如，当我希望在dsflab的服务器上安装pytorch3d时，以下是我面临的情况： 12345678(base) bash-4.2$ gcc -vUsing built-in specs.COLLECT_GCC=gccCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapperTarget: x86_64-redhat-linuxConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linuxThread model: posixgcc version 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC) 这是由于我们的系统gcc版本并不符合安装pytorch3d的要求。那么在这种情况下，我们有3条路可以选： 换一台服务器（一劳永逸） 安装低版本pytorch3d（目标降级） 更新gcc及g++版本（直面挑战） 当然我们肯定是选择第三条路，但要注意几点： 不影响其他user的使用 我们作为user没有root权限 则我们需要在无root权限情况下，在非系统路径中部署一个更高级版本的gcc或g++，然后只在我们自己使用时显式指定这个高级版本的库。 解决方案〓 ReTURN 〓 因地制宜首先，不要盲目地就开始跑去下载我们需要的gcc或者g++，一般来说，很多服务器会预先配置好很多个不同版本的gcc，g++，cuda等等，默认虽然是低版本的代码库，但是user可以根据自己的需要去指定某些高版本的库。因此很有可能，&nbsp;我们需要的目标版本的gcc或g++在服务器上已经有了，只需要我们去显式地指定使用它们&nbsp;。 12345678910111213141516(base) bash-4.2$ cd /usr/local/ # 一般多个版本的库都放在这个地方(base) bash-4.2$ lsbin cuda-10.0 cuda-10.1.105 cuda-11 cuda-11.0.207 cuda-11.4 cuda-11.5.0 cuda-11.7 cuda-11.8.0 cuda-8.0 cuda-8.0.61-1 cuda-9.1 cuda-9.2.148 etc include libexec packages softwarecuda cuda-10.0.130 cuda-10.2 cuda-11.0 cuda-11.1 cuda-11.4.100 cuda-11.6 cuda-11.7.0 cuda-7.0 cuda-8.0.44 cuda-9.0 cuda-9.1.85 cuda-9.2.88 GNU lib man sbin srccuda-10 cuda-10.1 cuda-10.2.89 cuda-11.0.182 cuda-11.1.74 cuda-11.5 cuda-11.6.2 cuda-11.8 cuda-7.5 cuda-8.0.61 cuda-9.0.176 cuda-9.2 dbpackages go lib64 nvidia share(base) bash-4.2$ cd GNU(base) bash-4.2$ lsautoconf binutils emacs gcc-10.2.0 gcc-12.2.0 gcc6 gcc-7.2.0 gcc-8.3.0 gdbm-1.11 glpk gnutls libgcrypt m4 mpfr perl-5autoconf-2.69 binutils-2.24 emacs-24.4 gcc-10.3.0 gcc4 gcc-6.1.0 gcc-7.3.0 gcc9 gettext glpk-4.65 gnutls-3.3.10 libgcrypt-1.7.6 m4-1.4.17 mpfr-3.1.2 perl-5.20.1automake binutils-2.25.1 flex gcc11 gcc-4.9.2 gcc-6.2.0 gcc-7.5.0 gcc-9.1.0 gettext-0.19.5.1 gmp gperf libgpg-error make mpfr-3.1.5 perl-5.22.0automake-1.13 binutils-2.28 flex-2.5.39 gcc-11.1.0 gcc5 gcc-6.3.0 gcc8 gcc-9.2.0 glibc gmp-6.0.0a gperf-3.1 libgpg-error-1.27 make-4.3 parallel readlineautomake-1.13.4 clisp gcc10 gcc-11.2.0 gcc-5.3.0 gcc7 gcc-8.1.0 gcc-9.3.0 glibc-2.22 gnuplot gsl libsigsegv mpc parallel-20210722 readline-6automake-1.15 clisp-2.49 gcc-10.1.0 gcc12 gcc-5.4.0 gcc-7.1.0 gcc-8.2.0 gdbm glibc-2.34 gnuplot-5.0.0 gsl-1.16 libsigsegv-2.10 mpc-1.0.2 perl5 readline-6.3 如我们所料，该文件下已经有各种高版本的gcc了，我们只需要在环境配置中显式指定他们就好。可以参考如下写法进行指定： 123#GCCexport PATH=/usr/local/GNU/gcc-11.2.0/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/GNU/gcc-11.2.0/lib:$LD_LIBRARY_PATH 这样，当我们source环境配置后，再检查gcc以及g++版本 123456789jlijz@bkzxcpu2a:/usr/local/GNU$ gcc -vUsing built-in specs.COLLECT_GCC=/usr/local/bin/gcc11COLLECT_LTO_WRAPPER=/usr/local/GNU/gcc-11.2.0/libexec/gcc/x86_64-pc-linux-gnu/11.2.0/lto-wrapperTarget: x86_64-pc-linux-gnuConfigured with: ../gcc-11.2.0/configure --prefix=/usr/local/GNU/gcc-11.2.0 --enable-clocale=generic --enable-host-shared --enable-sharedThread model: posixSupported LTO compression algorithms: zlibgcc version 11.2.0 (GCC) 寻求外援〓 ReTURN 〓 当本地确实不存在高版本的gcc，那么就必须寻求外援了。 下载123456wget http://ftp.gnu.org/gnu/gcc/gcc-5.4.0/gcc-5.4.0.tar.gz#5.4.0可以替换为你需要的版本号tar –zxvf gcc-5.4.0.tar.gzcd gcc-5.4.0./contrib/download_prerequisites#这一步是执行脚本自动下载安装所需的依赖 编译安装1234567cd gcc-5.4.0mkdir objdir cd objdir../configure --disable-checking --enable-languages=c,c++ --disable-multilib --prefix=/home/username/gcc-5.4 --enable-threads=posix#我这里使用的安装路径是/home/username/gcc-5.4，大家可以换成别的路径make -j4 //执行makefile （过程漫长，可用多线程，但我没用）make install &nbsp;注意事项&nbsp; --prefix=/home/username/gcc-5.4 这个选项的作用是指定GCC安装的目标目录。配置脚本会将编译好的GCC及其相关文件安装到这个目录中。这个路径不要填写 objdir 的绝对路径，而是填写你希望安装GCC的位置。 例如，假设你的用户名是xxx，你希望将编译好的GCC安装到你的主目录下，可以将 --prefix 设置为 /home/xxx/gcc-5.4。这样，当你运行 make install 时，GCC会被安装到这个指定的目录中。 &nbsp;报错记录（1）&nbsp; 相关博客 1234configure: error: *** LIBRARY_PATH shouldn't contain the current directory when*** building gcc. Please change the environment variable*** and run configure again. LD_LIBRARY_PATH以冒号结尾，GCC不赞成该冒号。 还应确保C_INCLUDE_PATH不以冒号结尾，以避免出现相关问题。 方法如下： 方法一： 重新export LIBRARY_PATH和C_INCLUDE_PATH 尾部不含冒号 方法二： 12export LIBRARY_PATH=$(echo $LIBRARY_PATH | sed 's/:$//; s/^://;')export C_INCLUDE_PATH=$(echo $C_INCLUDE_PATH | sed 's/:$//; s/^://;') 环境配置〓 ReTURN 〓 12345vim ~/.bashrcexport PATH=/home/username/gcc-5.4/bin:/home/username/gcc-5.4/lib64:$PATHexport LD_LIBRARY_PATH=/home/username/gcc-5.4/lib/:$LD_LIBRARY_PATH#注意这里的path一定要和make的时候--prefix的path要对的上source ~/.bashrc","link":"/2024/06/05/gcc%E4%B8%8Eg-%E7%9A%84%E7%BC%96%E8%AF%91%E5%8F%8A%E9%85%8D%E7%BD%AE/"},{"title":"SayPlan - Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning","text":"摘要: .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 Abstract Introduction Related Work SayPlan 问题表述 预备知识 场景表示：3D场景图 场景图模拟器 方法 语义搜索 迭代重新规划 实验设置 语义搜索 因果规划 结果 语义搜索 扩展性分析 因果规划 局限性 结论 Abstract〓 ReTURN 〓 大型语言模型（LLMs）在开发多任务通用规划代理方面已显示出令人印象深刻的成果。然而，将这些计划落实到广阔的、多楼层和多房间环境中，对机器人技术提出了巨大挑战。我们介绍了SayPlan，这是一种基于LLM的大规模任务规划方法，使用3D场景图（3DSG）表示来进行机器人任务规划。 为了确保我们方法的可扩展性，我们采取了以下措施： 利用3DSG的层次结构，使LLMs能够从完整图的较小压缩表示中进行任务相关子图的语义搜索； 通过整合经典路径规划器，减少LLM的规划范围； 引入迭代重新规划管道，利用场景图模拟器的反馈来优化初始计划，纠正不可行的动作并避免规划失败。 我们在两个大规模环境中评估了我们的方法，这些环境包含多达3层楼和36个房间，共有140个资产和对象。结果表明，我们的方法能够将抽象的自然语言指令转化为大规模、长远任务计划，并使移动操作机器人能够执行这些计划。 Introduction〓 ReTURN 〓 “给我做杯咖啡并放在我的桌子上”——如此看似简单的命令，对当今的机器人来说仍是艰巨的任务。这一挑战遍及机器人学的各个方面，包括导航、感知、操控以及高层次任务规划。大型语言模型（LLMs）的最新进展在将常识知识融入机器人领域方面取得了重大进展。这使得机器人能够为需要大量背景知识和语义理解的各种任务规划复杂策略。 为了使LLMs在机器人规划中有效，它们必须基于现实，即必须遵守机器人操作的物理环境所呈现的约束，包括可用的affordances、相关谓词以及动作对当前状态的影响。此外，在广阔的环境中，机器人还必须理解自己所在的位置，找到感兴趣的物品，并理解环境的拓扑结构，以便在必要的区域进行规划。为了解决这一问题，最近的研究探索了利用基于视觉的价值函数、物体检测器或场景的Planning Domain Definition Language (PDDL)描述来落实基于LLM的规划器的输出。然而，这些努力主要局限于小规模环境，通常是单个房间，预先编码了所有现有资产和物体的信息。 我们提出了一种可扩展的方法，用于在跨多个房间和楼层的环境中落实基于LLM的任务规划。我们通过利用日益增长的3D场景图（3DSG）研究实现这一目标。3DSGs捕捉了丰富的拓扑和层次化的语义图表示，能够编码任务规划所需的信息，包括物体状态、谓词、affordances和属性，使用自然语言进行编码，适合LLM解析。我们可以利用这种图的JSON表示作为预训练LLM的输入，为了确保规划的可扩展性，我们提出了三个关键创新。 首先，我们提出了一种机制，使LLM能够通过操控“压缩”3DSG的节点进行任务相关子图的语义搜索，通过expand和contract API函数调用，仅暴露完整图的顶层，从而使得在越来越大规模的环境中进行规划成为可能。在此过程中，LLM在规划期间保持对相对较小的信息子图的关注，不会超出其token限制。其次，由于任务计划的范围随着任务指令的复杂性和范围的增加而增长，LLM产生幻觉或生成不可行的动作序列的趋势也在增加。我们通过首先放宽LLM生成导航组件的需求，转而利用现有的最优路径规划器如Dijkstra来连接LLM生成的高层节点来应对这一问题。最后，为确保计划的可行性，我们引入了迭代重新规划管道，利用场景图模拟器的反馈验证和优化初始计划，以纠正任何不可执行的动作，例如在将物品放入冰箱之前忘记打开冰箱门，从而避免由于不一致、幻觉或违反环境所施加的物理约束和谓词而导致的规划失败。 我们的SayPlan方法确保了在跨多个楼层和房间的大规模环境中为移动操作机器人生成可行且基于现实的计划。我们在一系列90个任务中评估了我们的框架，这些任务分为四个难度级别，包括语义搜索任务（如“找些非素食的东西”）到互动性强、长远的任务，具有模糊的多房间目标，要求大量常识推理（如“和Niko开个玩笑”）。这些任务在两个广阔的环境中进行评估，包括一个包含37个房间和150个可互动资产和物体的大型办公楼层，以及一个拥有28个房间和112个物体的三层住宅。我们的实验验证了SayPlan在大型环境中进行任务规划的能力，同时保持较低的token占用率。通过引入语义搜索管道，我们可以将用于LLM解析的完整大规模场景表示减少多达82.1%，我们的迭代重新规划管道使得几乎完美的可执行率，适用于实际移动操作机器人执行。 Related Work〓 ReTURN 〓 机器人任务规划旨在生成一系列高层次的动作，以在特定环境中实现目标。传统方法使用领域特定语言，如PDDL和ASP，以及语义解析、搜索技术和复杂启发式方法来解决问题。然而，这些方法在处理大型环境时缺乏可扩展性，并且在现实世界操作中缺乏任务通用性。基于层次和强化学习的替代方法在数据需求和可扩展性方面面临挑战。我们的工作利用LLM的上下文学习能力，在3D场景图上生成任务计划。在这种情况下，任务可以用自然语言表达，LLM的互联网规模训练提供了所需的任务通用性，而3D场景图为大规模环境操作提供了必要的基础。这相比传统的非LLM方法，提供了一个通用且可扩展的框架。 使用LLM进行任务规划，即将自然语言提示转化为机器人任务计划，是该领域的新兴趋势。早期研究有效利用了预训练LLM的上下文学习能力，为具身代理生成可执行计划。机器人领域的一个关键挑战是将这些计划扎根于机器人的操作环境中。此前的研究探索了使用物体检测器、PDDL环境表示或价值函数来实现这一点，然而，这些方法主要局限于单房间环境，且随着场景中物体数量的增加，其扩展性较差，限制了其在多房间或多楼层环境中的规划能力。在本研究中，我们探索了3D场景图和LLM生成大规模场景计划的能力，利用这些表示的固有层次性和语义性质。 在LLM中整合外部知识是一项不断发展的研究方向，结合语言模型和外部工具以提高其输出的可靠性。在这种情况下，外部模块用于向LLM提供反馈或额外信息，以指导其输出生成。这可以通过API调用外部工具或从操作环境中获取文本反馈来实现。与我们的工作更为相关的是CLAIRIFY，它迭代利用编译器错误反馈来重新提示LLM生成语法有效的代码。在这些想法的基础上，我们提出了一种基于场景图模拟器的迭代计划验证过程，以确保所有生成的计划符合预构建场景图捕捉的约束和谓词。这确保了计划在对应真实环境中的移动机械手机器人上的直接可执行性。 SayPlan〓 ReTURN 〓 问题表述〓 ReTURN 〓 我们旨在解决基于自然语言指令的自主代理（如移动操作机器人）在大规模环境中的长程任务规划挑战。这需要机器人理解抽象且模糊的指令，理解场景，并生成包含导航和操作的任务计划。现有方法缺乏在跨多个楼层和房间的场景中进行推理的能力。 我们的重点是将大规模场景整合到基于语言模型（LLMs）的规划代理中，并解决可扩展性问题。我们旨在解决两个关键问题： 在LLM的token限制内表示大规模场景。 在大规模环境中生成长远计划时，减轻LLM产生的幻觉和错误输出。 预备知识在此，我们描述环境的3D场景图表示及其API模拟器，我们在整个方法中利用这些工具。 场景表示：3D场景图3D场景图（3DSG）最近作为机器人可操作的世界表示方式出现。它通过空间语义和对象关系在多个层次上对环境进行层次化抽象，同时捕捉环境中实体的相关状态、可供性和谓词。形式上，3DSG是一个层次化的多重图 ( G = (V, E) )，其中顶点集 ( V ) 包括 ( V1 ∪ V2 ∪ … ∪ VK )，每个 ( Vk ) 表示层次结构k中的顶点集。顶点 ( v ∈ Vk ) 发出的边仅可终止于 ( Vk−1 ∪ Vk ∪ Vk+1 )，即边连接相同层次或相邻层次的节点。 我们假设一个大规模环境的预构建3DSG表示是通过现有技术生成的。整个3DSG可以表示为一个NetworkX图对象，并以JSON数据格式进行文本序列化，可直接被预训练的LLM解析。3DSG的一个资产节点示例如下：{name: coffee_machine, type: asset, location: kitchen, affordances: [turn_on, turn_off, release], state: off, attributes: [red, automatic], position: [2.34, 0.45, 2.23]}，节点之间的边表示为{kitchen↔coffee machine}。3DSG层次结构主要分为四个层次：楼层、房间、资产和对象。顶层包含楼层，每个楼层分支到多个房间，这些房间通过姿势节点相互连接以表示环境的拓扑结构。在每个房间内，有资产（不可移动实体）和对象（可移动实体）。资产和对象节点都编码了包括状态、可供性、附加属性（如颜色或重量）和3D姿势在内的特定信息。图中还包含一个动态代理节点，表示机器人在场景中的位置。请注意，这个层次结构是可扩展的，节点层次可以调整以捕捉更大的环境，例如校园和建筑物。 场景图模拟器场景图模拟器是用于操作JSON格式的3DSG的一组API调用，主要功能包括： collapse(G)：给定完整的3DSG，此函数返回更新后的场景图，仅显示3DSG层次结构中的最高层次节点，例如楼层节点。 expand(node name)：返回更新后的3DSG，显示与节点名相关的下一级别的所有节点。 contract(node name)：返回更新后的3DSG，隐藏与节点名相关的下一级别的所有节点。 verify_plan(plan)：在由3DSG捕捉的抽象图层级模拟生成的计划，以检查每个动作是否符合环境的谓词、状态和可供性。如“无法拿起香蕉”，如果包含香蕉的冰箱是关闭的。 方法我们提出了一个可扩展的框架，使用3DSG表示将预训练LLM的通用任务规划能力与跨越多个楼层和房间的大规模环境结合起来。给定3DSG ( G ) 和自然语言定义的任务指令 ( I )，我们可以将框架SayPlan视为一个高层次任务规划器 ( π(a|I, G) )，能够生成在机器人操作环境中扎根的长远计划。这一计划随后被送入低层次的视觉基础运动规划器以实现现实世界的执行。为确保SayPlan的可扩展性，引入了两个阶段：语义搜索和迭代重新规划，详细如下。SayPlan流程的概述见图1，对应的伪代码见算法1。 语义搜索在使用LLM对3DSG进行规划时，我们注意到两个关键观察： 大规模环境的3DSG可以随着其包含的房间、资产和对象的数量无限增长，使其作为LLM的输入变得不切实际，因为token限制。 解决给定任务仅需全3DSG的一个子集 ( G′ )，例如制作一杯咖啡时不需要知道浴室里的牙膏。 为此，语义搜索阶段旨在从全3DSG中识别出包含解决给定任务所需实体的较小、任务特定的子图 ( G′ )。为识别 ( G′ )，我们利用这些表示的语义层次结构和LLM的推理能力。我们首先折叠 ( G )，仅显示其顶层节点，例如楼层节点，初始token表示减少约80%。LLM通过扩展和收缩API调用操纵这个折叠图，以根据给定指令 ( I ) 识别任务所需的子图。这通过一组输入输出示例中的上下文学习实现，并利用连锁思维提示指导LLM识别需要操作的节点。选择的API调用和节点在场景图模拟器中执行，更新后的3DSG返回给LLM进一步探索。如果扩展的节点被发现包含与任务无关的实体，LLM会收缩它以管理token限制并保持任务特定的子图。 为避免扩展已收缩的节点，我们维护一个先前扩展节点的列表，作为附加记忆输入传递给LLM，促进马尔可夫决策过程，使SayPlan能够扩展到广泛的搜索序列，而无需维护完整的交互历史。一旦在当前子图 ( G′ ) 中识别出所有必要的资产和对象，LLM就会自动进入规划阶段。语义搜索期间LLM与场景图的交互示例见附录K。 迭代重新规划在确定子图 ( G’ ) 和任务指令 ( I ) 后，LLM 进入管道的规划阶段。此时，LLM 需要生成一系列节点级别的导航（如 goto(pose2)）和操作（如 pickup(coffee_mug)）动作，以满足给定的任务指令。然而，LLM 并不是完美的规划代理，容易产生错误或幻觉输出。这在处理大规模环境或长远任务时更为明显。 我们通过两种机制来辅助 LLM 生成任务计划。首先，我们通过将姿态级路径规划委托给优化路径规划器（如 Dijkstra）来缩短 LLM 的规划范围。例如，将 [goto(meeting_room), goto(pose13), goto(pose14), goto(pose8), …, goto(kitchen), access(fridge), open(fridge)] 简化为 [goto(meeting_room), goto(kitchen), access(fridge), open(fridge)]。路径规划器处理高层位置之间的最优路线，允许 LLM 专注于任务的关键操作部分。 其次，我们利用 LLM 的自我反思能力，通过场景图模拟器的文本反馈来迭代地修正其生成的计划。该模拟器评估生成的计划是否符合场景图的谓词、状态和可供性。例如，如果机器人已经拿着东西或不在正确位置，或者冰箱未先打开，则 pick(banana) 操作可能会失败。这些失败被转换为文本反馈（如“无法拿起香蕉”），附加到 LLM 的输入中，并用于生成更新的可执行计划。 这种迭代过程包括规划、验证和反馈整合，直到获得可行计划。验证后的计划被传递给低级运动规划器以执行机器人操作。LLM 和场景图在迭代重新规划中的交互示例见附录 L。具体实现细节见附录 A。 实验设置〓 ReTURN 〓 我们设计实验以评估 LLM 在高层任务规划中的 3D 场景图推理能力，重点是移动操纵机器人。计划包括一个 7 自由度的机器人臂，配有两指夹持器和一个移动基座。我们使用两个大规模环境，分别展示多个房间和楼层，LLM 代理需要在这些环境中进行规划。 为更好地展示 SayPlan 的能力，我们将其语义搜索能力与总体因果规划能力分开评估，具体如下： 语义搜索在此测试中，LLM 在折叠的 3D 场景图上进行查询，需要推理房间和楼层节点名称及其属性，以找到执行任务指令所需的相关资产和对象。我们与人类基线进行比较，了解 LLM 的语义搜索能力与人类思维过程的对比。此外，为更好地理解不同 LLM 模型对图形推理的影响，我们还比较了使用 GPT-3.5 的 SayPlan 变体。 因果规划在此实验中，我们评估 SayPlan 生成可行计划以解决给定自然语言指令的能力。评估指标分为两部分：1）正确性，主要验证计划的总体目标及其与人类解决任务的方式的对齐；2）可执行性，评估计划与场景图环境约束的对齐及其由移动操纵机器人执行的能力。我们注意到，可执行的计划不一定正确，反之亦然。我们将 SayPlan 与两种基线方法进行比较，这些方法在任务规划中整合了 LLM：LLM-As-Planner 生成完整的计划序列，包括机器人完成任务必须执行的所有导航和操作动作序列；LLM+P 是 SayPlan 的一种简化版本，仅整合路径规划器以允许更短的规划序列，无需迭代重新规划。 结果〓 ReTURN 〓 语义搜索我们在表 1 中总结了语义搜索评估的结果。SayPlan (GPT-3.5) 在推理输入图表示时 consistently 失败，幻觉产生探索节点或多次停留在同一节点上。相比之下，SayPlan (GPT-4) 在简单和复杂搜索任务中分别达到 86.7% 和 73.3% 的成功率，显著优于 GPT-3.5。 尽管预期中人类基线在所有指令集上均达到 100%，我们更关注语义搜索中常识推理的定性评估。我们希望了解人类和 LLM 在任务指令下使用的语义搜索启发式方法的相似性。 我们在附录 F 中展示了 SayPlan (GPT-4) 和人类基线探索节点的完整序列。表中显示，对于大多数任务，SayPlan (GPT-4) 展现出与人类类似的语义和常识推理，例如在要求“找一个熟香蕉”时，LLM 首先探索厨房，然后是下一个最可能的位置——自助餐厅。在指令中没有语义的情况下（如“找物体 K31X”），LLM 代理能够进行类似广度优先搜索，遍历所有未探索节点。这突出显示了有意义的节点名称和属性的重要性，这些名称和属性捕捉了相关环境语义，LLM 可以利用这些语义进行有效搜索。 一个奇怪的失败案例发生在简单搜索指令中涉及否定的情况，例如“找一个没有柜子的办公室”或“找一个没有马桶的浴室”。这种情况下，代理总是失败。在复杂搜索指令中，还注意到LLM无法对图节点进行简单的基于距离和数量的推理。虽然这些对于人类来说是微不足道的，但对于LLM代理而言，需要同时推理多个节点，这常常会导致幻觉或错误计数。 扩展性分析我们还分析了SayPlan在语义搜索中的扩展性。表2展示了利用3D场景图的层次结构并允许LLM从压缩的初始状态探索图所带来的影响。这使得表示办公室环境所需的初始输入标记减少了82.1%，家庭环境减少了60.4%。图3展示了赋予LLM收缩不适合解决任务的已探索节点的能力，使其在整个语义搜索过程中，从标记角度保持近乎恒定的输入内存。请注意，最初存在的标记数量已经代表了附录J中给出的输入提示标记。关于SayPlan在更大3DSG上的扩展性的进一步消融研究见附录H。 因果规划表3（左）总结了简单和长时间指令的因果规划结果。我们将SayPlan的性能与两个基线方法进行比较：LLM-As-Planner和LLM+P。所有三种方法在简单规划任务中都表现出一致的正确性，达到93%，这主要是由于底层LLM推理能力的结果。然而，值得注意的是，在长时间任务中，路径规划器和迭代重新规划在提高正确性指标方面起着重要作用，通过缩短规划时间并允许LLM反思其先前的输出来实现。 结果表明，确保任务计划可执行性的关键是迭代重新规划。LLM-As-Planner和LLM+P都表现出较差的可执行性，而SayPlan由于迭代重新规划实现了近乎完美的可执行性，这确保了生成的计划符合环境的约束和前提条件。详细的任务计划和遇到的错误见附录G。我们在表3（右）中总结了这些错误，显示了LLM+P和LLM-As-Planner生成的计划中包含的各种错误，限制了它们的可执行性。尽管LLM+P通过经典路径规划器减少了导航路径规划错误，但仍然存在环境操作错误——缺失操作或违反环境前提的错误。SayPlan通过迭代重新规划减少了这些错误，但在6.67%的任务中，未能纠正某些幻觉节点。尽管我们认为这些错误最终可以通过迭代重新规划得到纠正，但我们在所有实验中将重新规划步骤限制为5步。我们在附录I中展示了使用SayPlan在配有视觉引导运动控制器的移动机械手机器人上生成的计划的实际执行情况。 局限性〓 ReTURN 〓 SayPlan受当前大型语言模型（LLM）固有局限性的制约，包括偏见和不准确性，影响其生成计划的有效性。具体来说，SayPlan受到底层LLM的基于图的推理能力的限制，这些模型在简单的基于距离和数量的推理以及节点否定方面表现不佳。未来的工作可以探讨针对这些特定任务对这些模型进行微调，或结合现有的更复杂的图推理工具以促进决策。其次，SayPlan的当前框架受到预先构建的3D场景图的需求限制，并假设在地图生成后对象保持静态，极大地限制了其适应动态真实环境的能力。未来的工作可以探讨如何将在线场景图SLAM系统集成到SayPlan框架中以解决这一问题。此外，将开放词汇表示引入场景图中可以产生一般场景表示，而不仅仅是文本节点描述。最后，当前系统的一个潜在局限性在于场景图模拟器及其捕捉环境中各种规划失败的能力。虽然在本文中展示的案例中效果良好，但对于涉及多样化前提和能力的更复杂任务，为每个实例提供相关反馈信息可能变得不可行，这是未来这一领域研究的重要方向。 结论〓 ReTURN 〓 SayPlan是一个面向机器人的自然语言驱动规划框架，集成了层次化3D场景图和LLM，以规划跨越多个楼层和房间的大规模环境。通过利用3D场景图的层次结构和LLM的语义推理能力，我们确保了方法的可扩展性，使代理能够从层次结构的最高级别探索场景图，从而显著减少捕捉更大环境所需的初始标记。一旦完成探索，LLM会生成移动机械手机器人的任务计划，场景图模拟器通过迭代重新规划确保计划的可行性和符合环境。该框架在生成正确、可执行的计划方面超越了现有技术，机器人可以遵循这些计划。最后，我们成功地将验证过的计划转化为在多个房间、资产和物体的大型办公室环境中运行的真实移动机械手代理。SayPlan代表了能够在家庭、医院和工作场所运行的通用服务机器人向前迈进了一步，为这一领域的未来研究奠定了基础。","link":"/2024/06/04/sayplan/"},{"title":"使用Blender进行ply转obj","text":"摘要: ply文件转obj在进行转换时，时常遇到的问题是，ply转为obj后丢失了材质或者颜色信息，使用Blender可进行完美转换。 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 bpy脚本 终端运行命令 最终可视化效果 这个方法在blender3.6LTS下已经走通 bpy脚本〓 ReTURN 〓 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119import bpyimport bmeshimport sysimport os# 获取命令行参数argv = sys.argvargv = argv[argv.index(&quot;--&quot;) + 1:] # 只保留 -- 之后的参数if len(argv) &lt; 2: print(&quot;Usage: blender --background --python convert_ply_to_obj.py -- &lt;input_ply_file&gt; &lt;output_obj_file&gt;&quot;) sys.exit(1)input_ply_path = argv[0]output_obj_path = argv[1]def load_ply(file_path): bpy.ops.import_mesh.ply(filepath=file_path)def add_vertex_color_material(): obj = bpy.context.object mat = bpy.data.materials.new(name=&quot;VertexColorMaterial&quot;) mat.use_nodes = True nodes = mat.node_tree.nodes for node in nodes: nodes.remove(node) output_node = nodes.new(type=&quot;ShaderNodeOutputMaterial&quot;) diffuse_node = nodes.new(type=&quot;ShaderNodeBsdfDiffuse&quot;) vertex_color_node = nodes.new(type=&quot;ShaderNodeVertexColor&quot;) output_node.location = (300, 0) diffuse_node.location = (0, 0) vertex_color_node.location = (-300, 0) mat.node_tree.links.new(output_node.inputs['Surface'], diffuse_node.outputs['BSDF']) mat.node_tree.links.new(diffuse_node.inputs['Color'], vertex_color_node.outputs['Color']) if obj.data.materials: obj.data.materials[0] = mat else: obj.data.materials.append(mat)def bake_vertex_colors_to_texture(output_image_path): bpy.context.scene.render.engine = 'CYCLES' bpy.context.scene.cycles.device = 'CPU' obj = bpy.context.object mat = bpy.data.materials.new(name=&quot;BakedMaterial&quot;) mat.use_nodes = True nodes = mat.node_tree.nodes for node in nodes: nodes.remove(node) output_node = nodes.new(type=&quot;ShaderNodeOutputMaterial&quot;) emission_node = nodes.new(type=&quot;ShaderNodeEmission&quot;) vertex_color_node = nodes.new(type=&quot;ShaderNodeVertexColor&quot;) output_node.location = (300, 0) emission_node.location = (0, 0) vertex_color_node.location = (-300, 0) mat.node_tree.links.new(output_node.inputs['Surface'], emission_node.outputs['Emission']) mat.node_tree.links.new(emission_node.inputs['Color'], vertex_color_node.outputs['Color']) if obj.data.materials: obj.data.materials[0] = mat else: obj.data.materials.append(mat) image = bpy.data.images.new(name=&quot;BakedVertexColors&quot;, width=1024, height=1024) texture_node = nodes.new(type=&quot;ShaderNodeTexImage&quot;) texture_node.image = image texture_node.location = (-600, 0) bpy.context.view_layer.objects.active = obj bpy.ops.object.mode_set(mode='EDIT') bpy.ops.uv.smart_project() bpy.ops.object.mode_set(mode='OBJECT') bpy.context.scene.render.bake.use_pass_direct = False bpy.context.scene.render.bake.use_pass_indirect = False bpy.context.scene.render.bake.use_pass_color = True bpy.context.scene.render.bake.target = 'IMAGE_TEXTURES' texture_node.select = True nodes.active = texture_node bpy.ops.object.bake(type='EMIT') image.filepath_raw = output_image_path image.file_format = 'PNG' image.save()def export_obj(file_path, texture_file): bpy.ops.export_scene.obj(filepath=file_path, use_materials=True) # 手动更新 MTL 文件以包含 map_Kd 信息 mtl_file_path = file_path.replace('.obj', '.mtl') if os.path.exists(mtl_file_path): with open(mtl_file_path, 'a') as mtl_file: mtl_file.write(f&quot;\\nmap_Kd {texture_file}\\n&quot;)def main(input_ply_path, output_obj_path): bpy.ops.wm.read_factory_settings(use_empty=True) load_ply(input_ply_path) add_vertex_color_material() output_image_path = os.path.splitext(output_obj_path)[0] + &quot;_vertex_colors.png&quot; bake_vertex_colors_to_texture(output_image_path) export_obj(output_obj_path, os.path.basename(output_image_path))main(input_ply_path, output_obj_path) 终端运行命令〓 ReTURN 〓 1blender --background --python bpy.py -- source.ply target.obj 输出结果会包括三个文件: .obj, .mtl, .png 其中obj文件是点云位置， mtl是材质，png是纹理贴图，若要正确打开这个obj，需要达成： &nbsp;在obj里面指明mtl文件位置，在mtl里面指明png贴图位置&nbsp; obj示例 1234567# Blender v3.6.12 OBJ File: ''# www.blender.orgmtllib target.mtl # 指明mtl材质的位置o scene0467_00_vh_clean_2v 1.311216 0.775513 -0.504970v 1.280774 0.862291 -0.442830...... mtl示例 12345678910111213# Blender MTL File: 'None'# Material Count: 1newmtl BakedMaterialNs 359.999993Ka 1.000000 1.000000 1.000000Kd 0.800000 0.800000 0.800000Ks 0.500000 0.500000 0.500000Ke 0.000000 0.000000 0.000000Ni 1.000000d 1.000000illum 2map_Kd target_vertex_colors.png # 指明纹理贴图的位置 最终可视化效果〓 ReTURN 〓","link":"/2024/06/15/%E4%BD%BF%E7%94%A8Blender%E8%BF%9B%E8%A1%8Cply%E8%BD%ACobj/"},{"title":"限制gitlab的docker容器日志空间占用","text":"摘要: 错误的容器配置会造成docker容器的日志不断累积的问题， 例如在我的电脑上曾累积超过200GB的日志。 通过修改配置， 可以有效避免这种问题的发生。 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 问题描述 配置更改 问题描述对于使用docker desktop部署了private gitlab的朋友， 如果配置不当， 会造成docker容器的日志不断累积的问题。 例如在我的Mac Mini上就累积了超过200GB的日志文件。 通过修改配置， 可以有效避免这种问题的发生。 配置更改假设我们有一个正在运行的gitlab容器，可以通过以下步骤更新其日志设置： 首先我们查询容器id， 并根据id停止容器的运行 123456# 查询iddocker ps# 停止容器运行docker stop your_container_id# 移除容器（注意，这不会删除容器的数据卷）：docker rm your_container_id 接着，我们重新运行docker run来启动一个新的容器， 但注意， 这次我们更新启动命令， 把对日志大小的限制放到启动命令里面去 1docker run -d --log-opt max-size=50m --log-opt max-file=3 your_image_name 举个例子， 我的gitlab的启动命令如下： 123456789101112sudo docker run --detach \\--hostname {你的ip} \\--publish {你的端口1} \\--publish {你的端口2} \\--name gitlab \\--restart always \\--log-opt max-size=50m \\ # 这是重点--log-opt max-file=3 \\ # 这是重点--volume ~/gitlab/config:/etc/gitlab:Z \\--volume ~/gitlab/logs:/var/log/gitlab:Z \\--volume ~/gitlab/data:/var/opt/gitlab:Z \\ {你的镜像} 这样子， 我们的gitlab日志就再不会大于50m了。","link":"/2024/06/02/%E9%99%90%E5%88%B6docker%E5%AE%B9%E5%99%A8%E6%97%A5%E5%BF%97%E7%A9%BA%E9%97%B4%E5%8D%A0%E7%94%A8/"},{"title":"TinyCudaNN","text":"摘要: TinyCudaNN是一个用于训练和查询神经网络的小型、自包含框架。最值得注意的是，它包含一个极快的“全融合”多层感知机（技术论文）、一个多功能的多分辨率哈希编码（技术论文），以及对各种其他输入编码、损失函数和优化器的支持。 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 源码主页 安装 便捷安装 源码安装 源码主页GitHub - NVlabs/tiny-cuda-nn: Lightning fast C++/CUDA neural network framework 我们需要在源码主页上查询最新版本对于一些硬件条件的支持，例如： 12345678910RequirementsAn NVIDIA GPU; tensor cores increase performance when available. All shown results come from an RTX 3090.A C++14 capable compiler. The following choices are recommended and have been tested:Windows: Visual Studio 2019 or 2022Linux: GCC/G++ 8 or higherA recent version of CUDA. The following choices are recommended and have been tested:Windows: CUDA 11.5 or higherLinux: CUDA 10.2 or higherCMake v3.21 or higher.The fully fused MLP component of this framework requires a very large amount of shared memory in its default configuration. It will likely only work on an RTX 3090, an RTX 2080 Ti, or higher-end GPUs. Lower end cards must reduce the n_neurons parameter or use the CutlassMLP (better compatibility but slower) instead. 安装便捷安装1pip install ninja git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch 便捷安装能成功的前提条件是上一节中描述的prerequirements都满足。这里需要着重注意cuda版本要满足，同时要在环境变量PATH中存在。 我之前安装老编译失败， 就是忘了export 12export PATH=&quot;/usr/local/cuda-11.8/bin:$PATH&quot;export LD_LIBRARY_PATH=&quot;/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH&quot; 源码安装12345678git clone --recursive https://github.com/nvlabs/tiny-cuda-nncd tiny-cuda-nncmake . -B build -DCMAKE_BUILD_TYPE=RelWithDebInfocmake --build build --config RelWithDebInfo -jcd bindings/torchpython setup.py install 同样要注意把环境都配置好再进行编译。","link":"/2024/06/06/TinyCudaNN/"},{"title":"awesome_X","text":"摘要: 本文章用列表记录了多个awesome-xxx的github项目地址， 每一个项目都相当于是该领域的一个小的论文综述或者项目合集。 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } id name url 1 Awesome NeRF https://github.com/awesome-NeRF/awesome-NeRF 2 Awesome Visual Grounding https://github.com/TheShadow29/awesome-grounding 3 Awesome CLIP https://github.com/yzhuoning/Awesome-CLIP 4 Awesome PointCloud Analysis https://github.com/Yochengliu/awesome-point-cloud-analysis","link":"/2024/07/20/awesome-X/"},{"title":"HKUST 校历","text":"摘要: HKUST校历 .blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 600px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } 〓 Table of Contents 〓 2024Fall-2025Spring 2024Fall-2025Spring〓 ReTURN 〓","link":"/2024/09/09/school-calendar/"}],"tags":[{"name":"技术百科","slug":"技术百科","link":"/tags/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/"},{"name":"三维重建","slug":"三维重建","link":"/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"},{"name":"点云","slug":"点云","link":"/tags/%E7%82%B9%E4%BA%91/"},{"name":"Transformer","slug":"Transformer","link":"/tags/Transformer/"},{"name":"好文转载","slug":"好文转载","link":"/tags/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"论文阅读","slug":"论文阅读","link":"/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"LLMAgent","slug":"LLMAgent","link":"/tags/LLMAgent/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"DSF","slug":"DSF","link":"/tags/DSF/"},{"name":"服务器","slug":"服务器","link":"/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"CUDA驱动","slug":"CUDA驱动","link":"/tags/CUDA%E9%A9%B1%E5%8A%A8/"},{"name":"computer_vision","slug":"computer-vision","link":"/tags/computer-vision/"},{"name":"PCDM","slug":"PCDM","link":"/tags/PCDM/"},{"name":"大神语录","slug":"大神语录","link":"/tags/%E5%A4%A7%E7%A5%9E%E8%AF%AD%E5%BD%95/"},{"name":"博士生涯","slug":"博士生涯","link":"/tags/%E5%8D%9A%E5%A3%AB%E7%94%9F%E6%B6%AF/"},{"name":"MacOS","slug":"MacOS","link":"/tags/MacOS/"},{"name":"外接设备","slug":"外接设备","link":"/tags/%E5%A4%96%E6%8E%A5%E8%AE%BE%E5%A4%87/"},{"name":"3DLLM","slug":"3DLLM","link":"/tags/3DLLM/"},{"name":"gcc","slug":"gcc","link":"/tags/gcc/"},{"name":"系统环境及编译","slug":"系统环境及编译","link":"/tags/%E7%B3%BB%E7%BB%9F%E7%8E%AF%E5%A2%83%E5%8F%8A%E7%BC%96%E8%AF%91/"},{"name":"conda-forge","slug":"conda-forge","link":"/tags/conda-forge/"},{"name":"视频编辑","slug":"视频编辑","link":"/tags/%E8%A7%86%E9%A2%91%E7%BC%96%E8%BE%91/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"gitlab","slug":"gitlab","link":"/tags/gitlab/"},{"name":"awesome合集","slug":"awesome合集","link":"/tags/awesome%E5%90%88%E9%9B%86/"},{"name":"HKUST","slug":"HKUST","link":"/tags/HKUST/"},{"name":"官方文件","slug":"官方文件","link":"/tags/%E5%AE%98%E6%96%B9%E6%96%87%E4%BB%B6/"}],"categories":[{"name":"技术百科","slug":"技术百科","link":"/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/"},{"name":"好文转载","slug":"好文转载","link":"/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"},{"name":"论文阅读","slug":"论文阅读","link":"/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"Python","slug":"技术百科/Python","link":"/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/Python/"},{"name":"三维重建","slug":"技术百科/三维重建","link":"/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"},{"name":"DSF","slug":"DSF","link":"/categories/DSF/"},{"name":"computer_vision","slug":"技术百科/computer-vision","link":"/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/computer-vision/"},{"name":"NLP","slug":"好文转载/NLP","link":"/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/NLP/"},{"name":"大神语录","slug":"好文转载/大神语录","link":"/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/%E5%A4%A7%E7%A5%9E%E8%AF%AD%E5%BD%95/"},{"name":"LLMAgent","slug":"论文阅读/LLMAgent","link":"/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLMAgent/"},{"name":"MacOS","slug":"技术百科/MacOS","link":"/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/MacOS/"},{"name":"3DLLM","slug":"论文阅读/3DLLM","link":"/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/3DLLM/"},{"name":"三维重建","slug":"论文阅读/三维重建","link":"/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"},{"name":"PCDM","slug":"论文阅读/PCDM","link":"/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/PCDM/"},{"name":"服务器环境配置","slug":"DSF/服务器环境配置","link":"/categories/DSF/%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"视频编辑","slug":"技术百科/视频编辑","link":"/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/%E8%A7%86%E9%A2%91%E7%BC%96%E8%BE%91/"},{"name":"系统环境及编译","slug":"技术百科/系统环境及编译","link":"/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/%E7%B3%BB%E7%BB%9F%E7%8E%AF%E5%A2%83%E5%8F%8A%E7%BC%96%E8%AF%91/"},{"name":"gitlab","slug":"技术百科/gitlab","link":"/categories/%E6%8A%80%E6%9C%AF%E7%99%BE%E7%A7%91/gitlab/"},{"name":"awesome合集","slug":"论文阅读/awesome合集","link":"/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/awesome%E5%90%88%E9%9B%86/"},{"name":"HKUST","slug":"HKUST","link":"/categories/HKUST/"},{"name":"官方文件","slug":"HKUST/官方文件","link":"/categories/HKUST/%E5%AE%98%E6%96%B9%E6%96%87%E4%BB%B6/"}],"pages":[{"title":"About Me","text":".blue-box { border: 2px solid blue; /* 设置边框为蓝色 */ padding: 5px; /* 设置内边距 */ background-color: lightblue; /* 设置背景颜色为浅蓝色 */ color: black; /* 设置文本颜色为黑色 */ width: 800px; /* 设置方框的宽度 */ margin: 5px; /* 设置外边距 */ } I am now a PhD student in Computer Science Department at The Hong Kong University of Science and Technology (HKUST), advised by Prof. Xiaofang Zhou. My current research focuses on enhancing large-scale 3D scene understanding by effectively integrating deep learning models for point cloud processing with the power of advanced Vision-Language Models (VLMs). I am developing novel methods to adapt and transfer knowledge from large pre-trained VLMs (primarily trained on 2D data) to enable robust interpretation of complex 3D environments directly from their native representations (e.g., point clouds). My specific interest lies in improving performance on challenging tasks such as 3D visual grounding involving complex spatial relationships within large, realistic indoor scenes. My broader interests include multimodal representation learning, large data systems, and embodied AI in 3D environments. News2024年12月20日 星期五 - 2024秋季学期结束, 所有课程都高分通过, 课程压力大大减少了.2024年9月4日 星期三 - 2024秋季学期开始，DSF-Spatiotemporal Analytics创建。2024年7月2日 星期二 - 🎉我们的工作《High-Quality Mesh Blendshape Generation from Face Videos via Neural Inverse Rendering》被ECCV24接收！🎉2024年6月5日 星期三 - 自从2005年后，HKUSTCSEDB大组的seminar正式恢复了！2024年5月30日 星期四 - 2024春季学期课程终于结束了！！2024年5月17日 星期五 - ECCV24的Rebuttal结束！！考试周开启！！ Publications[1] Ming, X., Li, J., Ling, J., Zhang, L., Xu, F. (2025). High-Quality Mesh Blendshape Generation from Face Videos via Neural Inverse Rendering. In: Leonardis, A., Ricci, E., Roth, S., Russakovsky, O., Sattler, T., Varol, G. (eds) Computer Vision – ECCV 2024. ECCV 2024. Lecture Notes in Computer Science, vol 15128. Springer, Cham. https://doi.org/10.1007/978-3-031-72897-6_7 [2] Li, J., Lyu, L., Shi, J., Zhao, J., Xu, J., Gao, J., He, R., &amp; Sun, Z. (2022). Generating community road network from GPS trajectories via style transfer. Proceedings of the 30th International Conference on Advances in Geographic Information Systems. https://doi.org/10.1145/3557915.3560958 [3] A Semantic Segmentation based POI Coordinates Generating Framework for On-demand Food Delivery Service. In Proceedings of the 29th International Conference on Advances in Geographic Information Systems, pp. 379-388. 2021. [4] Unclonable Photonic Crystal Hydrogels with Controllable Encoding Capacity for Anti-counterfeiting, ACS Applied Materials &amp; Interfaces 2021, 14, 2369-2380. [5] Spatial Technology Assessment of Green Space Exposure and Myopia. Ophthalmology. 129. 10.1016/j.ophtha.2021. 07.031. [6] Factors influencing subspecialty choice among medical students: a systematic review and meta-analysis, BMJ Open Mar 2019, 9 (3) e022097 [7] Acceptance Sampling Plans with Type-I Hybrid Censoring Scheme of Weibull Distribution. Advances in Applied Mathematics.03.184-191.10. 12677/AAM.2014.34027. Patent[1]An Unsupervised Method for Trajectory Generating Road Network Based on Style Transfer [2]A Route Mining Algorithm Based on Track Points [3]A Method for Enhancing Natural Language Feature Extraction Model using Knowledge Graph","link":"/about/index.html"}]}